\documentclass[11pt]{article}

\usepackage{amsfonts,amsmath,amsthm,amssymb}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{thmtools, thm-restate}
\usepackage{amsthm}
\usepackage{authblk}

%\newif\ifbigfont\bigfonttrue
\newif\ifbigfont\bigfontfalse

\ifbigfont

\usepackage[right=.5in,left=.5in,top=.6in,bottom=.6in]{geometry}
\usepackage[17pt]{extsizes}
\usepackage{newpxmath,newpxtext}

\else

\usepackage[margin=1in]{geometry}
%\usepackage{newpxmath,newpxtext}

\fi

\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\heading}[1]{\vspace{1ex}\par\noindent{\bf\boldmath #1}}

\newcommand{\cut}[1]{}
\newcommand{\ceil}[1]{\lceil #1 \rceil}

\newcommand{\R}{{\mathbb{R}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\N}{{\mathbb{N}}}
\renewcommand{\S}{\mathbb{S}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\T}{\mathsf T}

\newcommand\eps{\varepsilon}
\newcommand{\eqdef}{\triangleq}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\newcommand{\set}[1]{\left\{ #1 \right\}}

\DeclareMathOperator{\vollb}{volLB}
\DeclareMathOperator{\detlb}{detLB}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\hd}{hd}
\DeclareMathOperator{\vb}{vb}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\lspan}{span}
\DeclareMathOperator{\speclb}{specLB}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\bnds}{bounds}
\DeclareMathOperator{\rank}{rk}

\DeclarePairedDelimiter\norm{\lVert}{\rVert}

% MARGIN NOTES
\newif\ifnotes\notestrue
%\newif\ifnotes\notesfalse

\ifnotes
\usepackage{color}
%\definecolor{mygrey}{gray}{0.50}

\ifbigfont

\newcommand{\notename}[2]{{\textcolor{red}{{\bf (#1:} {#2}{\bf ) }}}}

\else

\newcommand{\notename}[2]{{\textcolor{red}{\footnotesize{\bf (#1:} {#2}{\bf ) }}}}

\fi

\newcommand{\noteswarning}{{\begin{center} {\Large WARNING: NOTES ON}\end{center}}}
\newcommand{\knote}[1]{{\notename{Kunal}{#1}}}
\newcommand{\nnote}[1]{{\notename{Nicole}{#1}}}
\newcommand{\dnote}[1]{{\notename{Daniel}{#1}}}
\newcommand{\snote}[1]{{\notename{Sasho}{#1}}}


\else

\newcommand{\notename}[2]{{}}
\newcommand{\noteswarning}{{}}
\newcommand{\knote}[1]{}
\newcommand{\nnote}[1]{}
\newcommand{\dnote}[1]{}
\newcommand{\snote}[1]{}

\fi



\begin{document}
\title{Balancing Vectors in Any Norm}
\author[1]{Daniel Dadush
\thanks{Email: \href{mailto:dadush@cwi.nl}{dadush@cwi.nl}.
Supported by NWO Veni grant 639.071.510.}} 
\author[2]{ 
Aleksandar Nikolov
\thanks{Email: \href{mailto:anikolov@cs.toronto.edu}{anikolov@cs.toronto.edu}.}}
\author[3]{
Kunal Talwar
\thanks{Email: \href{mailto:kunal@google.com}{kunal@google.com}.}} 
\author[4]{
Nicole Tomczak-Jaegermann
\thanks{Email: 
\href{mailto:nicole.tomczak@ualberta.ca}{nicole.tomczak@ualberta.ca}.}} 

\affil[1]{\small Centrum Wiskunde \& Informatica.}
\affil[2]{\small University of Toronto.}
\affil[3]{\small Google Brain.}
\affil[4]{\small University of Alberta.}

\maketitle

\noteswarning

\begin{abstract}
In the vector balancing problem, we are given symmetric convex bodies $C$ and
$K$ in $\R^n$, and our goal is to determine the minimum number $\beta \geq 0$,
known as the vector balancing constant from $C$ to $K$, such that for any
sequence of vectors in $C$ there always exists a signed combination of them
lying inside $\beta K$. Many fundamental results in discrepancy theory, such as
the Beck-Fiala (Discrete Appl. Math `81), Spencer's ``six standard deviations
suffice'' (Trans. Amer. Math. Soc `85) and Banaszczyk's vector balancing theorem
(RSA `98) correspond to bounds on vector balancing
constants. 

The above theorems have inspired much research in recent years within
theoretical computer science, from the development of efficient polynomial time
algorithms for matching existential vector balancing guarantees, to
their applications in the context of approximation algorithms. In this work, we
show that all vector balancing constants admit ``good'' approximate
characterizations, with approximation factors depending only polylogarithmically
on the dimension $n$. Firstly, we show that a volumetric lower bound due to
Banaszczyk is tight within a $O(\log n)$ factor. Our proof is algorithmic, where
we show that Rothvoss's (FOCS `14) partial coloring algorithm can be
analyzed to obtain these guarantees. Second, we present a novel convex program
which encodes the ``best possible  way'' to apply Banaszczyk's vector balancing
theorem for upper bounding vector balancing constants, and show that it is tight
within an $O(\log^{2.5} n)$ factor. This also directly yields a corresponding
polynomial time approximation algorithm for the hereditary discrepancy of any
sequence of vectors with respect to an arbitrary norm.   
  
Our results yield the first guarantees which depend only polygarithmically on
the dimension of the norm ball $K$. All prior works required the norm to be
polyhedral and incurred a dependence of $O(\sqrt{\log m})$, where $m$ is the
number of facets. Our techniques rely on a novel combination of techniques
convex geometry and discrepancy theory. In particular, we give a new way to
lower bound Gaussian measures using only volumetric information, which may be of
independent interest.
\end{abstract}

\textbf{Keywords.}  Discrepancy, Convex Geometry, Gaussian Measure, M-ellipsoid,
K-convexity.

\thispagestyle{empty}

\newpage

\setcounter{page}{1}

\section{Introduction}

The discrepancy of a set system is defined as the minimum, over the set of $\pm 1$ colorings of the elements, of the imbalance between the number of $+1$ and $-1$ elements in the most imbalanced set. Classical combinatorial discrepancy theory studies bounds on the discrepancy of set systems, in terms of their structure. 
%In classical combinatorial discrepancy, one studies for various set systems,
%over the set of $\pm 1$ colorings of the elements, what is the minimum
%worst-case imbalance, known as discrepancy, one guarantee between the number of
%$+1$ and $-1$ elements in every set? 
The tools developed for deriving bounds on
the discrepancy of set systems have found many applications in mathematics and
computer science~\cite{Matousek,Chazelle}, from the study of pseudorandomness,
to communication complexity, and most recently, to approximation algorithms and privacy. 

\paragraph{\bf Vector Balancing} In many instances, the best known techniques for finding
good bounds in combinatorial discrepancy were derived by working with more
general vector balancing problems, where convex geometric techniques can be
applied. Given symmetric convex bodies $C,K \subseteq \R^n$, the vector
balancing constant of $C$ into $K$ is defined as 
% i.e.~which induce the unit ball of a norm, and the
% goal is to understand the minimum number $M \geq 0$ such that for any sequence
% $(v_i)_{i=1}^N \in C$, there exists signs $(x_i)_{i=1}^N \in \set{-1,1}$ such
% that $\sum_{i=1}^N x_i v_i \in M D$. The minimum $M$ is denoted $\vb(C,D)$,
% the vector balancing constant of $C$ into $D$. 
\[
\vb(C, K) \eqdef \sup\Bigg\{ 
\min_{x \in \set{-1,1}^N} \Bigl\|\sum_{i = 1}^N x_i
u_i \Bigr\|_{K}: N \in \mathbb{N}, u_1, \ldots, u_N \in C \Bigg\},
\]
where $\norm{x}_K := \min \set{s \geq 0: x \in sK}$ is the norm induced by $K$.

As an example, one may consider Spencer's theorem~\cite{Spencer}, independently
obtained by Gluskin~\cite{gluskin}, which states that every set system on $n$
points and $n$ sets can be colored with discrepancy at most $O(\sqrt{n})$. In
the vector balancing context, the more general statement is that
$\vb(B_\infty^n,B_\infty^n) = O(\sqrt{n})$ (also proved
in~\cite{Spencer,gluskin}), where we use the notation $B_p^n = \set{x \in \R^n:
\norm{x}_p \leq 1}$, $p \in [1,\infty]$, to denote the unit ball of the $\ell_p$
norm. To encode Spencer's theorem, we simply represent the set system using its
incidence matrix $U \in \set{0,1}^{n \times n}$, where $U_{ji} = 1$ if element
$i$ is in set $j$ and $0$ otherwise. Here the columns of $U$ have $\ell_\infty$
norm $1$, and thus the sign vector $x \in \set{-1,1}^n$ satisfying $\norm{U
x}_\infty = O(\sqrt{n})$ indeed yields the desired coloring.   

Many fundamental discrepancy bounds, as well as conjectured bounds, can be
stated in terms of vector balancing constants. The Beck-Fiala theorem, which
bounds the discrepancy of any $t$-sparse set system by $2t-1$, i.e.~where each
element appears in at most $t$-sets, can be recovered from the bound
$\vb(B_1^n,B_\infty^n) < 2$~\cite{beckfiala}. The Beck-Fiala conjecture,
which asks whether the bound for $t$-sparse set systems can be improved to
$O(\sqrt{t})$, is generalized by the K{\'o}mlos conjecture~\cite{spencer-lectures},
which asks whether $\vb(B_2^n,B_\infty^n) = O(1)$. One of the most important
vector balancing bounds is due to Banaszczyk~\cite{bana}, who proved that for
any convex body $K \subseteq \R^n$ of Gaussian measure $1/2$, one has the bound
$\vb(B_2^n,K) \leq 5$. In particular, this implies the bound of
$\vb(B_2^n,B_\infty^n) = O(\sqrt{\log n})$ for the K{\'o}mlos conjecture. 

\paragraph{\bf Hereditary Discrepancy.} While vector balancing gives useful
worst-case bounds, one is often interested in understanding the discrepancy
guarantees one can get for instances derived from a fixed set of vectors, known
as hereditary discrepancy. Given vectors $(u_i)_{i=1}^N$ in $\R^n$, the
discrepancy and hereditary discrepancy with respect to a symmetric convex body
$K \subseteq \R^n$ are defined as:
\begin{align*}
\disc((u_i)_{i = 1}^N,K) &\eqdef \min_{\eps_1, \ldots,\eps_N \in
  \{-1, 1\}}
\Bigl\|\sum_{i = 1}^N{\eps_i u_i}\Bigr\|_{K};\\
\hd((u_i)_{i = 1}^N, K) &\eqdef \max_{S \subseteq [N]}{\disc((u_i)_{i \in S}, K)}.
\end{align*}

When convenient, we will also use the notation $\hd(U,K) :=
\hd((u_i)_{i=1}^N,K)$, where $U := (u_1,\dots,u_N) \in \R^{n \times N}$, and
$\disc(U_S,K) := \disc((u_i)_{i \in S},K)$ for any subset $S \subseteq [N]$.  In
the context of set systems, $\ell_\infty$ hereditary discrepancy corresponds to
the worst-case discrepancy of any element induced subsystem, which gives a
robust notion of how low discrepancy a set system is. As an interesting example,
a set system has $\ell_\infty$ hereditary discrepancy $1$ if and only if its
incidence matrix is totally unimodular. 

Beyond set systems, hereditary discrepancy can also usefully bound the
worst-case ``error'' required for rounding a fractional LP solution to an
integral one. More precisely, given any solution $y \in \R^n$ to a linear
programming relaxation $Ax \leq b$, $x \in [0,1]^n$, with $A \in \R^{m \times
n}, b \in \R^m$, of a binary IP, and given any norm $\norm{\cdot}$ on $\R^m$
measuring ``constraint violation'', one can ask what guarantees can be given on
$\min_{x \in \set{0,1}^n} \norm{A(y-x)}$? Using a well-known reduction of
Lov{\'a}sz, Spencer and Vesztergombi~\cite{LSV}, this error can be bounded by
$\hd(A,K)$ where $K$ is unit ball of $\norm{\cdot}$. Furthermore,
this reduction guarantees that $x$ agrees with $y$ on its integer coordinates.
Note that we have the freedom to choose the norm $\norm{\cdot}$ so that the
error bounds meaningfully relate to the structure of the problem. Indeed, much
work has been done on the achievable ``error profiles'' one can obtain
algorithmically, e.g.~for which $\Delta \in \R^m_{> 0}$ we can always find $x
\in \set{0,1}^m$ satisfying $|A(y-x)| \leq \Delta$, $\forall y \in [0,1]^m$?
Note that the feasibility of an error profile can be recovered from a bound of
$1$ on the hereditary discrepancy with respect to the weighted $\ell_\infty$
norm $\norm{y-x}_{\Delta} = \max_{i \in [m]} |y_i-x_i|/\Delta_i$.  Indeed, in
many instances, this is (at least implicitly) how these bounds are proved. These
error profile bounds have been fruitfully leveraged for problems where small
``additive violations'' to the constraints are either allow or can be repaired.
In particular, they were used in for the recent $O(\log n)$-additive
approximation for bin packing~\cite{HobergRothvoss17}, an additive approximation
scheme for the train delivery problem~\cite{R12}, and additive approximations of
the degree bounded matroid basis problem~\cite{BN15}. 

\paragraph{\bf Discrepancy Minimization.} The original proofs of many of the
aforementioned discrepancy upper bounds were existential, and did not come with
efficient algorithms capable of constructing the requisite low discrepancy
colorings. %Over the last eight years, starting with the breakthrough work of
%Bansal~\cite{Bansal10}, who gave a constructive version of Spencer's theorem
%using random walk and semidefinite programming techniques, almost all known
%bounds have been made algorithmic. 
Starting wiht the breakthrough work of Bansal~\cite{Bansal10}, who gave a constructive version of Spencer's theorem using random walk and semidefinite programming techniques, nearly all known bounds have been made algorithmic in the last eight years.

One of the most important discrepancy minimization techniques is known as the
partial coloring method, which covers most of the above discrepancy results
apart from Banaszczyk's vector balancing theorem. This method was first
primarily applied to $\ell_\infty$ discrepancy minimization problems of the form
\[
\min_{x \in \set{-1,1}^n} \Big\|\sum_{i=1}^n x_i v_i\Big\|_\infty, \text{ where
} (v_i)_{i=1}^n \in \R^m.
\]
As before, the goal is not to solve such problems near-optimally but instead to find
solutions satisfying a guaranteed error bound. The partial coloring method
solves this problem phases, where at each phase it ``colors'' (i.e.~sets to $\pm
1$) at least a constant fraction of the remaining uncolored variables.  This
yields $O(\log n)$ partial coloring phases, where the discrepancy of the full
coloring is generally upper bounded by the sum of discrepancies incurred in each
phase. The existence of low discrepancy partial colorings, i.e.~which color half
the variables, was initially established via the pigeon hole principle and
certain entropy bounds for random $\pm 1$ combinations of the vectors, known as
Beck's entropy method. In particular, the entropy method gave a general
sufficient condition for the feasibility of any error profile (as above) with
respect to partial colorings. This method was made constructive by Lovett and
Meka~\cite{lovettmeka} using random walk techniques. The entropy method was
further generalized by Giannopoulos~\cite{giann} to the general vector balancing
setting using Gaussian measure. Precisely, he showed that if a symmetric convex
body $K \subseteq \R^n$ has Gaussian measure at least $2^{-c n}$, for
$c$ small enough, then for any sequence vector $v_1,\dots,v_n \in B_2^n$,
there exists a partial coloring $x \in \set{-1,0,1}^n$, having support at least
$n/2$, such that $\sum_{i=1}^n x_i v_i \in O(1) K$. This method was made
constructive by Rothvoss~\cite{rothvoss-giann}, using a random projection
algorithm, and later by Eldan and Singh~\cite{ES14} who used the solution of a
random linear maximization problem. An important difference between the
constructive and existential partial coloring methods, is that the constructive
methods only guarantee that the ``uncolored'' coordinates of a partial coloring
$x$ are in $(-1,1)$ instead of equal to $0$. This relaxation seems to make the
constructive methods more robust, i.e.~the conditions needed for such ``fractional''
partial colorings are somewhat milder, without having noticeable drawbacks in most
applications. 

The main alternative to the partial coloring method comes from Banaszczyk's
vector balancing theorem~\cite{bana}. Comparing to Giannopoulos, when the input
norm is $\ell_2$ Banaszczyk's method proves the existence of a full coloring when
$K$ has gaussian measure $1/2$ compared to a partial coloring when $K$ has measure
$2^{-cn}$.  Banaszczyk's method was only very recently made constructive in
the sequence of works~\cite{BDG16,DGLN16,BDGL18}. In particular,~\cite{DGLN16}
showed an equivalence of Banaszcyzk's theorem to the existence of certain
subgaussian signing distributions, and~\cite{BDGL18} gave a random walk
algorithm to build such distributions.

\subsection{Approximating Vector Balancing and Hereditary Discrepancy}

Given the powerful tools that have been developed above, a natural question is
whether they can be extended to get nearly optimal bounds for any vector
balancing or hereditary discrepancy problem. More precisely, we will be
interested in the following computational and mathematical questions: 

\begin{enumerate}
\item Given vectors $(u_i)_{i=1}^N$ and a symmetric convex body $K$ in $\R^n$,
can we (a) efficently compute a coloring whose $K$-discrepancy is approximately
bounded by $\hd((u_i)_{i=1}^N,K)$? (b) efficiently approximate $\hd((u_i)_{i=1}^N,K)$?
\item Given two symmetric convex bodies $C,K \subseteq \R^n$, does $\vb(C,K)$
admit a ``good'' characterization? Namely, are there simple certificates which
certify nearly tight upper and lower bounds on $\vb(C,K)$?
\end{enumerate}

To begin, a few remarks are in order.  Firstly, question 2 can be inefficiently
encoded as question 1b, by letting $(u_i)_{i=1}^N$ denote a sufficiently fine
net of $C$. Thus ``good'' characterizations for hereditary discrepancy transfer
over to vector balancing, and thus we restrict for now the discussion to the
former. For question 1a, one may be tempted to ask whether we can directly
compute a coloring whose $K$-discrepancy is approximately
$\disc((u_i)_{i=1}^N,K)$ instead of $\hd((u_i)_{i=1}^N,K)$. Unfortunately, even
for $K = B_\infty^n$ and $(u_i)_{i=1}^n \in [-1,1]^n$, it is NP-hard to
distinguish whether $\disc((u_i)_{i=1}^n,B_\infty^n)$ is $0$ or
$\Omega(\sqrt{n})$ (note that $O(\sqrt{n})$ is guaranteed by Spencer's theorem),
thus one cannot hope for any non-trivial approximation guarantee in this
context. We now discuss prior work on these questions and then continue with our
main results. 

\paragraph{Prior work.} For both questions, prior work has mostly dealt with the
case of $\ell_\infty$ or $\ell_2$ discrepancy. Bounds on vector balancing
constants for some combinations of $\ell_p$ to $\ell_q$ have also been studied,
as described earlier, however without a unified approach. The question of
obtaining near-optimal results for general vector balancing and hereditary
discrepancy problems has on the other hand not been studied before. 

In terms of coloring algorithms, Bansal~\cite{Bansal10} gave a partial coloring
based random walk algorithm which on $U \in \R^{m \times n}$, produces a full
coloring of $\ell_\infty$ discrepancy $O(\sqrt{\log m \log \rank(U)}
\hd(U,B_\infty^m))$, where $\rank(U)$ is the rank of $U$. Recently,
Larsen~\cite{Larsen17} gave an algorithm for the $\ell_2$ norm achieving
discrepancy $O(\sqrt{\rank(U)}\hd(U,B_2^m))$. 

In terms of certifying lower bounds on $\hd(U,B_\infty^m))$, the main tool has
been the so-called determinant lower bound of~\cite{LSV}, where it was shown that
\[
\hd(U,B_\infty^m) \geq \detlb(U) := \max_k \max_B \frac{1}{2}|\det(B)|^{1/k}
\]       
where the maximum is over $k \times k$ submatrices $B$ of $U$.
Matousek~\cite{Matousek11}, built upon the results of~\cite{Bansal10} to show that 
\[
\hd(U,B_\infty^m) \leq O(\sqrt{\log m} \log^{3/2}(\rank(U)) \detlb(U)).
\]
For certifying tight upper bounds,~\cite{NT15,disc-gamma2} showed that
$\gamma_2$ norm of $U$, defined by 
\[
\gamma_2(U) := \min \set{\|A\|_{2 \rightarrow \infty} \|B\|_{1 \rightarrow 2}: U
= A B, A \in \R^{m \times k}, B \in \R^{k \times n}, k \in \N}  
\]
where $\|A\|_{2 \rightarrow \infty}$ is the maximum $\ell_2$ norm of any row of $A$,
and $\|B\|_{1 \rightarrow 2}$ is the maximum $\ell_2$ norm of any column of $B$,
satisfies
\begin{equation}
\Omega(\gamma_2(U)/\log(\rank(U))) \leq \detlb(U) \leq \hd(U,B_\infty^m) \leq O(\sqrt{\log m} \gamma_2(U)) 
\end{equation}
which implies a $O(\sqrt{\log m} \log \rank(U))$ approximation to $\ell_\infty$
hereditary discrepancy. For the context of $\ell_2$, it was shown in~\cite{NT15}
that a relaxation of $\gamma_2$ yields an $O(\log \rank(U))$-approximation to
$\hd(U,B_2^m)$. We note that part of the strategy of~\cite{NT15,disc-gamma2}
is to replace the $\ell_\infty$ norm via an averaged version of $\ell_2$, where
one optimizes over the averaging coefficients, which makes the $\ell_2$ norm by
itself an easier special case.

\paragraph{\bf Moving to general norms.} While at first glance it may seem that
the above techniques for $\ell_\infty$ do not apply to more general norms, this
is in some sense deceptive. Notwithstanding complexity considerations, every
norm can be isometrically embedded into $\ell_\infty$, where in particular any
polyhedral norm with $m$ facets can be embedded into $B^m_\infty$. Vice versa,
starting from $U \in \R^{m \times N}$, with $\rank(U) = n$ and rank
factorization $U = A B$, is it direct to verify $\hd(U,B^m_\infty) = \hd(B,K)$,
where $K = \set{x \in \R^n: |Ax| \leq 1}$ is an $n$-dimensional symmetric
polytope with $m$ facets. Thus, for any $U \in \R^{n \times N}$, one can
equivalently restate the guarantees of~\cite{Bansal10} as yielding colorings of
discrepancy $O(\sqrt{\log m \log n} \hd(U,K))$ and of~\cite{disc-gamma2} as a
$O(\sqrt{\log m}\log n)$ approximation to $\hd(U,K)$ for any $n$-dimensional
symmetric polytope $K$ with $m$ facets. A natural question is therefore
whether there exists corresponding coloring and approximation algorithms whose
guarantees depend only polygarithmically on the dimension of the norm and not on
the complexity its representation. 

We note that polynomial bounds in $n$ for general $K$ can be achieved by simply
approximating $K$ by a sandwiching ellipsoid $E \subseteq K \subseteq \sqrt{n}
E$ and applying the corresponding results for $\ell_2$, which yield $O(\sqrt{n
\log n})$ coloring and $O(\sqrt{n}\log n)$ approximations guarantees
respectively. Interestingly, these guarantees are identical to what can be
achieved by replacing $K$ by a symmetric polytope with $3^n$ facets, which can
achieve a sandwiching factor of $2$, and applying the $\ell_\infty$ results.

\subsection{Results} 
Our main results are that such polylogarithmic approximations are indeed
possible. In particular, given $U \in \R^{n \times N}$ and a symmetric convex
body $K \subseteq \R^n$ (by an appropriate oracle), we give randomized
polynomial time algorithms for computing colorings of discrepancy $O(\log n
\hd(U,K))$ and approximating $\hd(U,K)$ up to $O(\log^{2.5} n)$ factor.
Furthermore, if $K$ is a polyhedral norm with at most $m$ facets, our
approximation algorithm for $\hd(U,K)$ always achieves a tighter approximation
factor than the $\gamma_2$ bound, and hence gives an $O(\min \set{\log n
\sqrt{\log m}, \log^{2.5} n})$ approximation. To achieve these results, we first
show that Rothvoss' partial coloring algorithm~\cite{rothvoss-giann} is nearly
optimal for general hereditary discrepancy by showing near-tightness with
respect to a volumetric lower bound of Banaszczyk~\cite{Bana93}. Second, we show
that the ``best possible way'' to apply Banaszcyzk's vector balancing
theorem~\cite{bana} for the purpose of upper bounding $\hd(U,K)$ can be
encoded as a convex program, and prove that this bound is tight to within an
$O(\log^{2.5} n)$ factor. As a consequence, we show that Banaszczyk's theorem is
essentially ``universal'' for vector balancing. To analyze these approaches we
rely on a novel combination of tools from convex geometry and discrepancy. In
particular, we give a new way to prove lower bounds on Gaussian measure using
only volumetric information, which could be of independent interest.
Furthermore, we make a natural geometric conjecture which would imply that
Rothvoss' algorithm is a hereditary sense optimal for finding partial colorings
in any norm, and prove the conjecture for the special case of $\ell_2$.

Comparing to prior work, our coloring and hereditary discrepancy
approximation algorithms give uniformly better (or at at least no worse)
guarantees in almost every setting which has been studied. Furthermore our
methods provide a unified approach for studying discrepancy in arbitrary norms,
which we expect to have further applications.

Interestingly, our results imply a tighter relationship between vector balancing
and hereditary discrepancy than one might initially expect. That is,
neither the volumetric lower bound we use nor our factorization based upper
bound ``see'' the difference between them. More precisely, both bounds remain
invariant when replacing $\hd(U,K)$ by $\vb(\conv(\pm u_i: i \in [N]),K)$. This
has the relatively non-obvious implication that 
\begin{equation}
\label{eq:hd-to-vb}
\hd(U,K) \leq \vb(\conv(\pm u_i: i \in [N]),K) \leq O(\log n) \hd(U,K).
\end{equation}
We believe it is an interesting question to understand whether a polylogarithmic
separation indeed exists between the above quantities (we are currently unaware
of any examples), as it would give a tangible geometric obstruction for tighter
approximations.    

\subsection{Techniques}

Starting with hereditary discrepancy, to push beyond the limitations of prior
approaches the first two tasks at hand are: (1) find a stronger lower bound and
(2) develop techniques to avoid the ``union bound''. Fortunately, a solution to the
first problem was already given by Banaszczyk\cite{Bana93}, which we present in
slightly adapted form below.

\begin{restatable}[Volume Lower Bound]{lemma}{vollbstat} 
\label{lem:vol-lb}
Let $U=(u_1,\dots,u_N) \in \R^{n \times N}$ and $K \subseteq \R^n$ be a
symmetric convex body. For $S \subseteq [N]$, let $U_S$ denote the columns
of $U$ in $S$. For $k \in [n]$, define 
\begin{equation}
\label{eq:vol-lb}
\vollb^{\rm h}_k((u_i)_{i=1}^n,K) \eqdef \vollb^{\rm h}_k(U, K) \eqdef
\max_{S \subseteq [N],|S|=k} \vol_k(\{x \in \R^k: U_S x \in K\})^{-1/k}.
\end{equation}
Then, we have that
\begin{equation}
\label{eq:vol-lbh}
\vollb^{\rm h}((u_i)_{i=1}^n,K) \eqdef \vollb^{\rm h}(U,K) \eqdef \max_{k \in [n]} \vollb^{\rm h}_k(U,K) \leq \hd(U,K). 
\end{equation}
\end{restatable}

A formal proof of the above is given in the preliminaries (see
section~\ref{sec:proof-vollb}). At a high level, the proof is a simple covering
argument, where it is argued that for any subset $S$, $|S|=k$, every point in
$[0,1]^k$ is at distance at most $\hd(U,K)$ from $\set{0,1}^k$ under the norm
induced by $C := \set{x \in \R^k: U_S x \in K}$. Equivalently an $\hd(U,K)$
scaling of $C$ placed around the points of $\set{0,1}^k$ cover $[0,1]^k$, and
hence by a standard lattice argument must have volume at least that of
$[0,1]^k$, namely $1$. This yields the desired lower bound after rearranging. 

We note that the volume lower bound extends in the obvious way to vector
balancing. In particular, for two symmetric convex bodies $C,K \subseteq \R^n$,
\begin{equation}
\label{eq:vol-lb-vb}
\vollb^{\rm h}(C,K) \eqdef \sup \set{\vollb((u_i)_{i=1}^k,K):k \in [n],
u_1,\dots,u_k \in C} \geq \vb(C,K).
\end{equation}

The above lower bound can be substantially stronger than the determinant lower
bound for $\ell_\infty$ discrepancy. As a simple example, let $U \in \R^{2^n
\times n}$ be the matrix having a row for each vector in $\set{-1,1}^n$. Since
$U$ has rank $n$, the determinant lower bound is restricted to $k \times k$
matrices for $k \in [n]$. Hadamard's inequality implies for any $k \times k$
matrix $B$ with $\pm 1$ entries that $|\det(B)|^{1/k} \leq \sqrt{k} \leq
\sqrt{n}$. A moments thought however, reveals that for $x \in \R^n$,
$\norm{Ux}_\infty = \norm{x}_1$ and hence any coloring $x \in \set{-1,1}$ must
have discrepancy $\norm{x}_1 = n$. Using the previous logic, the volume lower
bound to the full system yields by standard estimates
\[
\vollb(U,B^m_\infty) \geq \vol_n(\set{x \in \R^n: \norm{x}_1 \leq 1})^{-1/n}
= \vol_n(B_1^n)^{-1/n} = (n!/2^n)^{1/n} \geq n/(2e), 
\]
which is essentially tight. 

\paragraph{\bf From Volume to Coloring.} The above example gives hope that the
volume lower bound can circumvent a dependency on the facet complexity of
the norm. Our first main result, shows that indeed this is the case:

\begin{restatable}[Tightness of the Volume Lower Bound]{theorem}{tightvollb} 
\label{thm:tightness}
For any $U \in \R^{n \times N}$ and symmetric convex body $K$ in $\R^n$, we have that
\begin{equation}
\label{eq:tightness}
\vollb^{\rm h}(U,K) \leq \hd(U,K) \leq O(\log n) \vollb^{\rm h}(U,K) \text{ ,}
\end{equation}
Furthermore, there exists a randomized polynomial time algorithm that computes a
coloring of $U$ with $K$-discrepancy $O(\log n \vollb^{\rm h}(U,K))$, given a
membership oracle for $K$. 
\end{restatable}

We note that the above immediately implies the corresponding approximate
tightness of the volume lower bound for vector balancing. The above bound can
also be shown to be tight. In particular, the counterexample to the
3-permutations conjecture from~\cite{NNN12}, which has $\ell_\infty$ discrepancy
$\Omega(\log n)$, can be shown to have volume lower bound $O(1)$. The
computations for this are somewhat technical, so we defer a detailed discussion
to the full version. As mentioned previously, an interesting property of the
volume lower bound is its invariance under taking convex hulls, namely $\vollb^{\rm
h}(\conv(\pm U),K) = \vollb^{\rm h}(U,K)$. In combination with
Theorem~\ref{thm:tightness}, this establishes the claimed
inequality~\ref{eq:hd-to-vb}. This invariance is proved in
section~\ref{sec:conv-hulls}, where we use a theorem of Ball~\cite{Ball88} to
show that the volume lower bound is essentially convex, and hence maximized at
extreme points.

Our proof of Theorem~\ref{thm:tightness} is algorithmic, and relies on iterated
applications of Rothvoss's partial coloring algorithm. We now explain our high
level strategy as well as the differences with respect to prior approaches. 

For simplicity of the presentation, we shall assume that $U=(e_1,\dots,e_n) \in
\R^{n \times n}$ and that the volume lower bound $\vollb^{\rm
h}((e_i)_{i=1}^n,K)=1$. This can be (approximately) achieved by applying a
standard reduction to the case where $U$ is non-singular, so $N \leq n$,
``folding'' $U$ into $K$, and appropriately guessing the volume lower bound (see
section~\ref{sec:tightness} for full details). 

For any subset $S \subseteq [n]$, let $K_S := \set{x \in K: x_i = 0, i \in [n]
\setminus S}$ denote the coordinate section of $K$ induced by $S$. Since the
vectors of $U$ now correspond to the coordinate basis, it is direct to verify that
\[
\vollb^{\rm h}((e_i)_{i=1}^n,K) = \max_{S \subseteq [n],k:=|S|}
\vol_k(K_S)^{-1/k} .
\]  
In particular, the assumption $\vollb^{\rm h}((e_i)_{i=1}^n,K) = 1$ implies that
\begin{equation}
\label{eq:big-volume}
\vol_{|S|}(K_S) \geq 1,~\forall S \subseteq [n]. 
\end{equation}
Under this condition, our goal can now be stated as finding a coloring $x \in
\set{-1,1}^n \in O(\log n) K$.

When $K$ is a symmetric polytope $|Ax| \leq 1$, with $m$ facets,
Bansal~\cite{Bansal10} uses a ``sticky'' random walk on the coordinates, where
the increments are computed via an SDP to guarantee that their variance along
any facet is at most $\hd((e_i)_{i=1}^n,K)^2$, while the variance along all
(active) coordinate directions is at least $1$ (i.e.~we want to hit cube
constraints faster). As this only gives probabilistic error guarantees for each
constraint in isolation, a union bound is used to get a global guarantee,
incurring the $O(\sqrt{\log m})$ dependence. 

To avoid the ``union bound'', we instead use Rothvoss's partial coloring
algorithm, which simply samples a random Gaussian vector $X \in \R^n$ and
computes the closest point in Euclidean distance $x$ to $X$ in $K \cap [-1,1]^n$
as the candidate partial coloring. As long as $K$ has ``large enough'' Gaussian
measure, Rothvoss shows that $x$ has at least
a constant fraction of its components at $\pm 1$. While this method can in
essence better leverage the geometry of $K$ than Bansal's method (in particular,
it does not need an explicit description of $K$), it is apriori unclear why Gaussian
measure should be large enough in the present context. 

Our main technical result is that if all the coordinate sections of
$K$ have volume at least $1$ (i.e.~condition~\ref{eq:big-volume}), then there indeed
exists a section of $K$ of dimension close to $n$, whose Gaussian measure is
``large'' after appropriately scaling. Specifically, we show that for any
$\delta \in (0,1)$, there exists a subspace $H$ of dimension $(1-\delta)n$ such
that the Gaussian measure of $2^{O(1/\delta)} (K \cap H)$ is at least
$2^{-\delta n}$ (see Theorem~\ref{thm:gauss-via-volume} for the exact
statement). We sketch the ideas in the next subsection.

The existence of a large section of $K$ with not too small Gaussian measure in
fact suffices to run Rothvoss's partial coloring algorithm (see
Theorem~\ref{thm:roth-giann}). Conveniently, one does not need to know the
section explicitly, as its existence is only used in the analysis of the
algorithm. Since condition~\ref{eq:big-volume} is hereditary, we can now
find partial colorings of $K$-discrepancy $O(1)$ on any subset of coordinates.
Thus, applying $O(\log n)$ partial coloring phases in the standard way yields
the desired full coloring. 

A useful restatement of the above is that Rothvoss's algorithm can always find
partial colorings with discrepancy $O(1)$ times the volume
lower bound. We note that this guarantee is a natural by-product of algorithm
(once one has guessed the appropriate scaling), which does not need to be
explicitly enforced as in Bansal's algorithm.

\paragraph{\bf Finding a section with large Gaussian measure.} We now sketch how
to find a section of $K$ of large Gaussian measure the assumption that
$\vol_{|S|}(K_S) \geq 1, \forall S \subseteq [n]$. The main tool we require is
the M-ellipsoid from convex geometry~\cite{Milman86-reverseBM}. The M-ellipsoid
$E$ of $K$ is an ellipsoid which approximates $K$ well from the perspective of
covering, that is $2^{O(n)}$ translates of $E$ suffice to cover $K$ and vice
versa. 

The main idea is to use the volumetric assumption to show that the largest
$(1-\delta)n$ axes of $E$, for $\delta \in (0,1)$ of our choice, have length at
least $\sqrt{n}2^{-O(1/\delta)}$, and then use the subspace generated by these
axes for the section of $K$ we use. On this subspace $H$, we have that a
$2^{O(1/\delta)}$ scaling of $E \cap H$ contains the $\sqrt{n}$ ball, and thus
by the covering estimate $2^{O(n)}$ translates of $2^{O(1/\delta)}(K \cap H)$
covers the $\sqrt{n}$ ball. Since the $\sqrt{n}$ ball on $H$ has Gaussian
measure at least $1/2$, the prior covering estimate indeed implies that
$2^{O(1/\delta)}(K \cap H)$ has Gaussian measure $2^{-O(n)}$, noting that
shifting $2^{O(1/\delta)}(K \cap H)$ away from the origin only reduces Gaussian
measure. Using an M-ellipsoid with appropriate regularity properties (see
Theorem~\ref{thm:reg-m}), one can scale $K \cap H$ by another $2^{O(1/\delta)}$
factor, so that the preceding argument yields Gaussian measure at least
$2^{-\delta n}$. 

We now explain why the axes of $E$ are indeed long. By the covering estimates,
for any $S \subseteq [n]$, $|S| = \delta n$, the sections $E_S$ and $K_S$ satisfy
\[
\vol_{\delta n}(E_S)^{1/{\delta n}} \geq 2^{-O(1/\delta)} \vol_{\delta
n}(K_S)^{1/{\delta n}} \geq 2^{-O(1/\delta)},
\]
where the last inequality is by assumption. Using a form of the
restricted invertibility principle for determinants, one can show that if all
coordinate sections of $E$ of dimension $\delta n$ have large volume, then so
does every section of $E$ of the same dimension.  Precisely, one gets that 
\[
\min_{\dim(W)=\delta n} \vol_{\delta n}(E \cap W)^{1/\delta n} \geq \binom{n}{\delta
n}^{-1/\delta n} \min_{|S|=\delta n} \vol_{\delta n}(E_S)^{1/\delta n} \geq
2^{O(-1/\delta)}.
\]
In particular, the above implies that the geometric average of the
\emph{shortest} $\delta n$ axes of $E$ (corresponding to the minimum volume
section above), must have length $\sqrt{n}2^{-O(1/\delta)}$ since the ball of
volume $1$ in dimension $\delta n$ has radius $\Omega(\sqrt{\delta n})$. But
then, the longest $(1-\delta) n$ axes all have have length
$\sqrt{n}2^{-O(1/\delta)}$. This completes the proof sketch.  

\paragraph{\bf The Discrepancy of Partial Colorings.} Our analysis of Rothvoss's
algorithm opens up the tantalizing possibility that it may indeed be optimal for
finding partial colorings in a hereditary sense. More precisely, we conjecture
that if when run on an instance $U$ with norm ball $K$, the algorithm almost
always produces partial colorings with $K$-discrepancy at least $D$, then there
exists a subset of $S$ of the columns of $U$ such that every partial coloring
of $U_S$ has discrepancy $\Omega(D)$. The starting point for this conjecture is
our upper bound of $O(1) \vollb^{\rm h}(U,K)$, on the discrepancy of the partial
colorings the algorithm computes. We now provide a purely geometric conjecture,
which would imply the above ``hereditary optimality'' for Rothvoss's algorithm. 

As in the last subsection, we may assume that $U=(e_1,\dots,e_n)$ is the
standard basis of $\R^n$ and that $\vollb((e_i)_{i=1}^n,K) = 1$. To prove the
conjecture, it suffices to show that exists some subset $S \subseteq [n]$ of
coordinates, such that all partial colorings have $K$-discrepancy $\Omega(1)$.
For concreteness, let us ask for partial colorings which color at least $|S|/2$
coordinates (the precise constant will not matter). For $x \in [-1,1]^n$, define
$\bnds(x) = \set{i \in [n]: x_i \in \set{-1,1}}$.  With this notation, our goal
is to find $S \subseteq [n]$, such that $\forall x \in [-1,1]^S$, $|\bnds(x)|
\geq |S|/2$, $\norm{\sum_{i \in S} x_i e_i}_K \geq \Omega(1)$.

We explain the candidate geometric obstruction to low discrepancy partial
colorings, which is a natural generalization of the so-called spectral lower
bound for $\ell_2$ discrepancy. Assume now that for some subset $S \subseteq
[n]$, we have that 
\begin{equation}
\label{eq:spec-obst-intro}
K_S \subseteq c \sqrt{|S|} B_2^S,
\end{equation}
where $B_2^S := (B_2^n)_S$, for some constant $c > 0$. Since any partial
coloring $x \in [-1,1]^S$, $|\bnds(x)|\geq |S|/2$, clearly has $\norm{x}_2 \geq
\sqrt{|S|/2}$, we must have that
\begin{equation}
\label{eq:spec-lb-intro}
\frac{1}{c\sqrt{2}} 
\leq \Bigl\|\sum_{i \in S} x_i e_i\Bigr\|_{c\sqrt{|S|}B_2^S} 
\leq \Bigl\|\sum_{i \in S} x_i e_i\Bigr\|_{K_S}.
\end{equation}
In particular, every partial coloring on $S$ has discrepancy at least $\tfrac{1}{c
\sqrt{2}} = \Omega(1)$, as desired.

Given the above, we may now reduce the conjecture to the following natural
geometric question:

\begin{restatable}[Restricted Invertibility for Convex
Bodies]{conjecture}{ripconvex} 
\label{conj:rip-convex-bodies}
There exists an absolute constant $c \geq 1$, such that for any $n \in \N$
and symmetric convex body $K \subseteq \R^n$ of volume at most $1$, there exists $S
\subseteq [n]$, $S \neq \emptyset$, such that $K_S \subseteq c\sqrt{|S|}B_2^S$. 
\end{restatable}

To see that this indeed implies the required statement, note that if
$\vollb((e_i)_{i=1}^n,K) = 1$, then by definition there exists $A \subseteq
[n]$, $|A| \geq 1$, such that $\vol_{|A|}(K_A) \leq 1$. Now applying the above
conjecture to $K_A$ yields the desired result. 

Two natural relaxations of the conjectures are to ask (1) does it hold for
ellipsoids and (2) does it hold for general sections instead of coordinate
sections? Our main evidence for this conjecture is that indeed both these
statements are true. We note that (1) indeed implies the optimality of
Rothvoss's partial coloring algorithm for $\ell_2$ discrepancy. Our results here
are slightly stronger than (1)+(2), as we in some sense manage to get ``halfway
there'' with coordinates sections, by working with the M-ellipsoid, and only for
the last step do we need to resort general sections. We note that the above
conjecture is closely related to the Bourgain-Tzafriri restricted invertibility
principle, and indeed our proof for ellipsoids reduces to it. We refer the
reader to section~\ref{sec:rip-convex} for further details and proofs.

\paragraph{\bf A Factorization Approach for Vector Balancing.}   
\dnote{TODO: talk about factorization approach}

As mentioned above, our approximate characterization of hereditary discrepancy
transfers seemlessly to the vector balancing setting (note that the
approximation bounds do not depend on $N$). We note that these characterizations
easily imply lower and upper bounds for $\vb(B_p^n,B_q^n)$ for any $p,q \in
[1,\infty]$, which are tight with an $O(\min \set{q,\sqrt{\log n}})$ factor.
While not novel, they perhaps illustrate the utility of the general approach.  

\subsection{Organization}

In section~\ref{sec:prelims} we present basic definitions and preliminary
material. In section~\ref{sec:tightness}, we present our proof of
Theorem~\ref{thm:tightness}. In subsection~\ref{sec:rip-convex}, we present our
partial progress on the restricted invertibility conjecture for convex bodies.
Lastly, in section~\ref{sec:factorization}, we present the proofs for our
factorization approach to vector balancing.

% \section{Definitions and Statement of the Volume Lower Bound}
% 
% Let $C$ and $K$ be two symmetric convex bodies in $\R^n$. We define the vector
% balancing constant of $C$ and $K$ by 
% \[
% \vb(C, K) \eqdef \sup\Bigg\{ 
% \min_{\eps_1, \ldots,\eps_N \in \{-1, 1\}} \Bigl\|\sum_{i = 1}^N \eps_i
% u_i \Bigr\|_{K}: N \in \mathbb{N}, u_1, \ldots, u_N \in C \Bigg\}.
% \]
% Given a sequence $(u_1, \ldots, u_N)$ of vectors in $\R^n$, we define the
% discrepancy and hereditary discrepancy with respect to $K$ as follows:
% \begin{align*}
% \disc((u_i)_{i = 1}^N,K) &\eqdef \min_{\eps_1, \ldots,\eps_N \in
%   \{-1, 1\}}
% \Bigl\|\sum_{i = 1}^N{\eps_i u_i}\Bigr\|_{K};\\
% \hd((u_i)_{i = 1}^N, K) &\eqdef \max_{S \subseteq  [N]}{\disc((u_i)_S, K)}.
% \end{align*}
% It follows trivially from the definitions that 
% \[
% \vb(C, K) = \sup\{\hd((u_i)_{i = 1}^N, K)\},
% \]
% where the supremum is taken over all finite sequence of vectors in $C$.

% For a non-singular matrix $A \in \R^{n \times k}$, we define $\detb(A) :=
% \det(A^\T A)^{1/2}$.  
% Let $G$ be the Gram matrix of $u_1, \ldots, u_N$, i.e. $g_{ij} =
% \langle u_i, u_j \rangle$, where $\langle \cdot, \cdot \rangle$ is
% the standard inner product on $\R^n$.  Banaszczyk~\cite{Bana93}
% established the following lower bound on $\beta((u_i)_{i = 1}^N, Y)$:
% \[
% \beta((u_i)_{i = 1}^N, Y) \ge \frac{\det(G)^{1/(2n)}}{\vol_n(B_Y)^{1/n}}.
% \]
% It is easy to see that this inequality is not always tight: for
% example, it is possible that $\beta((u_i)_{i = 1}^N, Y) > 0$, but the
% vectors $u_1, \ldots, u_N$ are not linearly independent, so $\det(G) =
% 0$. However, the inequality can be  strengthened by
% maximizing over subsequences of $(u_i)_{i = 1}^N$:
% \begin{equation}
%   \label{eq:vol-lb}
%   \beta((u_i)_{i = 1}^N, Y) \ge \vollb((u_i)_{i = 1}^N, Y) \eqdef
%   \max_{k = 1}^n \max_{S \subseteq [N]:|S| = k} \frac{\det(G_{S,S})^{1/(2k)}}{\vol_k(B_Y \cap \lspan\{u_i: i \in S\})^{1/k}}.
% \end{equation}
% Here $G_{S,S}$ is the principle submatrix of $G$ whose rows and
% columns are indexed by the set $S$.


\section{Preliminaries}
\label{sec:prelims}

In what follows, we use we will use $\inner{\cdot}{\cdot}$ for the standard
inner product on $\R^n$. Duality and traces are defined with respect to this
inner product. $K^\circ$ denotes the polar of a set $K \subseteq \R^n$. We use
$\vol_n(K)$ for the $n$-dimensional volume (standard Lebesgue measure in $\R^n$)
of $K$ and $\kappa_n$ for the volume $B_2^n$, the unit Euclidean ball. For a
symmetric convex body $K$, we define $\norm{x}_K = \min \set{s \geq 0: x \in sK}$
to be the norm induced by $K$.


For vectors $x,y \in \R^n$, we define $\inner{x}{y} = x^\T y$ to be the standard
inner product in $\R^n$. For a matrix $A \in \R^{n \times k}$, we will be
interested in the volume of the parallelepiped generated by the columns of $A$,
which can be computed by $\vol_k(A[0,1)^k) = \det(A^\T A)^{1/2}$. Banaszczyk
established the following volumetric lower bound on hereditary discrepancy:

We define $\gamma_n$ to be the standard Gaussian measure on $\R^n$, that is
$\gamma_n(A) = \frac{1}{\sqrt{2\pi}^n} \int_A e^{-\|x\|^2/2}$. We will often use
the $k$-dimensional Gaussian measure restricted to $k$-dimensional linear
subspace $H$ of $\R^n$, for which we use the notation $\gamma_H$.

For an $n \times n$ positive definite matrix $A$, we define the ellipsoid $E(A)
= \set{x \in \R^n: x^\T A x \leq 1}$. We recall that the polar ellipsoid
$E(A)^\circ = E(A^{-1})$, and that $\vol_n(E(A)) = \kappa_n \det(A)^{-1/2}$.

For a linear subspace $W \subseteq \R^n$, we denote the orthogonal projection
onto $W$ by $\pi_W$. For $S \subseteq [n]$, we write $\pi_S$ to denote the
projection onto the coordinate subspace $\lspan\{e_i: i \in S)\}$.

For a symmetric convex body $K \subseteq \R^n$ and subset $S \subseteq [n]$, we
denote the coordinate section of $K$ on $S$ by $K_S \eqdef \set{x \in K: x_i =
0, ~\forall i \notin S}$. We use $B_p^n$, $p \in [1,\infty]$, to denote the unit
$\ell_p$ ball in dimension $n$, $B_p^S := (B_p^n)_S$ and $B_p^W := (B_p^n) \cap W$
for corresponding coordinate and general sections. For a vector $x \in
[-1,1]^n$, we write $\bnds(x) = \set{i \in [n]: x_i \in \set{-1,1}}$.

\subsection{Proof of Volume Lower Bound}
\label{sec:proof-vollb}

\vollbstat*
\begin{proof}
For $S \subseteq [N]$, $|S| = k \in [n]$, let $C = \set{x \in \R^k: U_S x \in
K}$. It is direct to verify $\hd((e_i)_{i=1}^k,C) = \hd(U_S,K) \leq \hd(U,K)$,
where $(e_i)_{i=1}^k$ is the standard basis of $\R^k$. Thus, it suffices to
show that $\hd((e_i)_{i=1}^k,C) \geq \vol_k(C)^{-1/k}$. For $x \in \R^k$, $A
\subseteq \R^k$ finite, let $d(x,A) := \min_{a \in A} \norm{a-x}_C$
denote the minimum distance between $x$ and $A$ under the \\ (semi-)norm induced by
$C$. From here, we apply the standard reduction from linear discrepancy to
hereditary discrepancy~\cite{LSV}, to get
\begin{align*}
\max_{x \in [0,1]^k} d(x,\{0,1\}^k) &\leq \max_{x \in [0,1]^n}
d(x,\{0,\tfrac12,1\}^k) + \max_{x' \in \{0,\tfrac12,1\}} d(x',\set{0,1}^n) \\
&\leq \frac{1}{2} \max_{x \in [0,1]^k} d(x,\set{0,1}^k) + \frac{1}{2}
\hd((e_i)_{i=1}^k,C) \\
&\Rightarrow \\
\max_{x \in [0,1]^k} d(x,\{0,1\}^k) &\leq \hd((e_i)_{i=1}^k,C).
\end{align*}
Let $r = \hd((e_i)_{i=1}^k,C)$, we in particular have that $[0,1]^k \subseteq
\set{0,1}^k + rC$. Thus
\begin{align*}
\vol_k(rC) &\geq \vol_s(\cup_{x \in \set{0,1}^k} rC \cap (-x+[0,1)^k)) 
             = \vol_k(\cup_{x \in \set{0,1}^k} (rC+x) \cap [0,1)^k) \\
             &= \vol_k((\set{0,1}^k+rC) \cap [0,1)^k) 
              \geq \vol_k([0,1)^k) = 1 .
\end{align*}
In particular, $r \geq \vol_k(C)^{-1/k}$ as needed.
\end{proof}

\section{Tightness of the Volume Lower Bound}
\label{sec:tightness}

In this section, we will show that the volume lower bound \eqref{eq:vol-lb} is
tight within a logarithmic factor. 

\tightvollb*
% \begin{theorem}\label{thm:tightness}
% There exists a universal constant $C \geq 1$ such that the following holds. Let
% $K \subseteq \R^n$ be a convex body $\R^n$, and let $u_1, \ldots, u_N \in \R^n$.
% Then, the following holds:
% \begin{enumerate}
% \item $1 \le \frac{\hd((u_i)_{i = 1}^N, K)}{\vollb^{\rm h}((u_i)_{i = 1}^N, K)} 
%          \le C(1+\log n)$.\\
% \item $\hd((u_i)_{i = 1}^N, K) \leq C \sum_{k=1}^n \frac{1}{k} \vollb^{\rm
% h}_k((u_i)_{i=1}^N,K)$.
% \end{enumerate}
% Furthermore, there exists a polynomial time algorithm which can compute a
% coloring of $(u_i)_{i=1}^N$ of $K$-discrepancy at most $C(1 + \log n)\vollb^{\rm
% h}((u_i)_{i=1}^N,K)$. \dnote{Fix the proof to show the above.} 
% \end{theorem}

The main technical result of this section is that the volume lower bound,
restricted to subsets of size at least $\Omega(n)$, is in fact an upper bound on
the discrepancy of so-called partial colorings. This allows us to easily recover
Theorem~\ref{thm:tightness} using $O(\log n)$ partial coloring phases in the
standard way. We state our technical result below, restricted to the case where
the vectors are aligned with the standard basis. We note that since the output
norm is general, this is essentially without loss of generality. 

\begin{lemma}[Partial Colorings via Volume] \label{lem:partial-via-volume}
There exists a universal constants $C \geq 1, \eps_0 \in (0,1), \delta \in (0,1)$, such that
for any $y \in (-1,1)^n$ and symmetric convex body $K \subseteq \R^n$ satisfying
$\forall S \subseteq [n]$, $|S| = \ceil{\delta n}$, $\vol_{|S|}(K_S) \geq 1$, there
exists a polynomial time algorithm which with high probability finds $x \in
[-1,1]^n$ with $|\bnds(x)| \geq \ceil{\eps_0 n}$ and
$x-y \in C K$.  
\end{lemma}

We now give the straightforward reduction from Theorem~\ref{thm:tightness} to
Lemma~\ref{lem:partial-via-volume}.

\begin{proof}[Proof of Theorem~\ref{thm:tightness}]
By Lemma~\ref{lem:vol-lb}, we may restrict attention to the upper bound. In
particular, given $u_1,\dots,u_N \in \R^n$ and a convex body $K$ in $\R^n$, it
suffices to show that $\disc((u_i)_{i=1}^N) \leq C(1+\log n)\vollb^{\rm
h}((u_i)_{i=1}^N,K)$.  

To begin, we compute a basic solution to the linear program $\sum_{i=1}^N x_i
u_i = 0$, $x \in [-1,1]^N$. After relabeling, we may assume the variables not
hitting their $\set{-1,1}$ bounds are $x_1,\dots,x_l$, noting that if there are
no such variables we already have a $0$ discrepancy coloring. Since
$x$ is basic, we know that the vectors $u_1,\dots,u_l$ must be linearly
independent. Therefore, we may apply a invertible linear
transformation $T:\R^n \rightarrow \R^n$ sending $u_1,\dots,u_l$ to
$e_1,\dots,e_l$. In particular, letting $K' := TK$, we have that 
\begin{align*}
\disc((u_i)_{i=1}^N,K) &\leq \min_{z \in \set{-1,1}^l} \|\sum_{i=1}^l z_i e_i +
\sum_{i=l+1}^N x_i T u_i\|_{K'} \\
                &= \min_{z \in \set{-1,1}^l} \|\sum_{i=1}^l (z_i-x_i)e_i\|_{K'}, 
\end{align*}
Furthermore, a direct computation shows that
\[
\vollb^{\rm h}((u_i)_{i=1}^l,K) = \vollb^{\rm h}((e_i)_{i=1}^l,K') = \max_{S \subseteq [l]}
\vol_{|S|}(K'_S)^{-1/|S|} \text{ .}
\] 
Let us assume that we have computed $M > 0$ satisfying 
\[
M/2 \leq \vollb^{\rm h}((e_i)_{i=1}^l,K') \leq M. 
\]
From here, it suffices to compute $z \in
\set{-1,1}^l$ such that $\sum_{i=1}^l (z_i-x_i) e_i \in O(M\log n) K'$. Note
that by assumption on $M$, $\vol(MK'_S) \geq 1, \forall S \subseteq [l]$.
Therefore, repeatedly applying Lemma~\ref{lem:partial-via-volume} on $MK'$, letting $x^0 := x$ we compute a sequence
$x^1,\dots,x^T \in [-1,1]^l$, $T = \lceil \log l/\eps_0
\rceil = O(\log n)$, such that $\forall t \in [T]$, we have that

\begin{enumerate}
\item $\sum_{i=1}^l (x^t_i-x^{t-1}_i)e_i \in O(M) K'$.
\item $|\set{i \in [l]: x^t_i \in (-1,1)}| \leq (1-\eps_0)|\set{i \in [l]:
x^{t-1}_i \in (-1,1)}|$.
\end{enumerate}

By our choice of $T$, it is direct to check that $x^T \in \set{-1,1}^l$ and by
the triangle inequality that $\sum_{i=1}^l (x^T_i-x^0_i)e_i \in T M K' = O(M\log n) K'$. Thus, setting $z = x^T$ satisfies the requirements.

We now discuss the computation of $M$. We first note that 
\[
M_1 := \max_{i \in [l]} \vol_1(K'_i)^{-1} = \max_{i \in [l]} \|e_i\|_{K'} =
\max_{i \in [l]} \|u_i\|_K \text{ ,}
\]
and thus the restricted maximum can be efficiently computed. Note that by
construction ${\rm conv}(\pm e_1,\dots, \pm e_l)/M_1 \subseteq K'_{[l]}$. Thus,
for any $S \subseteq [l]$,$|S|=k$, we see that
\[
\vol_k(l M_1 K'_S) \geq \vol_{k}(l \cdot {\rm conv}(\pm e_i: i \in S)) = \frac{(2l)^k}{k!}
 \geq 1 \text{ .}
\]
In particular, we get that $M_1 \leq \vollb^{\rm h}((e_i)_{i=1}^l,K') \leq l M_1$.
Hence, as input to the stage above we may successively try the values $M_1
2^k$, $k \in \set{0,\dots,\log_2 l}$, stopping the first time we find a valid
coloring. 
\end{proof}

Lemma~\ref{lem:partial-via-volume} should be viewed as a volumetric analogue of
a theorem of Rothvoss~\cite{rothvoss-giann}, who both extended and made
algorithmic vector balancing results of Giannopoulos~\cite{giannop}. We state a
slight variant of~\cite[Lemma 9]{rothvoss-giann} below.

% \dnote{I
% would like to say something about how you probably can't get this type of result
% using the determinant lower bound. In particular, you lose a log there even for
% partial coloring in $\ell_\infty$. I think this is better save for the intro,
% where its easier to compare things.} 

\begin{theorem}[Partial Colorings via Gaussian Measure]\label{thm:roth-giann}
Let $0 < \eps \leq 1/60000$ and $\delta = \frac{3}{2}\eps \log_2
\frac{1}{\eps}$. Let $K \subseteq \R^n$ be a symmetric convex body and assume
that for some subspace $H \subseteq \R^n$ of dimension at least $(1-\delta)n$,
we have that $\gamma_H(K) \geq e^{-\delta n}$. Then for any $y \in (-1,1)^n$,
there exists a polynomial time algorithm which with high probability finds $x
\in [-1,1]^n$ satisfying $|\bnds(y)| \geq \eps n/2$ and $x-y \in K$.
\end{theorem}

We recall that Rothvoss' algorithm, for the special case $y = 0$ (the general
case is similar), works by computing the Euclidean projection of a Gaussian
random vector onto $K \cap [-1,1]^n$. The above statement deviates from the
corresponding Lemma in~\cite{rothvoss-giann} in that it does not assume that $K
\subseteq H$ or that $H$ is known to the algorithm. It is not hard to verify
however that this condition is not needed in analysis, so we defer discussion of
the proof of this statement to the full version. The flexibility gained by not
needing to know the subspace in advance will be very useful in the sequel. We
note that one can also adapt the analysis of the algorithm of Singh and
Eldan~\cite{ES14}, which maximizes over $K \cap [-1,1]^n$ using the Gaussian as
the objective vector instead of projecting it, to work in the above setting.   

Our proof of Lemma~\ref{lem:partial-via-volume} will in fact be a direct
reduction to Rothvoss' theorem. The core of our reduction is the following
geometric theorem, which shows that if all the coordinate sections of $K$ of
proportional dimension have large volume, then there exists a subspace $H$ of
proportional dimension on which $K$ has large Gaussian measure. 

\begin{theorem}[Gaussian Measure via Volume]
\label{thm:gauss-via-volume}
There exists a decreasing function $\eta: (0,1) \rightarrow \R_+$, $\eta(\delta)
= e^{O(1/\delta)}$, such that the following holds. For any $n \in \N$, $K
\subseteq \R^n$ symmetric convex body, $2 \leq k \leq n-1$, $\alpha = k/n$, such
that $\forall S \subseteq [n]$, $|S| = \alpha n$, $\vol_{\delta n}(K_S) \geq 1$,
there exists a linear subspace $H$ of dimension $(1-\delta)n$ for which
$\gamma_H(\eta(\delta) K) \geq e^{-\delta n}$.
\end{theorem}

We note that the above theorem is primarily interesting in the case where
$\delta$ is a fixed constant, as $\eta(\delta) = e^{O(1/\delta)}$ blows up quite
quickly as $\delta \rightarrow 0$. Lemma~\ref{lem:partial-via-volume} now
follows directly combining the above with Theorem~\ref{thm:roth-giann}, as shown
below.

\begin{proof}[Proof of Lemma~\ref{lem:partial-via-volume}] 
Let $\eps = 1/60000$ and $\delta = (3/2) \eps \log_2(1/\eps)$. By
Theorem~\ref{thm:gauss-via-volume}, $\eta(\delta) K$ satisfies the conditions
for applying Theorem~\ref{thm:roth-giann} on any $x \in (-1,1)^n$ with
`parameters $\eps$ and $\delta$ as in the last sentence. This yieds
Lemma~\ref{lem:partial-via-volume} with $\eps_0 = \eps/2$, $\delta = \delta$ and
$C = O(\eta(\delta)) = O(1)$, as needed.   
\end{proof}

While there are examples where the volume lower bound is a $O(\log n)$ factor
off from hereditary discrepancy (e.g.~$3$ permutations), we conjecture that the
volume lower bound actually characterizes a hereditary version of partial
coloring discrepancy. Namely, if the volume lower bound is $D$, we conjecture
that there exists a subset of vectors for which every (fractional) partial coloring
has discrepancy $\Omega(D)$. Recall that Lemma~\ref{lem:partial-via-volume}
gives the other direction, i.e.~that there always exist partial colorings of
discrepancy $O(D)$. If this conjecture were true, then Rothvoss' algorithm,
which we use as a blackbox, would in a weak sense be optimal for finding
partial colorings. We discuss this conjecture in more detail in the next
subsection. 

Comparing to prior works, Theorem~\ref{thm:gauss-via-volume} provides a useful
and different route for proving that a body (or at least a large section of it)
has exponentially small Gaussian measure. In the context of discrepancy, to the
authors' knowledge, only two main techniques were used to prove such bounds,
neither of which is directly applicable in the above setting. The first
technique consists of combining chaining techniques and moment bounds, which can
generally only measure when the body has Gaussian measure close to $1/2$.  This
approach loses the leverage we have in only needing exponentially small bounds
and thus often incurs additional logarithmic factors. The second strategy is
based on the positive correlation properties of Gaussian measure, first proved
for the intersection of symmetric slabs (i.e.~Sidak's lemma), and with the
recent resolution of the Gaussian correlation conjecture~\cite{Royen14}, for the
intersection of arbitrary symmetric convex bodies. More precisely, one tries to
show that $K$ contains (or equals) the intersection of ``simpler'' symmetric
convex bodies $K_1,\dots,K_T$ (most often slabs) to deduce that $\gamma_n(K)
\geq \prod_{i=1}^T \gamma_n(K_i)$, from which one can usefully get exponentially
small bounds.

In contrast, our proof of the $e^{-\delta n}$ lower bounds on Gaussian measure
in the above theorem proceeds via a direct covering argument. Namely, if one can
show that $e^{\delta n-1}$ translates of $K$ cover the Euclidean ball of radius
$\sqrt{n}$, which has Gaussian measure $\geq 1/2$, then one can directly deduce
that 
\[
1 \leq 2\gamma_n(\sqrt{n}B_2^n) \leq 2N(\sqrt{n} B_2^n,K) \max_{t \in \R^n}
\gamma_n(K+t) \leq e^{\delta n} \gamma_n(K) ,
\]
where we have used that $\max_{t \in \R^n} \gamma_n(K+t) = \gamma_n(K)$ for a
centrally symmetric convex body, which follows by the symmetry and logconcavity
of Gaussian measure. We will adopt this strategy on a section of $K$, which is
chosen to align with the longest axes of a so-called regular M-ellipsoid for
$K$. The volumetric condition in Lemma~\ref{lem:partial-via-volume} will in fact
be used to guarantee that these axes have length $\Omega(\sqrt{n})$, which makes
the above strategy plausible. We recall that an M-ellipsoid~ $E$ of $K$ is an
ellipsoid which approximates $K$ well from the perspective of covering,
i.e.~$2^{O(n)}$ shifts of $K$ suffice to cover $E$ and vice versa. The existence
of such ellipsoids was first proven by Milman~\cite{Milman86-reverseBM}. We give
precise definitions below. 

For any two sets $A,B \subseteq \R^n$, let 
\[
N(A,B) = \min \set{|\Lambda|: \Lambda \subset \R^n, A \subseteq \Lambda+B} ,
\]
denote the minimum number of shifts of $B$ needed to cover $A$. The following
theorem of Pisier~\cite{Pisier-book}, gives the existence of M-ellipsoids whose
covering estimates have polynomial decay. The decay estimate will be used to
make the Gaussian measure of the large section of $K$ we find as close to $1$ as
we like after a sufficient scaling. 

\begin{theorem}[Regular M-ellipsoid]
\label{thm:reg-m}
There exists an absolute constant $c_0 > 0$, such that any $0 < \alpha < 2$,
letting $\sigma(\alpha) = c_0(2-\alpha)^{-1/2}$, $n \in \N$, and symmetric
convex body $K \subseteq \R^n$, there exists an ellipsoid $E \subseteq \R^n$,
$\vol_n(E)=\vol_n(K)$, such that for all $t \geq 1$
\[
\max \set{N(K,tE),N(E,tK),N(K^\circ,tE^\circ),N(E^\circ,tK^\circ)} \leq
e^{\sigma(\alpha) n / t^\alpha} \text{ .}
\]
Such an ellipsoid will be referred to as an $\alpha$-regular M-ellipsoid.
\end{theorem}

To relate volumes of sections of $K$ to corresponding projection of $K^\circ$,
we will need the following well-known inequality:

\begin{theorem}[Blashke-Santal{\'o}]\label{thm:santalo} 
Let $K \subseteq \R^n$ be a symmetric convex body. Then $\vol_n(K)\cdot
\vol_n(K^\circ) \leq \kappa_n^2$, where equality holds if and only if $K$ is
an origin centered ellipsoid. Here $\kappa_n = \vol_n(B_2^n)$.
\end{theorem}


To show that the axes of the M-ellipsoid of $K$ are long, we will need to relate
the axis lengths to the volumes of coordinate projections of the polar
ellipsoid. For this, purpose we will require the following formula for
coordinate projection volumes. 

\begin{lemma}\label{lem:ellipsoid-volumes}
Let $E := E(Q) \subseteq \R^n$ be an origin center ellipsoid. Then, for any
$S \subseteq [n]$, $|S| = k$, we have that 
\[
\vol_k(\pi_S(E^\circ)) = \kappa_k \det(Q_{S,S})^{1/2} \text{ .}
\]
\end{lemma}
\begin{proof}
Recall that $E^\circ = E(Q^{-1}) = Q^{1/2} B_2^n$, where $Q^{1/2}$ is the
positive definite square root of $Q$. To begin, we recall that the support
function of $E^\circ$ can be computed by
\begin{align*}
h_{E^\circ}(w) &\eqdef \max_{y \in E^\circ} \inner{w}{y} 
               = \max_{z \in B_2^n} \inner{w}{Q^{1/2} z} 
               = \|Q^{1/2} w\| = \sqrt{w^\T Q w}.
\end{align*}
Let $W_S = \lspan\{e_i: i \in S\}$. Note that by construction, $\pi_S(E^\circ)
\subseteq W_S$ and $h_{\pi_S(E^\circ)}(w) = h_{E}(w)$, $\forall w \in W_S$.
Furthermore, by duality, among convex bodies these conditions uniquely define $\pi_S(E^\circ)$. 

Let $s_1 < s_2 < \dots < s_k$ be the elements of $S$ and let $P_S =
(e_{s_1},\dots,e_{s_k})$, noting that $P_S P_S^\T = \pi_S$. Let $T = P_S
(Q_{S,S})^{1/2} \in \R^{n \times k}$. We now show that $TB_2^k = \pi_S(E^\circ)$
using the aforementioned conditions. Clearly $TB_2^k \subseteq W_S$ since
$\lspan(P_S) = W_S$. For $w \in W_S$, letting $w_S = (w_{s_1},\dots,w_{s_k})^\T$
denote the restriction to the coordinates in $S$, we have that 
\begin{align*}
h_{TB_2^k}(w) &= \max_{z \in B_2^k} \inner{T^\T w}{z} 
              = \max_{z \in B_2^k} \inner{(Q_{S,S})^{1/2} w_S}{z} \\ 
              &= \sqrt{w_S^\T Q_{S,S} w_S} = \sqrt{w^\T Q w} ,
\end{align*} 
where the last equality follows since $w_i = 0$ for $i \notin S$. Thus
$\pi_S(E^\circ) = TB_2^k$ as claimed. The volume can now be computed as follows:
\begin{align*}
\vol_k(TB_2^k) &= \kappa_k \det(T^\T T)^{1/2} 
               = \kappa_k \det((Q_{S,S})^{1/2} (P_S^\T P_S)
(Q_{S,S})^{1/2})^{1/2} \\
               &= \kappa_k \det(Q_{S,S})^{1/2} \text{ ,}
\end{align*}
as needed.
\end{proof}

The next lemma we will need is a simple determinantal analogue of the restricted
invertibility principle.

\begin{lemma}\label{lm:rip-det}
  Let $Q$ be an $n\times n$ real positive semi-definite matrix with
  eigenvalues $\lambda_1 \ge \ldots \ge \lambda_n$. For any integer
  $k$, $1 \le k \le n$, there exists a set $S \subseteq [n]$ of size $k$
  such that
  \[\prod_{i=1}^k \lambda_i \leq \binom{n}{k} \det(Q_{S,S}).\]
\end{lemma}
\begin{proof}
To prove the lemma, we will rely on the classical identity for applying the
elementary symmetric polynomials to the eigen values of $Q$:
\begin{equation*}
\sum_{S \in [n],|S|=k} \prod_{i \in S} \lambda_i \eqdef p_k(\lambda) = \sum_{S \subset [n]: |S| = k}\det(Q_{S,S}).
\end{equation*}
To verify this equation, consider the coefficient of $t^{n-k}$ in the polynomial
$\det(Q + tI)$. Calculating the coefficient using the Leibniz formula for the
determinant gives the right hand side; calculating it using $\det(Q + tI) =
(\lambda_1 + t)\ldots(\lambda_n + t)$ gives the left hand side. Since the eigen
values are all non-negative, we get that
\[
\prod_{i=1}^k \lambda_i \leq p_k(\lambda) =  
 \sum_{S \subseteq [n]: |S|=k} \det(Q_{S,S}) \leq \binom{n}{k} \max_{S
\subseteq [n]: |S|=k} \det(Q_{S,S}),
\]
as needed.
\end{proof}

We now have all the ingredients needed to prove our main geometric estimate.

\begin{proof}[Proof of Theorem~\ref{thm:gauss-via-volume}]
Let $E := E(Q)$ denote a $1$-regular M-ellipsoid for $K$ and let $\sigma :=
\sigma(1)$. Let $l_1 \geq \dots \geq l_n > 0$ denote
the length of the principal axes of $E$, where we recall that
$l_n^{-2}\geq \dots \geq l_1^{-2} > 0$ are then the eigen values of $Q$.

By the Blashke-Santal{\'o} inequality, for all $S \subseteq
[n]$, $|S|=\delta n$, we have that 
\[
\vol_{\delta n}(K_S)\vol_{\delta n}((K_S)^\circ) = 
\vol_{\delta n}(K_S) \vol_{\delta n}(\pi_S(K^\circ)) \leq
\kappa_{\delta n}^2 \text{ .}
\]
Since we assume $\vol_{\delta n}(K_S) \geq 1$, the above implies that
$\vol_{\delta n}(\pi_S(K^\circ)) \leq \kappa_{\delta n}^2$. The
coordinate projections of $E^\circ$ thus have volume at most 
\[
\vol_{\delta n}(\pi_S(E^\circ)) \leq N(E^\circ,K^\circ)
\vol_{ \delta n}(\pi_S(K^\circ)) \leq e^{\sigma n} \kappa_{\delta n}^2 \text{ .}
\]
Combining Lemma~\ref{lm:rip-det} and~\ref{lem:ellipsoid-volumes}, we have that
\begin{align*}
\prod_{i=(1-\delta)n+1}^n l_i^{-1} &\leq \binom{n}{\delta n}^{1/2} \max_{S \subseteq
[n],|S|=\delta n} \det(Q_{S,S})^{1/2} \\
&= \binom{n}{\delta n}^{1/2} \max_{S \subseteq [n],|S|=\delta n}
\vol_{\delta n}(\pi_S(E^\circ)) \kappa_{\delta n}^{-1} \\
&\leq \binom{n}{\delta n}^{1/2} e^{\sigma n} \kappa_{\delta n} \text{ .}
\end{align*}
From here, we conclude that 
\[
l_{(1-\delta)n} \geq \prod_{i=(1-\delta)n+1}^n l_i^{\frac{1}{\delta
n}} \geq \binom{n}{\delta 
n}^{-\frac{1}{2\delta n}} e^{-\frac{\sigma}{\delta }} \kappa_{\delta
n}^{-\frac{1}{\delta n}} \geq 
\frac{1}{e^{2\frac{\sigma}{\delta }}} \cdot \sqrt{\frac{\delta n}{2\pi e}} :=
c(\delta)^{-1} \sqrt{n} \text{ .}
\] 
Letting $H$ be the span of the first $(1-\delta)n$ principal
axes of $E$, we thus conclude that 
\[
\sqrt{n} (B_2^n \cap H) \subseteq c(\delta) (E \cap H). 
\]
Using the $1$-regularity of $E$, letting $t = 2 \sigma / \delta$, we derive the
following covering estimate 
\begin{align*}
N(\sqrt{n} (B_2^n \cap H), 2 t c(\delta) (K \cap H))
&\leq N(c(\delta)(E \cap H), 2 t c(\delta) (K \cap H)) \\
&\leq N(E, t K) \leq e^{\sigma n / t} = e^{\delta n/2} \text{ .}
\end{align*}
Since $\gamma_H(\sqrt{n} B_2^n \cap H) \geq 1/2$, setting $\eta := \eta(\delta)
=\frac{2c(\delta)\sigma}{\delta} = e^{O(1/\delta)}$, we get that $\gamma_H(\eta K
\cap H) \geq \frac{1}{2} e^{-\delta n/2} \geq e^{-\delta n}$, as needed. Lastly,
$\eta$ as defined above is easily checked to be decreasing in $\delta$.
\end{proof}

\subsection{The Discrepancy of Partial Colorings}
\label{sec:rip-convex}

In this section we discuss a geometric conjecture which would imply that a tight
relationship between the discrepancy of partial colorings and the volume lower
bound, and thus a weak form of optimality for Rothvoss' partial coloring
algorithm. For this purpose, we formally define the partial coloring discrepancy
as well as its hereditary version. Given $(u_i)_{i=1}^N \in \R^n$, symmetric
convex body $K \subseteq \R^n$ and $\alpha \in (0,1]$,  we define  
\begin{equation}
\label{def:partial-disc}
\begin{split}
&\disc_\alpha(( u_i)_{i=1}^N,K) \eqdef \min_{\substack{x \in [-1,1]^N \\
|\bnds(x)| \geq \alpha N}} \Bigl\|\sum_{i=1}^N x_i u_i\Bigr\|_K .
\end{split}
\end{equation}
and
\begin{equation}
\label{def:partial-hd}
\hd_\alpha((u_i)_{i=1}^N,K) \eqdef \max_{S \subseteq [N]} \disc_\alpha((u_i)_{i \in S}, K) .
\end{equation}

We recall that (repeated applications) of Lemma~\ref{lem:partial-via-volume}
implies the upper bound
\[
\hd_{1/2}((u_i)_{i=1}^N,K) \leq O(1) \vollb^{\rm h}((u_i)_{i=1}^N,K).
\]
Here we conjecture that the reverse inequality should also hold.

\begin{conjecture}
\label{conj:partial-volume}
There exists a universal constant $c \geq 1$, such that for any $n \in \N$,
$u_1,\dots,u_n \in \R^n$ linearly independent and symmetric convex body $K
\subseteq \R^n$: 
\begin{equation}
\vollb^{\rm h}((u_i)_{i=1}^n,K) \leq c \hd_{1/2}((u_i)_{i=1}^n,K)
\end{equation}
\end{conjecture}

Note that we restrict above to linear independent subsets of vectors, but as is
well-known (e.g.~see proof of Theorem~\ref{thm:tightness}), this is without loss
of generality. As a pathway to prove the conjecture, we suggest the following
natural geometric analog of the so-called spectral lower bound for discrepancy
into $\ell_2$.

\begin{lemma}
\label{lem:speclb}
Let $(u_i)_{i=1}^n \in \R^n$ be linearly independent, $K \subseteq \R^n$ be a
symmetric convex body, and $\alpha \in [0,1]$. For any subset $S \subseteq [n]$,
$|S|=k$, letting $W_S := \lspan\{u_i: i \in S\}$, define
\[
\speclb((u_i)_{i \in S},K) := 
\max \set{r \geq 0: r K \cap W_S \subseteq k \conv(\pm u_i: i \in S)}.
\]
Then, we have that
\begin{equation}
\label{eq:spec-lb}
\disc_\alpha((u_i)_{i \in S},K) \geq \alpha \speclb((u_i)_{i \in S},K).
\end{equation}
In particular, defining
\[
\speclb^{\rm h}((u_i)_{i=1}^n,K) := \max_{S \subseteq [n]} \speclb((u_i)_{i \in
S},K)
\]
we have that
\begin{equation}
\label{eq:hd-spec-lb}
\hd_{\alpha}((u_i)_{i=1}^n,K) \geq \alpha \speclb^{\rm h}((u_i)_{i=1}^n,K).
\end{equation}
\end{lemma}
\begin{proof}
We prove only~\eqref{eq:spec-lb} since then \eqref{eq:hd-spec-lb} follows
trivially. For~\eqref{eq:spec-lb}, by replacing $K$ by $K \cap W_S$, we may wlog
assume that $|S|=n$ and $W_S = \R^n$. 

Let $x \in [-1,1]^n$, $|\bnds(x)| \geq \alpha n$, and $u_x = \sum_{i=1}^n x_i
u_i$. Our goal is to show that $\beta := \norm{u_x}_K \geq \alpha r$, where $r
:= \speclb((u_i)_{i=1}^n,K)$.

Let $(u_i^*)_{i=1}^n$ denote the corresponding dual basis of
$(u_i)_{i=1}^n$, i.e.~satisfying $\inner{u_i^*}{u_j} = 1$ if $i=j$ and $0$
otherwise, which exists by linear independence. Now letting $v_x =
\sum_{i=1}^n {\rm sign}(x_i) u_i^*$, it is easy to check that 
\[
\inner{v_x}{u_x} = \sum_{i=1}^n |x_i| \geq |\bnds(x)| \geq \alpha n .
\]
Since $u_x \in \beta K$ and $r K \subseteq n \conv(\pm u_i: i
\in [n])$, we have that
\begin{align*}
\alpha n \leq \beta \max_{z \in K} \inner{v_x}{z} 
         \leq \frac{\beta}{r} n 
               \max_{z \in \conv(\pm u_i: i \in [n])} \inner{v_x}{z}
          = \frac{\beta}{r} n .
\end{align*}
The desired inequality now follows by rearranging.
\end{proof}

We note that as with $\vollb^{\rm h}$, one may extend the $\speclb^{\rm h}$ to
an arbitrary sequence of vectors $(u_i)_{i=1}^N$, however one must take care to
optimize only over subsets of linearly independent vectors, since otherwise the
conclusion of Lemma~\ref{lem:speclb} is false.

Given the above, it suffices to prove Conjecture~\ref{conj:partial-volume} with
$\hd_{1/2}$ replaced by $\speclb^{\rm h}$. The resulting stronger conjecture has a
very natural geometric interpretation which we expand on below. For
$(u_i)_{i=1}^n \in \R^n$ linearly independent and $K \subseteq \R^n$ a symmetric
convex body, letting $T$ denote the linear map sending $(u_i)_{i=1}^n$ to
$(e_i)_{i=1}^n$, it is direct to check that $\tau((u_i)_{i=1}^n,K) =
\tau((e_i)_{i=1}^n,TK)$ for $\tau \in \set{\speclb^{\rm h},\vollb^{\rm h}}$.
Thus, for the purpose of the conjecture, it suffices to consider the setting
where the vectors are the standard basis. In this setting, we see that
\[
\speclb^{\rm h}((e_i)_{i=1}^n,K) = \max_{S \subseteq [n]} \max \set{r \geq
0: r K_S \subseteq |S| B_1^S} 
\]
and that 
\[
\vollb^{\rm h}((e_i)_{i=1}^n,K) = \max_{S \subseteq [n]}
\vol_{|S|}(K_S)^{-1/|S|} \text{ .} 
\]
The goal is now to show that for every $S_0 \subseteq [n]$, there exists $S_1
\subseteq [n]$, such that 
\begin{equation}
\label{eq:conj-simple}
\vol_{|S_0|}(K_{S_0})^{-1/|S_0|} \leq c \max \set{r \geq 0: r K_{S_1} \subseteq
|S_1| B_1^{|S_1|}} \text{ .}
\end{equation}
Since this must hold for every symmetric convex body $K$, we may assume that
$S_0 = [n]$ (and thus $S_1 \subseteq S_0$). Furthermore, by homogeneity, we may
also assume that $\vol_n(K)=1$. In this case~\eqref{eq:conj-simple}, and hence
Conjecture~\ref{conj:partial-volume}, directly reduces to the following
geometric conjecture.

\begin{conjecture}[Restricted Invertibility Principle for Convex Bodies]
\label{conj:conv-restr-iso} There exists an absolute constant $c \geq 1$, such
for any $n \in \N$, symmetric convex body $K \subseteq \R^n$ of volume $1$,
there exists $S \subseteq [n]$ such that $K_S \subseteq c |S| B_1^S$.
\end{conjecture}

Another natural, and again stronger conjecture, would be to ask for containment
inside $c \sqrt{|S|} B_2^S \subseteq c |S| B_1^S$. We suspect that this version
should also hold, and indeed, our evidence for the conjecture supports this
belief. 

Two natural weakenings of Conjecture~\ref{conj:conv-restr-iso} are to ask
whether (a) it holds for ellipsoids and (b) whether it holds for general bodies
but with coordinate sections replaced by arbitrary sections. As our main
evidence for the conjecture, we show that both assertions indeed hold with the
stronger containment relation with respect to $B_2^n$. We note that (a) indeed
implies Conjecture~\ref{conj:partial-volume} when $K$ is an ellipsoid.  

Before formally stating our partial results, we give a last simplification of
Conjecture~\ref{conj:conv-restr-iso}. In particular, when attempting to prove
the conjecture we may additionally assume that $\vol_{|S|}(K_S) \geq 1$ for all
$S \subset [n]$. This follows by noting that if there exists $S \subset [n]$
with $\vol_{|S|}(K_S) < 1$, we may simply apply induction on $K_S$, after
scaling it up to have volume $1$ (which only makes the task more difficult). The
main two lemmas we can prove, from which the above results easily follow,
are stated below. 

\begin{lemma}
\label{lem:axis-m-ell}
Let $K \subseteq \R^n$ be a symmetric convex body of volume $1$ satisfying
for all $S \subseteq [n]$, $\vol_{|S|}(K_S) \geq 1$. Let $E \subseteq \R^n$ be a
$1$-regular M-ellipsoid of $K$. Then there exists $S \subseteq [n]$, $|S| =
\Theta(n)$, such that $E_S \subseteq O(\sqrt{|S|}) B_2^S$.
\end{lemma}

\begin{lemma}
\label{lem:cover-to-section}
Let $K \subseteq \R^n$ be a symmetric convex body such that $N(K,\sqrt{n}B_2^n)
\leq 2^{O(n)}$. Then there exists a linear subspace $W \subseteq \R^n$, $\dim(W) =
\Theta(n)$, such that $K \cap W \subseteq O(\sqrt{|W|}) B_2^W$.  
\end{lemma}

To derive the conjecture when $K$ is an ellipsoid, we simply apply
Lemma~\ref{lem:axis-m-ell} with $E=K$. To derive the conjecture for general
sections, we first apply Lemma~\ref{lem:axis-m-ell} to $K$, noting that the
produced section $K_S$ now satisfies the conditions of
Lemma~\ref{lem:cover-to-section}, from which we derive the result.  

For the proof of Lemma~\ref{lem:axis-m-ell}, we use the volumetric conditions to
show that the central axes of $E$ all have length $\Theta(\sqrt{n})$,
complementing the $\Omega(\sqrt{n})$ lower bound used in the proof of
Theorem~\ref{thm:gauss-via-volume} under the additional assumption that
$\vol_n(K)=1$, and derive the existence of the
corresponding section from the Bourgain-Tzafriri restricted invertibility principle
applied to the quadratic form defining $E$. The proof of
Lemma~\ref{lem:cover-to-section} follows relatively directly from Milman's
quotient of subspace (QS) theorem, which states that there exists a projection
of a section of $K$ of proportional dimension which is constant factor
isomorphic to a Euclidean ball. We remark that the main step that seems to be
missing for the proof of Conjecture~\ref{conj:conv-restr-iso} is a
corresponding ``coordinate version'' of the QS theorem. As the proofs of the
above lemmas require a fair amount of machinery, and only yield partial evidence
for a conjecture, we defer detailed proofs to the full version.  

\section{The Factorization Approach}
\label{sec:factorization}

In the inductive proof of Theorem~\ref{thm:tightness} we need to keep
alternating between computing Gaussian estimates based on volume
information, and applying Theorem~\ref{thm:roth-giann}, as we induct
on lower dimensional subspaces. One downside of this indirect nature
of our proof is that it does not lead to an algorithm for computing
the vector balancing constant $\vb(C,K)$ of two symmetric convex
bodies $C$ and $K$, or of the hereditary discrepancy $\hd((u_i)_{i =
  1}^N, K)$. In this section, we explore a different approach, which
aims at separating computing a Gaussian estimate from volume
information, on one hand, and the vector balancing argument on the
other. The approach is based on recasting vector balancing in the
language of linear operators between normed spaces, and on factoring
such operators through Euclidean space. This factorization strategy is
a significant generalization of the connection between the $\gamma_2$
norm and hereditary discrepancy used in~\cite{disc-gamma2}.

Let $U:X \to Y$ be a linear operator between two $n$-dimensional
normed spaces $(X, \| \cdot\|_X)$ and $(Y, \| \cdot\|_Y)$. A natural
way to define the vector balancing constant of $U$ is
\[
\vb(U) = \sup\left\{
  \min_{\eps_1, \ldots, \eps_N \in \{-1, 1\}} 
  \biggl\|\sum_{i = 1}^N \eps_i U(x_i)\biggr\|_Y:
  N \in \mathbb{N}, x_1, \ldots, x_N \in B_X\right\}
\]
where $B_X = \{x: \|x\|_X \le 1\}$ is the unit ball of $X$. If $C$ and
$K$ are two centrally symmetric convex bodies in $\R^n$, and we define
the corresponding normed spaces $X_C = (\R^n, \|\cdot\|_C)$ and $X_K =
(\R^n, \|\cdot\|_K)$, then the vector balancing constant $\vb(I)$ of
the formal identity operator $I:X_C \to X_K$ recovers $\vb(C, K)$.

We will relate $\vb(U)$ to an efficiently computable quantity
$\lambda(U)$, which will allow us to efficiently approximate $\vb(U)$
(given appropriate access to the domain and range $X$ and
$Y$). Moreover, we will see that for the operator $U:\ell_1^N \to X_K$
given by $Ue_i = u_i$, $\lambda(U)$ also approximates $\hd((u_i)_{i =
  1}^N, K)$. Once again our main tool for giving lower bounds on
$\vb(U)$ is the volume lower bound, which we reformulate in the
operator setting.

\begin{lemma}
\label{lem:vol-lb-oper}
Let $(X, \|\cdot\|_X)$ and $(Y, \|\cdot\|_Y)$ be two $n$-dimensional
normed spaces, and let $U:X \to Y$ be an invertible linear
operator. Define
\[
\vollb_k^{\rm h}(U) \eqdef
\sup\bigl\{\vol_k\bigl(\set{a \in \R^k: \|UV a\|_Y \le 1}\bigr)^{-1/k}\bigr\},
\]
where the supremum is over all operators $V:\ell_1^k \to X$ of
operator norm $1$ and rank $k$. Then, letting
\[
\vollb^{\rm h}(U) \eqdef \max_{k \in [n]} \vollb_k^{\rm h}(U),
\]
we have that
\[
\vb(U) \geq \vollb^{\rm h}(U).
\]
\end{lemma}
\begin{proof}
  The lemma is just a reformulation of Lemma~\ref{lem:vol-lb} in the
  language of linear operators. Because 
  \[
  \max_{i = 1}^k{\|Ve_i\|_X} = \|V\| = 1,
  \]
  the points $u_1, \ldots, u_k$ defined by $u_i = V e_i$ lie in $B_X$,
  where $e_i$ is the $i$-th standard basis vector of $X$. Then the
  lemma follows directly from Lemma~\ref{lem:vol-lb} with $C = B_X$
  and $K = \{x \in X: Ux \in B_Y\}$. 
\end{proof}

\subsection{Preliminaries}

\snote{Put all prelims in one place, Schur convexity}

To every finite dimensional normed space $(X, \|\cdot\|)$ over $\R$ we
associate its dual space $(X^*, \|\cdot\|_*)$ defined over linear maps
from $X$ to $\R$ (i.e.~linear functionals) with the dual norm
$\|x^*\|_* = \sup\{\langle x, x^*\rangle: \|x\|_X \le 1\}$. Here the
notation $\langle x, x^*\rangle$ means ``the functional $x^*$ applied
to $x$'', i.e.~$x^*(x)$. For any finite dimensional space $X$ we have
$X^{**} = X$. Any vector $y \in \R^n$ gives a linear functional over
$\ell_2^n$ via the standard inner product, i.e. $\langle x, y \rangle
= \sum_{i = 1}^n{x_i y_i}$, and by the Cauchy-Schwarz inequality this
linear functional has norm equal to $\|y\|_2$. For this reason, we can
identify $(\ell_2^n)^*$ with $\ell_2^n$, and for $x, y \in \R^n$
identify $\langle x , y \rangle$ with the standard inner product.

As usual, given an operator $A:X \to Y$, we define its dual operator
$A^*:Y^* \to X^*$ on every $y^* \in Y^*$ by $\langle x ,
A^*y^*\rangle = \langle Ax, y^*\rangle\ \forall x \in X$. We use the
shorthand $A^{-*}$ for $(A^{*})^{-1} = (A^{-1})^*$. 

We will use the tensor product notation $x^* \otimes y$ for the rank-1
linear operator from a normed space $X$ to a normed space $Y$, defined
by $(x^* \otimes y)(x) = \langle x, x^*\rangle y$, where $x^* \in X^*$
and $y \in Y$. Note that if $X$ and $Y$ are normed spaces over $\R^n$,
the matrix of $x^* \otimes y$ with respect to the standard basis is
$yx^\T$. Any linear operator $A:X \to X$ on an $n$-dimensional normed
space $X$ can be written as as sum of rank-1 operators $A = \sum_{i =
  1}^n{x^*_i \otimes y}$ for $x_1^*, \ldots, x_n^* \in X^*$ and $y_1,
\ldots, y_n \in X$. The trace of $A$ is then defined by
\[
\tr(A) \eqdef \sum_{j = 1}^n{\langle y_j, x^*_j \rangle}.
\]
This abstract definition agrees with the usual one, i.e.~if the matrix
of $A$ with respect to a basis of $X$ and the corresponding dual basis
of $X^*$ is $M$, then $\tr(A) = \tr(M)$. This also shows that $\tr(A)$
is uniquely defined, independent of how we write $A$ as a sum of
rank-1 operators. We will identify linear functionals on operators
$A:X \to Y$, where $X$ and $Y$ are $n$-dimensional, with operators
$B:Y \to X$ via $f_B(A) = \tr(BA)$.

A linear operator $A: X \to X^*$ on normed space $X$ defines a
bilinear form $B$ on $X \times X$, given by $B(x,y) = \langle x, Ay
\rangle$. We will say that $A$ is positive definite if the
corresponding bilinear form is symmetric and positive definite,
i.e.~if $\langle x, Ay \rangle = \langle Ax, y\rangle$ for all $x,y
\in X$, and $\langle x, Ax \rangle >0$ for all nonzero $x \in X$; $A$
is positive semidefinite if instead we have $\langle x, Ax \rangle \ge
0.$ In the case of $X = \ell_2^n$ this is equivalent to stating that
the matrix $M$ of $A$ with respect to the standard basis is positive
definite, i.e.~is symmetric and all its eigenvalues are positive. We
write $A \succ 0$ to denote that $A$ is positive definite, and $A
\succeq 0$ to denote that it is positive semidefinite.

For a positive definite operator $A:\ell_2^n \to \ell_2^n$, and a
positive integer $k$, there exists a unique positive definite operator
$B:\ell_2^n \to \ell_2^n$ such that $B^k = A$. We use the notation
$A^{1/k}$ for $B$. We also use the shorthand notation $A^{\ell/k} \eqdef
(A^\ell)^{1/k}$ for (positive or negative) integers
$\ell$. Equivalently, we can derive $A^{\ell/k}$ by raising every
eigenvalue of $A$ to the power $\ell/k$ in the spectral decomposition
of $A$.

We also recall that the operator norm $\|A\|$ of a linear operator
$A:X \to Y$ is defined by 
\[
\|A\| = \sup\{\|Ax\|_Y: \|x\|_X \le 1\},
\]
where $X = (\R^n, \|\cdot\|_X)$ and $Y = (\R^n, \|\cdot\|_Y)$ are
normed spaces. 

A related norm on operators is the nuclear norm. Here we only use the
nuclear norm $\nu(A)$ of an operator $A:\ell_2^n \to \ell_2^n$, which
equals the sum of its singular values. It is easy to see that
\[
\nu(A) = \tr((AA^*)^{1/2}).
\]
The nuclear norm is dual to the operator norm, and in particular we
have the identity
\[
\nu(A) = \sup\{\tr(AO): O \text{ orthogonal transformation}\},
\]
where the supremum is over orthogonal transformations on $\ell_2^n$. 



\subsection{The Factorization Constant $\lambda$}

In what follows we fix two $n$-dimensional normed spaces $(X,
\|\cdot\|_X)$ and $(Y,\|\cdot\|_Y)$, and an invertible linear operator
$U:X \to Y$. Since we work with general finite-dimensional normed
space, restricting to spaces of equal dimension and to invertible
operators is without loss of generality: given an operator $U:X \to Y$
which has a nontrivial kernel $W$, we can replace $X$ by the
quotient space $X/W$, and $Y$ by its subspace given by the range of
$U$, and $U$ by the induced operator $\tilde{U}:X/W \to U(X)$ which
sends $x+W$ to $Ux$. It is straightforward to check that this not
change any of the quantities we study.

In this section we reduce all upper bounds on vector balancing
constants to the following deep theorem of Banaszczyk.
\begin{theorem}[\cite{bana}]\label{thm:bana}
  Let $K$ be a convex body in $\R^n$ such that $\gamma_n(K) \ge
  \frac12$. Then, $\vb(B_2^n, K) \le 5$.
\end{theorem}

As a warm-up, let us see how we can use Theorem~\ref{thm:bana} to get
nearly tight bounds on $\vb(B^n_p, B^n_q)$, or, equivalently, on
$\vb(I)$, where $I:\ell_p^n \to \ell_q^n$ is the identity. For any
$x_1, \ldots, x_N \in B_p^n$, the points defined by $u_i \eqdef
{x_i}/{\max\{1, n^{1/2 -1/p}\}}$ lie in $B_2^n$ and we can apply
Banaszczyk's theorem to them and the convex body $K \eqdef C \sqrt{q}
n^{1/q} B_q^n$, which has Gaussian
measure at least $\frac12$ as long as we choose $C$ to be a large
enough constant. We get that there exist signs $\eps_1, \ldots, \eps_N
\in \{-1, 1\}$ such that 
\[
\left\|\sum_{i = 1}^N{u_i}\right\|_K \le 5
\iff
\left\|\sum_{i = 1}^N{x_i}\right\|_q \le
 5C\sqrt{q}\max\{n^{1/q}, n^{1/q + 1/2 -1/p}\}. 
\]
In other words, we have that 
\[
\vb(B_p^n, B_q^n) = \vb(I) \le
5C\sqrt{q}\max\{n^{1/q}, n^{1/q + 1/2 -1/p}\}.
\]
The volume lower bound (Lemmas~\ref{lem:vol-lb},\ref{lem:vol-lb-oper})
can be used to show that this bound is tight up to the $O(\sqrt{q})$
factor. Indeed one can show that $B_p^n$ contains $n$ vectors $u_1,
\ldots u_n$ such that the matrix $U \eqdef  (u_1, \ldots, u_n)$ has
determinant $\det(U) \ge e^{-1}  \max\{1, n^{1/2 -1/p}\}$
(see~\cite{Ball89}~or~\cite{N15}). By standard estimates,
$\vol(B_q^n)^{1/n} \ge c n^{1/q}$ for an absolute constant
$c >0$. Plugging these estimates into Lemma~\ref{lem:vol-lb} shows
$\vb(B_p, B_q) \ge c' \max\{n^{1/q}, n^{1/q + 1/2 -1/p}\}$ for a
constant $c' > 0$. 

One take away from the example above is that Theorem~\ref{thm:bana}
can be used for balancing vectors in the unit ball $B_X$ of any norm
$X$ by appropriately ``embedding'' $B_X$ into $B_2^n$. In the case
$\ell^n_p$ and $\ell^n_q$, this embedding just corresponds to
rescaling the unit ball $B_p^n$. For general norms this does not
necessarily make sense. However, we can instead choose a linear map $T:X \to
\ell_2^n$ so that $T(B_X) \subseteq B_2^n$, i.e.~$\|T\| \le 1$. Our
approach is based on this idea, and, in particular, on choosing such a
map $T$ optimally.

To capture the intution above into a definition, we will use the
$\ell$-norm, which has been extensively studied in the theory of
operator ideals, and in asymptotic convex geometry (see
e.g.~\cite{TJ-book,Pisier-book,AGM-book}). For a linear operator
$S:\ell_2^n \to Y$, where $Y = (\R^n, \|\cdot\|_Y)$ is a normed space,
the $\ell$-norm of $S$ is defined as
\[
\ell(S) \eqdef \left( \int \|S(x)\|_Y^2 d\gamma_n(x) \right)^{1/2},
\]
where $\gamma_n$ is the standard Gaussian measure on $\R^n$. I.e., if
$Z$ is a standard Gaussian random variable in $\R^n$, then $\ell(S) =
(\E \|S(Z)\|_Y^2)^{1/2}$. It is easy to verify that $\ell(\cdot)$ is a
norm on the space of linear operators from $\ell_2^n$ to $Y$, for any
normed space $Y$ as above. 

Moreover, we can define a dual norm $\ell^*$ via trace duality: for
any linear opartor $R: Y \to \ell_2^n$, we define
\[
\ell^*(R) \eqdef \sup\{\tr(RS): S: \ell_2^n \to Y, \ell(S) \le 1\}.
\]
Then $\ell$ and $\ell^*$ form a dual pair, and in particular we have
\[
\ell(S) = \sup\{\tr(RS): R:Y\to\ell_2^n, \ell^*(R) \le 1\}.
\]
For a finite dimensional space $Y$, both suprema above are achieved.
An in-depth discussion of the $\ell$ norm can be found, for example,
in the book~\cite{TJ-book}.



We now define the main object of study in this section. Let $X$ and
$Y$ be $n$-dimensional normed spaces, and let $U:X \to Y$ be a
linear operator. We define a factorization constant
\[
\lambda(U) \eqdef \inf \{\ell(S)\|T\|: T: X \to \ell_2^n,\ S: \ell_2^n
\to Y, U = ST\}.
\]
In other words, $\lambda(U)$ is the minimum of $\ell(S)\|T\|$ over all
ways to factor $U$ through $\ell_2^n$ as $U = ST$. A standard
compactness argument shows that the infimum is in fact achieved. 

\cut{It is
also worth noting that we can choose the operator $T$ to map $X$ to
$\ell_2^m$, and $S$ to map $\ell_2^m$ to $Y$, for any $m \ge
\rank(U)$, without changing the value of $\lambda(U)$. The reason is
that, if we choose $W$ to be the subspace of $\R^m$ of dimension $r
\eqdef \rank(U)$ such that $S(W)$ equals the range of $U$, then we can
replace $S$ with $S\pi$ and $T$ with $\pi T$, where $\pi:\ell_2^m
\to\ell_2^{r}$ is the orthogonal projection that sends $W$ to
$\R^{r}$. This is still a factorization of $U$ and the norms $\ell(S)$
and $\|T\|$ remain unchanged.}
We have the following theorem, which shows that $\lambda(U)$ is, up to
constants, an upper bound on $\vb(U)$ for any operator $U$. 

\begin{theorem}\label{thm:factorization}
  There exists a constant $C$ such that for any linear operator $U:X
  \to Y$ between two $n$-dimensional normed spaces $X, Y$, we have
  \[
  \vb(U) \le C\lambda(U).
  \]
\end{theorem}
\begin{proof}
  As mentioned above, we can assume $U$ to be invertible.  Let $x_1,
  \ldots, x_N \in B_X$ be arbitrary, and let $T:X \to \ell_2^n$,
  $S:\ell_2^n \to Y$ be such that $ST = U$, $\ell(S)\le \lambda(U)$
  and $\|T\| \le 1$. For $i \in \{1, \ldots, N\}$, define $u_i \eqdef
  Tx_i$; since $\|T\| \le 1$ by assumption, we have $u_i \in B_2^n$
  for all $i$. Let $K = \sqrt{2}\ell(S) S^{-1}(B_Y)$, and let, as
  usual, $\|\cdot\|_K$ be the norm with unit ball $K$. Observe that
  for any $x \in \R^n$, $\|x\|_K =
  \frac{1}{\sqrt{2}\ell(S)}\|Sx\|_Y$. By Chebyshev's inequality, for a
  standard Gaussian $Z$,
  \[
  1 - \gamma_n(K)  \le \E\|Z\|_K^2 
  =   \frac{1}{2\ell(S)^2} \E\|Z\|_Y^2 = \frac12.
  \]
  We can, therefore, apply Theorem~\ref{thm:bana}, and have that there
  exist signs $\eps_1, \ldots, \eps_N$ such that 
  \begin{align*}
  \sum_{i =1}^N{\eps_i u_i} \in 5K
  \iff
  \left\|\sum_{i =1}^N{\eps_i u_i}\right\|_K \le 5
  %\sum_{i = 1}^N{\eps_i ST x_i} \in (5\sqrt{2}\ell(S))B_Y
  &\iff 
  \left\|\sum_{i = 1}^N{\eps_i Su_i}\right\|_Y \le 5\sqrt{2}\lambda(U)\\
  &\iff   \left\|\sum_{i = 1}^N{\eps_i Ux_i}\right\|_Y \le 5\sqrt{2}\lambda(U)b,
  \end{align*}
  where we used that $Su_i = STx_i = Ux_i$.
  This completes the proof.
\end{proof}

Theorem~\ref{thm:factorization} refines and generalizes the connection
between the $\gamma_2$ norm and hereditary discrepancy
from~\cite{disc-gamma2}. The $\gamma_2$ norm of an operator $U:X \to
Y$ is defined by
\[
\gamma_2(U) = \inf\{\|S\| \|T\|:  T: X \to \ell_2^n,\ S: \ell_2^n
\to Y, U = ST\}.
\]
In~\cite{disc-gamma2}, the authors studied the special case in which
$X = \ell_1^n$ and $Y = \ell_\infty^m$. In that case, $\|S\|$ is the
largest Euclidean norm of a column in the matrix of $S$, and $\|T\|$
is the largest Euclidean norm of a row of the matrix of $T$. It then
follows from standard concentration of measure arguments that, for an absolute constant $C$,
\begin{equation}
  \label{eq:ell-vs-opnorm}
  \|S\| \le \ell(S) \le C\sqrt{1 + \log m} \cdot\|S\|,
\end{equation}
for any operator $S: \ell_2^n \to \ell_\infty^m$. Therefore,
for any $U:\ell_1^n \to \ell_\infty^m$, we have 
\[
\gamma_2(U) \le \lambda(U) \le C\sqrt{1 + \log m}\cdot \gamma_2(U).
\]
This inequality and Theorem~\ref{thm:factorization} recover the upper
bound on hereditary discrepancy in terms of the $\gamma_2$ norm
from~\cite{disc-gamma2}. It also shows that $\lambda$ provides at
least as good an approximation to hereditary discrepancy as $\gamma_2$.
However, \eqref{eq:ell-vs-opnorm} is often
not tight, and Theorem~\ref{thm:factorization} provides a tighter
upper bound.

Another interesting special case is $X = \ell_1^n$ and $Y =
\ell_2^m$. Since for any $S:\ell_2^n\to \ell_2^n$, $\ell(S) =
\|S\|_{HS}$, where $\|A\|_{HS} = \tr(SS^*)^{1/2}$ is the
Hilbert-Schmidt norm of $A$, we have
\[
\lambda(U) = \inf\{\|S\|_{HS}\|T\|: 
T: \ell_1^n \to \ell_2^n,\ S: \ell_2^n \to \ell_2^n, U = ST\}.
\]
This function was studied by two of the authors in~\cite{NT15}, where
they showed that it approximates hereditary discrepancy with respect
to $\ell_2^n$ up to a factor of $O(\log n)$. 

Our goal in the remainder of the section is to prove that the
inequality in Theorem~\ref{thm:factorization} holds in the reverse
direction as well, as captured in the following theorem.

\begin{theorem}\label{thm:fact-vollb}
  There exists a constant $C$ such that the following holds. Let $X$
  and $Y$ be two $n$-dimensional normed spaces and let $U:X \to Y$ be
  a linear operator between them.
  %, such that the unit
  %ball of $X$ is $B_X = \mathrm{conv}\{\pm v_1, \ldots, \pm v_m\}$. 
  Then
  \[
  \lambda(U) \le C K(Y) (1 + \log n)^{3/2}\vollb^{\rm h}(U),
  \]
  where $K(Y) = O(\log n)$ is the $K$-convexity constant of $Y$.
  \footnote{See Section~\ref{sect:fact-pf} for a definition of
    $K(Y)$.}  
  
  Moreover, there exists an integer $k\le n$, and a rank $k$ operator
  $V:\ell_1^k\to X$ of operator norm $1$ so that for any standard
  basis vector $e_i$, $Ve_i$ is an extreme point of $B_X$, and
  \[
  \lambda(U) \le C K(Y) (1 + \log n)^{3/2}
  \vol_k\bigl(\set{a \in \R^k: \|UV a\|_Y \le 1}\bigr)^{-1/k}.
  \]
\end{theorem}

Theorems~\ref{thm:factorization}~and~\ref{thm:fact-vollb} together
with Lemma~\ref{lem:vol-lb} give a characterization of $\vb(U)$ in
terms of $\lambda(U)$.

\begin{corollary}\label{cor:vb-apx}
  There exists a constant $C$ such that for any two $n$-dimensional
  normed spaces $X$ and $Y$, and any linear operator $U:X \to Y$
  between them, we have
  \[
  \frac1C \le \frac{\lambda(U)}{\vb(U)} \le C K(Y) (1 + \log n)^{3/2},
  \]
  where $K(Y) = O(\log n)$ is the $K$-convexity constant of $Y$. 
\end{corollary}

The statement after ``moreover'' in Theorem~\ref{thm:fact-vollb}
allows us to also show that $\lambda(U)$ approximately characterizes
hereditary discrepancy as well.
\begin{corollary}\label{cor:hd-apx}
  Given a sequence of vectors $(u_i)_{i = 1}^N$ in $\R^n$ and a convex
  body $K$ in $\R^n$, define $X$ to be the normed space with
  unit ball $B_X = \conv\{\pm u_1, \ldots \pm u_N\}$, $Y$ to be
  the normed space with unit ball $K$, and $I:X \to Y$ to be the
  identity map. Then, for an absolute constant $C$,
  \[
  \frac{\lambda(I)}{C K(Y) (1 + \log n)^{3/2}} 
  \le \hd((u_i)_{i = 1}^N, K) \le \vb(I) 
  \le C \lambda(I)
  \]
\end{corollary}
\begin{proof}
 The inequality $\hd((u_i)_{i = 1}^N, K) \le \vb(I)$ is trivial from
 the definitions, and then the final inequality follows from
 Theorem~\ref{thm:factorization}. For the first inequality, observe
 that the points $Ve_1, \ldots, Ve_k$ belong to $(u_i)_{i = 1}^n$, so
 there is some subset $S \subseteq [N]$ of size $k$ for which
 $(Ve_i)_{i = 1}^k$ equals $(u_i)_{i \in S}$ (possibly after
 rearrangement), so that 
 \[
 \vol_k\bigl(\set{a \in \R^k: \|UV a\|_Y \le 1}\bigr)^{-1/k} =
 \vollb((u_i)_{i \in S}, K).
 \]
 Then, the first inequality follows from Lemma~\ref{lem:vol-lb} and
 the statement after ``moreover'' in Theorem~\ref{thm:fact-vollb}.
\end{proof}

Finally, in Section~\ref{sect:fact-alg} we show that, under reasonable
assumptions on how we are given access to the normed spaces $X$ and
$Y$, $\lambda(U)$ can be computed in polynomial time. Given
Corollaries~\ref{cor:vb-apx}~and~\ref{cor:hd-apx}, this also implies a
polynomial time approximation algorithm for vector balancing and
hereditary discrepancy with arbitrary norms. 

\subsection{Convex And Dual Formulations}

The factorization constant $\lambda(U)$ can be formulated as the
solution to a convex optimization problem, i.e.~as the infimum of a
convex function over a convex domain. This fact is useful since it
allows applying generic convex optimization techniques to compute
$\lambda(U)$ given appropriate access to the unit balls of $X$ and
$Y$. It will also be useful in deriving a dual formulation of
$\lambda(U)$, which we then use to prove Theorem~\ref{thm:fact-vollb}.

The most immediate way to formulate $\lambda(U)$ as an optimization
problem is 
\begin{align*}
  &\inf %\ell(B^{-1})
  \ell(UT^{-1})\\
  &\text{s.t.}\\
  &T:X \to\ell_2^n,\|T\| \le 1\\
  &A \succ 0.
\end{align*}
Unfortunately, this optimization problem is not convex in $T$: the
value of the objective function is finite for any nonzero $T$, but
infinite for $0 = \frac12(T + (-T))$, for example. The key observation
that allows us to circumvent this issue is that the objective function
is completely determined by the operator $A \eqdef T^*T$, and is in
fact convex in $A$. 


The objective function $f(A)$ of our convex optimization formulation of
$\lambda(U)$ is then defined as follows: for any positive definite
operator $A:X \to X^*$, we set
\begin{equation}
  \label{eq:obj-def}
  f(A) \eqdef \ell(UT^{-1}),
\end{equation}
where $T:X \to \ell_2^n$ is an invertible linear operator such that $T^*T = A$. 

A couple of clarifications are in order. First, we claim that such an
operator $T$ exists, by the positive definiteness of $A$. Indeed, we
can choose a basis $e_1, \ldots, e_n$ of $X$, and a corresponding dual
basis $e_1^*, \ldots, e_n^*$ of $X^*$, and define $M$ to be the matrix
of $A$ with respect to these bases. Then $M$ is a positive definite
matrix, so it admits a Cholesky decomposition $M = L^\T L$. We can
then define $T$ to be the operator whose matrix with respect to $e_1,
\ldots, e_n$ and the standard basis of $\ell_2^n$ is $L$; the matrix
of the dual operator $T^*$ is $L^\T$, so we have $T^*T = A$ as required.

A second concern is whether $f(A)$ is well-defined. To see that this
is the case, observe that $A = T^*T = S^*S$ implies that there is an
orthogonal transformation $O:\ell_2^n \to \ell_2^n$ for which $S =
OT$.  Then, $S^{-1} = T^{-1}O^{-1}$, and, for a standard Gaussian
random variable $Z$,
\[
\ell(US^{-1}) = (\E\|UT^{-1}O^{-1}Z\|_Y^2)^{1/2} =
(\E\|UT^{-1}Z\|_Y^2)^{1/2}
= \ell(UT^{-1}),
\]
where we used the fact that $O^{-1}Z$ and $Z$ are identically
distributed because $O^{-1}$ is an orthogonal transformation. 

Having defined the objective function, we are now ready to specify our
convex optimization problem for $\lambda(U)$.
\begin{restatable}{lemma}{factconvex}
  \label{lm:fact-convex}
  For any two $n$-dimensional normed spaces $X$ and $Y$, and any
  invertible linear operator $U:X \to Y$, $\lambda(U)$ equals
  \begin{align}
    &\inf  f(A)  \label{eq:fact-obj}\\
    &\text{s.t.}\notag\\
    &A: X \to X^*,\|A\| \le 1\label{eq:fact-constr}\\
    &A \succ 0.\label{eq:fact-psd}
  \end{align}
  The function $f$ is the one defined in \eqref{eq:obj-def}.

  Moreover, the objective \eqref{eq:fact-obj} and the constraints
  \eqref{eq:fact-constr}--\eqref{eq:fact-psd} are convex in $A$.
\end{restatable}

In addition to allowing us to efficiently compute $\lambda(U)$, the
optimization problem \eqref{eq:fact-obj}--\eqref{eq:fact-psd} makes it
possible to use Lagrange duality to derive a dual formulation of
$\lambda(U)$ as a supremum over ``dual certificates''. Such a
formulation is useful in proving Theorem~\ref{thm:fact-vollb}, because
it allows us to reduce the proof to relating the dual certificates
to the terms in the volume lower bound~\eqref{eq:vol-lb}. If we can
show that every dual certificate bounds from below one of the terms of
the volume lower bound (up to factors polynomial in $\log n$), then we
can conclude that $\lambda(U)$ also bounds the volume lower bound from
below.

The dual formulation of $\lambda(U)$ is given in the following lemma. 
\begin{restatable}{lemma}{dual}
  \label{lm:dual}
  Let $X$ and $Y$ be two $n$-dimensional normed spaces, such that the
  unit ball of $X$ is $B_X = \mathrm{conv}\{\pm x_1, \ldots, \pm
  x_m\}$. Then, for any linear operator $U:X \to Y$, $\lambda(U)$
  equals
  \begin{align}
    &\sup \tr((RU(\sum_{i = 1}^m{p_i x_i \otimes  x_i})U^*R^*)^{1/3})^{3/2}\label{eq:dual-obj}\\
    \text{s.t.}\notag\\
    &R: Y \to \ell_2^n, \ell^*(R) \le 1 \label{eq:dual-ellstar}\\
    &\sum_{i = 1}^m{p_i} = 1\label{eq:dual-prob}\\
    &p_1, \ldots, p_m \ge 0. \label{eq:dual-nonneg}
  \end{align}
\end{restatable}

We prove Lemmas~\ref{lm:fact-convex}~and~\ref{lm:dual} in
Section~\ref{sect:fact-proofs}. Before doing so, we use them to prove
Theorem~\ref{thm:fact-vollb} in the following section. 

\subsection{Proof of Theorem~\ref{thm:fact-vollb}}
\label{sect:fact-pf}

We will use Lemma~\ref{lm:dual} to prove Theorem~\ref{thm:fact-vollb}.
In the proof, we need to relate the objective function
\eqref{eq:dual-obj} of the dual formulation to volumetric information
about $X$ and $Y$. We do so in two steps. In the first step, we
consider a soluion $R, p$ of
\eqref{eq:dual-obj}--\eqref{eq:dual-nonneg}, and, using purely
linearly algebraic techniques, we find a subset $S$ of $\{1, \ldots,
m\}$ so that the Gram matrix of the vectors $(RUx_i)_{i \in S}$ has large
determinant  in relation to the value of
\eqref{eq:dual-obj}. This allows us to define an operator $V$ from
$\ell_1^k$ (for  $k = |S|$) to $X$ so that the set
$V^*U^*R^*(B_2^n))$ has large volume. In the second step of the proof
we give a lower bound on the volume of the set $V^*U^*(B_Y)$ in terms
of the volume of $V^*U^*R^*(B_2^n))$. Here we use classical
connections between the $\ell^*$ norm and the $\ell$ norm (via
$K$-convexity) and between the $\ell$ norm and covering numbers (via
the dual Sudakov inequality). Since $V^*U^*(B_Y)$ is polar to the set
$\{a: \|UVa\|_Y \le 1\}$ appearing in the volume lower bound, we can
finish the proof by appealing to the Blaschke-Santal\'o inequality.


In the context of linear operators it is often convenient use to the
notion of entropy numbers instead of covering numbers.  The entropy
number $e_k(A)$ of a linear operator $A:X \to Y$ is defined by
\[
e_k(u) = \inf\{\varepsilon: N(u(B_X), \varepsilon B_Y) \le 2^{k-1}\}. 
\]
It is well known that covering numbers give both upper and lower
estimates for the supremum of a Gaussian process. Here we use the dual
Sudakov inequality, which in the language of entropy numbers has the
following simple form: there exists a constant $C$ such that for any
linear operator $A:\ell_2^n \to X$, we have
\begin{equation}
  \label{eq:sudakov}
  \max_{k = 1}^n \sqrt{k}e_k(u) \le C\ell(u). 
\end{equation}
This inequality is due to~\cite{PTJ85}. See \cite[Section
3.3]{LT91-book} for an easy proof. 

Another important tool in the proof of Theorem\ref{thm:fact-vollb} is
$K$-convexity, introduced by Maurey and Pisier~\cite{MP76}. The
$K$-convexity constant $K(Y)$ of an $n$-dimensional normed space $Y$
is the infimum over all constants $K$ for which the inequality
\begin{equation}
  \label{eq:K-conv-def}
\ell(A^*) \le K\ell^*(A)
\end{equation}
holds for every operator $A:Y \to \ell_2^n$. (See
\cite{Pisier-book} or \cite{TJ-book} for an equivalent definition.)
An important estimate of Pisier~\cite{P80} shows that that there
exists an absolute constant $C$ such that for any $n$-dimensional
normed space $Y$, 
\begin{equation}
  \label{eq:K-conv-Pisier}
  K(Y) \le C(1+\log d(Y,\ell_2^n))\le C(1+\log n).
\end{equation}
Above $d(Y, \ell_2^n)$ is he Banach-Mazur distance between $Y$ and
$\ell_2^n$, equal to the minimum of $\|T\| \|T^{-1}\|$ over linear
operators $T:Y \to \ell_2^n$. Equivalently, it is equal to the
smallest $d$ for which there exists a linear operator $T$ such that
$B_2^n \subseteq T(B_Y) \subseteq d B_2^n$. For any $n$-dimensional
normed space $Y$, $d(Y,\ell_2^n)$ is bounded by $\sqrt{n}$ by John's
theorem, which implies the second inequality.

We also make use of a weighted version of Lemma~\ref{lm:rip-det}.
\snote{try to reconcile and combine with Lemma~\ref{lm:rip-det}.}
\begin{lemma}\label{lm:rip-det-weighted}
  Let $u_1, \ldots, u_m \in \R^n$, and let $p_1, \ldots, p_m \ge 0$,
  $\sum_{i = 1}^mp_i = 1$. Let $\lambda_1 \ge \ldots \ge \lambda_n$ be
  the eigenvalues of the matrix $\sum_{i=1}^m{p_i u_i^\T u_i}$, and
  let $G$ be the Gram matrix of $u_1, \ldots, u_m$, i.e.~$g_{ij} =
  \langle u_i, u_j\rangle$. For any integer $k$ such that $1 \le k \le
  n$, there exists a set $S \subseteq [m]$ of size $k$ such that
  \[
  \frac{\det(G_{S,S})}{k!} \ge\lambda_1 \ldots \lambda_k. 
  \]
\end{lemma}
\begin{proof}
  Consider the matrix $H = (\sqrt{p_ip_j}\langle u_i, u_j
  \rangle)_{i,j = 1}^m$. This matrix has the same nonzero eigenvalues as
  $\sum_{i=1}^m{p_i u_i^\T u_i}$, and, therefore,
  \[
  \sum_{S \subseteq [m]: |S| = k}{\left(\prod_{i \in S}{p_i}\right)\det(G_{S,S})}
  = \sum_{S \subseteq [m]: |S| = k}\det(H_{S,S}) = s_{k,n}(\lambda),
  \]
  where $s_{k,n}$ is the degree $k$ elementary symmetric polynomial in $n$
  variables. (See the proof of Lemma~\ref{lm:rip-det} for a
  justification of the final equality.) Therefore, 
  \[
  \max_{S \subseteq [m]: |S| = k}\det(G_{S,S}) 
  \ge \frac{s_{k,n}(\lambda)}{s_{k,m}(p)}. 
  \]
  We have the trivial inequality
  \[
  s_{k,n}(\lambda) \ge \lambda_1 \ldots \lambda_k,
  \]
  since $\lambda_1 \ldots \lambda_k$ is one of the terms of
  $s_{k,n}(\lambda)$. 
  
  To bound $s_{k,m}(p)$ from above, observe that
  \[
  s_{k,m}(p) \le \frac{(p_1 + \ldots + p_k)^k}{k!} = \frac{1}{k!},
  \]
  since each term of $s_{k,m}(p)$ appears exactly $k!$ times in 
  $(p_1 + \ldots + p_k)^k$.  Combining the inequalities finishes the proof. 
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:fact-vollb}]
  We can approximate the unit ball $B_X$ of $X$ arbitrarily well by a
  symmetric polytope, so we may assume that $B_X = \mathrm{conv}\{\pm
  x_1, \ldots, \pm x_m\}$. Then, by Lemma~\ref{lm:dual}, there exists an
  operator $R: \ell_2^n \to Y$, $\ell^*(R) \le 1$ and non-negative
  reals $p_1, \ldots, p_m \ge 0$, $\sum_{i = 1}^m{p_i} = 1$, such that
  \[
  \lambda(U)^{2/3} 
  = \tr((RU(\sum_{i = 1}^m{p_i x_i  \otimes  x_i})U^*R^*)^{1/3})
  = \tr((\sum_{i = 1}^m{p_i RU(x_i)  \otimes  RU(x_i)})^{1/3}).
  \]
  Let $u_i \eqdef RU(x_i) \in \R^n$, and let $\lambda_1 \ge \ldots \ge
  \lambda_n \ge 0$ be the eigenvalues of $\sum_{i = 1}^m{p_i u_i
    \otimes u_i}$, so that $\lambda(U)^{2/3} = \sum_{i =
    1}^n{\lambda_i^{1/3}}$. We have the following elementary but very useful
  inequality, which is an approximate reverse of the AM-GM inequality:
  \begin{align*}
  \sum_{i = 1}^n{\lambda_i^{1/3}} \le 
  \sum_{i = 1}^n{\left(\prod_{j = 1}^i{\lambda_j}\right)^{1/3i}}
  &= \sum_{i = 1}^n{\frac{1}{i} \cdot i\left(\prod_{j =   1}^i{\lambda_j}\right)^{1/3i}}\\
  &\le \left(\sum_{i = 1}^n{\frac{1}{i}}\right) 
  \max_{i = 1}^n{i\left(\prod_{j =   1}^i{\lambda_j}\right)^{1/3i}}.
  \end{align*}
  Let us fix a value $k$ so that the maximum on the right hand side is
  achieved. Then, the above inequality implies
  \[
  \lambda(U)^{2/3} \le C_0 (1+\log n) k (\lambda_1 \ldots \lambda_k)^{1/(3k)},
  \]
  for an absolute constant $C_0$. Observe that the matrix of $\sum_{i
    = 1}^m{p_i u_i \otimes u_i}$ with respect to the standard basis is
  $\sum_{i = 1}^m{p_i u_i^\T u_i}$, so, by
  Lemma~\ref{lm:rip-det-weighted},  there exists a set $S
  \subseteq [m]$ of size $k$ such that
  \[
  (\lambda_1 \ldots \lambda_k)^{1/k} \le
  \frac{\det(G_{S,S})^{1/k}}{(k!)^{1/k}},
  \]
  where $G$ is the Gram matrix of $u_1, \ldots, u_m$. By Stirling's
  estimate, this implies that 
  \begin{equation*}
  \lambda(U)^{2/3} \le C_1 (1+\log n) k^{2/3} \det(G_{S,S})^{1/(3k)},
  \end{equation*}
  or, equivalently,
  \begin{equation}\label{eq:fact-det}
  \lambda(U)\le  C_1^{3/2} (1+\log n)^{3/2} k \det(G_{S,S})^{1/(2k)},
  \end{equation}
  for an absolute constant $C_1$. 

  To finish the proof, we need to relate the right hand side of
  \eqref{eq:fact-det} to the volume lower bound. Let $V:\ell_1^S \to X$ be
  the operator defined by $V(a) \eqdef \sum_{i \in S}{a_i x_i}$, where
  $\ell_1^S$ is the coordinate subspace of $\ell_2^n$ spanned by the standard
  basis vectors $\{e_i: i \in S\}$. We have that 
  \[
  \det(G_{S,S})^{1/2} 
  = \frac{\vol_k(RUV(B_2^n\cap \R^S))}{\vol_k(B_2^n \cap \R^S)}
  = \frac{\vol_k(V^*U^*R^*(B^n_2\cap W))}{\vol_k(B_2^k)},
  \]
  where $W$ is the range of $RUV$ in $\R^n$. By the definition of
  entropy numbers,
  \[
  \vol_k(V^*U^*R^*(B^n_2\cap W))\le
  \vol_k(V^*U^*R^*(B_2^n))
  \le 2^{k-1}e_k(R^*)^k \vol_k(V^*U^*(B_{Y^*})), 
  \]
  since $R^*(B_2^n)$ can be covered by $2^{k-1}$ translates of
  $e_k(R^*)B_{Y^*}$. By \eqref{eq:sudakov} and \eqref{eq:K-conv-def}, we
  have
  \[
  e_k(R^*) \le C_2 \frac{\ell(R^*)}{\sqrt{k}} 
  \le C_2 K(Y) \frac{\ell^*(R)}{\sqrt{k}}
  \le C_2 K(Y) \frac{1}{\sqrt{k}},
  \]
  for an absolute constant $C_2$. Combining the inequalities so far,
  we have
  \begin{align*}
  \det(G_{S,S})^{1/2k} &\le
  2C_2 K(Y) 
  \frac{\vol_k(V^*U^*(B_{Y^*}))^{1/k}}{\sqrt{k}\vol_k(B_2^k)^{1/k}}\\
  &\le
  2C_2 K(Y)
  \frac{\vol_k(B_2^k)^{1/k}}{\sqrt{k}\vol_k(\{a: \|UVa\|_Y\le 1\})^{1/k}},
  \end{align*}
  where in the final step we used the Blaschke-Santal\'{o} inequality and
  the fact that 
  \[
  (V^*U^*(B_{Y^*}))^\circ = \{a: \|UVa\|_Y\le 1\}.
  \]
  By a standard estimate, $\frac{\vol_k(B_2^k)^{1/k}}{\sqrt{k}} \le
   \frac{C_3}{k}$ for a constant $C_3$, and, combining with \eqref{eq:fact-det}, we get
  \[
  \lambda(U)\le \frac{C K(Y) (1+\log n)^{3/2} }{\vol_k(\{a: \|UVa\|_Y\le 1\})^{1/k}}
  \le C K(Y) (1+\log n)^{3/2} \vollb^{\rm h}(U),
  \]
  for an absolute constant $C$, as desired. The statement after
  ``moreover'' follows by observing that we can assume $x_1, \ldots,
  x_m$ to be extreme points of $B_X$, and since the points $Ve_i$ are
  a subset of them, they are extreme as well.
\end{proof}

\subsection{Proofs of Convexity and Duality}
\label{sect:fact-proofs}

In this section we supply the missing proofs of
Lemmas~\ref{lm:fact-convex}~and~\ref{lm:dual}, i.e.~the fact that the
convex optimization  problem \eqref{eq:fact-obj}--\eqref{eq:fact-psd}
is indeed convex and equal to $\lambda(U)$, and also the derivation of
the dual maximization problem. 

We first prove some key technical properties of the function $f$.
\begin{lemma}\label{lm:obj-f}
  The following statements hold for the function $f$ defined on
  positive definite operators $A:X \to X^*$ as in \eqref{eq:obj-def}:
 \begin{itemize}
  \item $f$ is a differentiable convex function on positive definite
    operators $A:X \to X^*$;
  \item $f$ is given by the formula
    \begin{equation}\label{eq:f-formula}
    f(A) = \sup\{\tr((RUA^{-1}U^*R^*)^{1/2}): 
    R: Y \to \ell_2^n, \ell^*(R) \le 1\};
    \end{equation}
  \item the derivative of $f$ at $A$ is 
    \begin{equation}\label{eq:f-derivative}
    \nabla f(A) = 
    -\frac{1}{2} (RU)^{-1} ((RU)^{-*} A (RU)^{-1})^{-3/2} (RU)^{-*},
    \end{equation}
    where $R: Y \to \ell_2^n$, $\ell^*(R) \le 1$ is such that $f(A) = 
    \tr((RUA^{-1}U^*R^*)^{1/2})$.
  \end{itemize}
\end{lemma}
Note that the derivative $\nabla f(A)$ is a linear functional on
operators from $X$ to $X^*$, and, via the trace, we identify such functionals with
operators from $X^*$ to $X$. In what follows we will, therefore, treat $\nabla
f(A)$ as a linear operator from $X^*$ to $X$.

Before we prove Lemma~\ref{lm:obj-f}, we need an auxiliary lemma.
\begin{lemma}\label{lm:invertible}
  Let $Y$ be an $n$-dimensional normed space, and let $S:\ell_2^n \to
  Y$ be an invertible linear operator. Then any operator $R:Y \to
  \ell_2^n$ such that $\tr(RS) = \ell(S)$ is invertible.
\end{lemma}
\begin{proof}
  Assume for contradiction that $R$ is not invertible, i.e.~it has a
  non-trivial kernel. Let $k<n$ be the dimension of the kernel of $R$,
  and let $W$ be the $k$-dimensional subspace of $\R^n$ such that $S(W)$
  is the kernel of $R$. Then, if $\pi$ is the orthogonal projection onto
  the orthogonal complement $W^\perp$ of $W$, we have
  \[
  \ell(S) = \tr(RS) = \tr(RS\pi) \le \ell(S\pi).
  \]
  Define the convex body $K = S^{-1}(B_Y)$. Using integration by
  parts, we have
  \begin{align*}
  \ell(S\pi)^2 &= \int_{t = 0}^\infty (1-\gamma_{n-k}(\sqrt{t}K\cap W^\perp))dt\\
  \ell(S)^2 &= \int_{t = 0}^\infty (1-\gamma_n(\sqrt{t}K))dt\\
  &= \int_{t = 0}^\infty \int_{W}(1-\gamma_{n-k}(\sqrt{t}K \cap (y + W^\perp))d\gamma_{k}(y))dt
  \end{align*}
  By the logconcavity of Gaussian measure and the symmetry of $K$,
  $\gamma_{n-k}(\sqrt{t}K \cap (y + W^\perp)) \le \gamma_{n-k}(\sqrt{t}K \cap
  W^\perp)$ for all $y \in W$, and, for all $y$ outside a
  compact set we have $\gamma_{n-k}(\sqrt{t}K \cap (y + W^\perp)) = 0 <
  \gamma_{n-k}(\sqrt{t}K \cap W^\perp)$. Therefore, $\ell(S\pi) < \ell(S)$, a
  contradiction.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lm:obj-f}]
  We begin with the proof of differentiability. Because $A$ is
  positive definite, it defines an inner product $\langle y,x\rangle_A
  = \langle y, Ax\rangle$ on $X$, and a corresponding Gaussian measure
  $\gamma_A$. Let $B:X \to X^*$ be a self-adjoint operator. For small
  enough $t \in \R$ the operator $A + tB$ is positive definite, and we
  have
  \[
  f(A+tB)  = \int_X \|Ux\|_Y d\gamma_{A + tB}(x)
  = \int_X \|Ux\|_Y \frac{d\gamma_{A+tB}}{d\gamma_A}(x)\ d\gamma_A(x).
  \]
  For any $x$, 
  \[
  \frac{d\gamma_{A+tB}}{d\gamma_A}(x) =
  \sqrt{\frac{\det(A+tB)}{\det(A)}} e^{-t\langle x, Bx\rangle / 2}
  \]
  is differentiable in $t$, and, by the dominated convergence theorem,
  $\frac{d}{dt}f(A+tB)$ exists and is given by differentiating under the
  integral sign. Moreover, it is easy to see that the derivative is
  continuous in $A$. This implies that all directional derivatives of $f(A)$
  exist and are continuous in its domain, and, therefore, $f(A)$ is
  differentiable everywhere in its domain. 

  Next we prove the identity \eqref{eq:f-formula}. Observe that for
  any orthogonal transformation $O:\ell_2^n \to \ell_2^n$ and any
  operator $V: \ell_2^n \to Y$, $\ell(VO) = \ell(V)$ by the
  rotational invariance of the Gaussian measure. Therefore,
  \begin{align*}
  f(A) = \ell(UT^{-1}) &= \sup\{\ell(UT^{-1}O): O \text{ orthogonal}\}\\
  &= \sup\{\tr(RUT^{-1}O): R:Y\to\ell_2^n, \ell^*(R) \le 1, O \text{ orthogonal}\}\\
  &= \sup\{\nu(RUT^{-1}): R:Y\to\ell_2^n, \ell^*(R) \le 1\}\\
  &= \sup\{\tr((RUA^{-1}U^*R^*)^{1/2}): R:Y\to\ell_2^n, \ell^*(R) \le  1\}. 
  \end{align*}
  Moreover, since we assumed that $U$ is invertible, by
  Lemma~\ref{lm:invertible}, we can assume that $R$ is invertible.
  Given this formula, in order to prove convexity, it is enough to
  prove that for any invertible operator $V:X \to \ell^n_2$ (which, in
  our case, equals $RU$), the function $\tr((VA^{-1}V^*)^{1/2})$ is
  convex in $A$ for $A:X\to X^*$ positive definite. Then, we would
  have that $f(A)$ is a supremum of convex functions, and, therefore,
  convex.  The convexity of $\tr((VA^{-1}V^*)^{1/2})$ follows from a
  standard argument based on majorization, which we give next.  Since
  $V$ is invertible, we have $VA^{-1}V^* = (V^{-*}AV^{-1})^{-1}$, and
  $\tr((VA^{-1}V^*)^{1/2}) = \tr(((V^{-*}AV^{-1})^{-1/2})$. Let
  $\alpha \in (0,1)$ be arbitrary, and let $A_1, A_2$ be two positive
  definite operators from $X$ to $X^*$. Let $\mu$ be the function that
  maps a self-adjoint operator on $\ell_2^n$ to the vector of its
  eigenvalues. By the Ky-Fan inequalities, $\mu(V^{-*}(\alpha A_1 +
  (1-\alpha)A_2)V^{-1})$ is majorized by $\alpha \mu(V^{-*}A_1V^{-1}) + (1-\alpha)
  \mu(V^{-*}A_2V^{-1})$. Let $g$ be the function defined on vectors $x \in
  \R^n$ with positve coordinates by $g(x) =
  \sum_{i=1}^n{x_i^{-1/2}}$. It is easy to verify that $g$ is convex
  and Schur-convex, so
  \begin{align*}
  g(\mu(V^{-*}(\alpha A_1 + (1-\alpha)A_2)V^{-1}))
  &\le
  g(\alpha \mu(V^{-*}A_1V^{-1}) + (1-\alpha) \mu(V^{-*}A_2V^{-1}))\\
  &\le 
  \alpha g(\mu(V^{-*}A_1V^{-1})) + (1-\alpha) g(\mu(V^{-*}A_2V^{-1})).
  \end{align*}
  Since the left hand side above equals   
  \[
  \tr((V^{-*}(\alpha A_1 + (1-\alpha)A_2)V^{-1})^{-1/2})
  = 
  \tr((V(\alpha A_1 + (1-\alpha)A_2)^{-1}V^*)^{1/2}),
  \]
  and the right hand side equals
  \begin{align*}
  \alpha \tr((V^{-*}A_1V^{-1})^{-1/2}) &+  (1- \alpha) \tr((V^{-*}A_2V^{-1})^{-1/2})\\
  &= 
  \alpha \tr((VA_1^{-1}V^*)^{1/2}) +  (1- \alpha) \tr((VA_2^{-1}V^*)^{1/2}),
  \end{align*}
  we have established convexity.   
 

  From \eqref{eq:f-formula}, we can see that the subgradient of
  $f$ at $A$ is
  \begin{align*}
  \partial f(A) &= \mathrm{conv}
  \{\nabla \tr((RUA^{-1}U^*R^*)^{1/2}):\\
  &\hspace{10em}
  R: Y \to \ell_2^n, \ell^*(R) \le 1,
  f(A) = \tr((RUA^{-1}U^*R^*)^{1/2})\}\\
  &= 
  \mathrm{conv}\{
  \nabla \tr(((RU)^{-1}A(RU)^{-*})^{-1/2}):\\ 
  &\hspace{10em}
  R: Y \to \ell_2^n, \ell^*(R) \le 1, 
  f(A) = \tr((RUA^{-1}U^*R^*)^{1/2})\}\\
  &= 
  \mathrm{conv}\{
  -\frac{1}{2} 
  (RU)^{-1} ((RU)^{-*} A (RU)^{-1})^{-3/2} (RU)^{-*}:\\ 
  &\hspace{10em}
  R: Y \to \ell_2^n, \ell^*(R) \le 1, 
  f(A) = \tr((RUA^{-1}U^*R^*)^{1/2})\}.
  \end{align*}
  Above we used Lemma~\ref{lm:invertible}, and
  the fact that 
  \[
  \nabla \tr(X^{-1/2}) =
  -\frac{1}{2}X^{-3/2}
  \]
  for any positive definite $X:\ell_2^n\to\ell_2^n$ (see~\cite{Lewis95}). Since $f$ is
  differentiable, we have that $\partial f(A)$ is a singleton set,
  i.e.~
  \[
  \nabla f(A) = 
  -\frac{1}{2} (RU)^{-1} ((RU)^{-*} A (RU)^{-1})^{-3/2} (RU)^{-*}
  \]
  for the an invertible operator $R: Y \to \ell_2^n$, such that
  $\ell^*(R) \le 1$ and $f(A) =
  \tr((RUA^{-1}U^*R^*)^{1/2})$. This finishes the proof
  of the lemma.
\end{proof}

We are now ready to prove Lemma~\ref{lm:fact-convex}, restated for
convenience below.
\factconvex*
\begin{proof}%[Proof of Lemma~\ref{lm:fact-convex}]
  The convexity of the constraints
  \eqref{eq:fact-constr}--\eqref{eq:fact-psd} is apparent from the
  definition, and the convexity of the objective was proved in
  Lemma~\ref{lm:obj-f}. We proceed to show that the value of
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd} equals $\lambda(U)$.

  Let $T:X \to \ell_2^n$ and $S:\ell_2^n \to Y$, $ST = U$ be a
  factorization achieving $\lambda(U)$ such that $\|T\| = 1$ and
  $\ell(S) = \lambda(U)$. Then we claim that $A \eqdef T^*T$ satisfies
  \eqref{eq:fact-constr}--\eqref{eq:fact-psd} and $f(A) = \ell(S) $,
  so the value of \eqref{eq:fact-obj}--\eqref{eq:fact-psd} is at most
  $\ell(S) = \lambda(U)$. Indeed, $A$ is clearly positive
  semidefinite, and must be positive definite, as $T$ is invertible,
  because $U$ is invertible. Furthermore, since $\|T\|\le 1$, we have
  that for any $x \in B_X$,
  \[\langle x, Ax \rangle = \langle Tx, Tx\rangle =
  \|Tx\|^2_2 \le 1.\] On the other hand, since $A$ is self-adjoint,
  $\|A\| = \sup\{\langle x, Ax\rangle: x \in B_X\}$, and we have shown
  that $\|A\| \le 1$. Moreover, since $A = T^* T$, by the definition
  of $f(A)$
  \[
  f(A) = \ell(UT^{-1}) = \ell(S) = \lambda(U).
  \]
  This finishes the proof of the claim that the value of
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd} is at most $\lambda(U)$.
  
  Next we prove the reverse inequality. Given a feasible solution $A$
  to \eqref{eq:fact-obj}--\eqref{eq:fact-psd}, we take an operator
  $T:X \to \ell_2^n$ such that $A = T^* T$. Then we construct a
  factorization $U=ST$ by setting $S = UT^{-1}$.  By
  \eqref{eq:fact-constr}--\eqref{eq:fact-psd}, for any $x \in B_X$ we
  have
  \[
  \|Tx\|^2_2 = \langle Tx, Tx\rangle = \langle x, Ax \rangle  \le
  1,\] so $\|T\| \le 1$. Moreover, $\ell(S) = f(A)$ by
  definition. This proves that the value of
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd} is at least
  $\lambda(U)$, and, since we already showed that it is also at
  most $\lambda(U)$, the two are equal.
\end{proof}

The derivation of the dual formulation from the
convex program \eqref{eq:fact-obj}--\eqref{eq:fact-psd} is mostly
routine using Lagrange duality (see,
e.g.~\cite{BoydV04}). Nevertheless, because of the complicated nature
of our objective, the derivation is quite technical. Once again, we
restated Lemma~\ref{lm:dual}, which gives our dual formulation, for
convenience before the proof. 

\dual*
\begin{proof}%pf of lm:dual
  Since $B_X = \mathrm{conv}\{\pm x_1, \ldots, \pm x_m\}$, we can can
  rewrite \eqref{eq:fact-obj}--\eqref{eq:fact-psd} as
  \begin{align}
    &\inf f(A)\ \ \ 
    \text{s.t.}\label{eq:poly-obj}\\
    &\langle x_i, Ax_i\rangle \le 1 \ \ \forall i \in [m]\\
    &A \succ 0\label{eq:poly-psd},
  \end{align}
  By Lemma~\ref{lm:fact-convex} this is a convex minimization problem
  and its value equals $\lambda(U)$.

  The conjugate function $f^*$ of $f$ is defined on self-adjoint
  linear operators $Q:X^* \to X$ by:
  \[
  f^*(Q) \eqdef \sup\{\tr(QA) - f(A): A \succ 0\}.
  \]
  Since $f^*(Q)$ is the supremum of affine functions, it is convex and
  lower semicontinuous. The set on which $f^*$ takes a finite value is
  called its domain. The significance of $f^*$ is the fact
  \begin{equation}\label{eq:lagrange}
  \lambda(U) = 
  \sup\left\{-\sum_{i = 1}^m{q_i} - 
    f^*\left(-\sum_{i = 1}^m{q_i x_i\otimes x_i}\right):
      q_1, \ldots, q_m \ge 0\right\}.
  \end{equation}
  This follows from the duality theory of convex optimization, since
  \eqref{eq:poly-obj}--\eqref{eq:poly-psd} satisfies Slater's
  condition, which in this case reduces to just checking the existence
  of a feasible $A$ (see~\cite[Chapter 5]{BoydV04}). We proceed to
  compute $f^*(Q)$.

  It is easy to see that unless $-Q \succeq 0$, $f^*(Q) = \infty$,
  and, conversely, if $-Q \succeq 0$, $f^*(Q) \le 0 <
  \infty$. Therefore, the domain of $f^*$ is $\{Q: -Q \succeq 0\}$. We
  will first handle the case $-Q \succ 0$, and then we will extend our
  formula for $f^*(Q)$ to $-Q \succeq 0$ by continuity. Assume then
  that $-Q \succ 0$. As $f$ is a differentiable convex function, the
  range of the derivative $\nabla f$ includes the relative
  interior of the domain of $f^*$, i.e.~the set of linear operators
  $\{X: -X \succ 0\}$ (see Corollary~26.4.1.~in~\cite{Rockafellar});
  from \eqref{eq:f-derivative} it is also apparent that $\nabla f(A)$
  is negative definite for any positive definite $A$, so the range of
  $\nabla f(A)$ is exactly $\{X: -X \succ 0\}$. This means that
  the equation 
  \[
  0 = \nabla(\tr(QA) - f(A)) = Q - \nabla f(A)
  \]
  has a solution over $A \succ 0$, and $f^*(Q)$ is achieved at this
  solution. Fix $A:X \to X^*$ to be a solution to this equation, and
  let $R:Y \to \ell_2^n$ be an invertible map such that
  $\ell^*(R) \le 1$ and 
  \[
  f(A) = \tr((RUA^{-1}U^*R^*)^{1/2}) = \tr(((RU)^{-*}A(RU)^{-1})^{-1/2}).
  \]
  The equations $\nabla f(A) = Q$ and \eqref{eq:f-derivative} imply
  \[
  ((RU)^{-*} A (RU)^{-1})^{-3/2} = -2 RUQU^*R^*,
  \]
  and, therefore,
  \begin{align*}
    f^*(Q) &= \tr(QA) - \tr((RUA^{-1}U^*R^*)^{1/2})\\
    &= \tr((RUQU^*R^*)((RU)^{-*}A(RU)^{-1})) -   \tr(((RU)^{-*}A(RU)^{-1})^{-1/2})\\
    &= -\frac{3}{2^{2/3}} \tr((-RUQU^*R^*)^{1/3}).
  \end{align*}
  We have proved that
  \begin{equation}\label{eq:conjugate-lb}
  f^*(Q) \ge 
  \inf\left\{ -\frac{3}{2^{2/3}} \tr((-RUQU^*R^*)^{1/3}):
  R:Y \to \ell_2^n,\ell^*(R) \le 1\right\},
  \end{equation}
  for any $Q:X^*\to X$ such that $-Q \succ 0$.  Let $\mathcal{D}$ be
  the set of invertible maps $R:Y \to \ell_2^n$ such that $\ell^*(R)
  \le 1$. By \eqref{eq:f-formula} and the definition of the
  conjugate function $f^*$ we also have
  \begin{align*}
  f^*(Q) &= 
  \sup_{A: A \succ 0} 
  \inf_{R \in \mathcal{D}}
  \tr(QA) - \tr((RUA^{-1}U^*R^*)^{1/2})\\
  &\le
  \inf_{R \in \mathcal{D}}
  \sup_{A: A \succ 0} 
  \tr(QA) - \tr((RUA^{-1}U^*R^*)^{1/2}).
  \end{align*}
  For any $R \in \mathcal{D}$, the supremum on the right hand side
  equals the value at $Q$ of the conjugate $h^*_R$ of the function
  $h_R(A) = \tr((RUA^{-1}U^*R^*)^{1/2})$. A calculation analogous to
  the one above for $f^*$ shows that $h_R^*(Q) = -\frac{3}{2^{2/3}}
  \tr((-RUQU^*R^*)^{1/3})$. 

  Therefore,
  \[
    f^*(Q) \le
    \inf\left\{ -\frac{3}{2^{2/3}} \tr((-RUQU^*R^*)^{1/3}):
    R:Y \to \ell_2^n,\ell^*(R) \le 1\right\},
  \]
  and, together with \eqref{eq:conjugate-lb}, we have established 
  \begin{equation}
    \label{eq:conjugate}
    f^*(Q) =
    \inf\left\{ -\frac{3}{2^{2/3}} \tr((-RUQU^*R^*)^{1/3}):
    R:Y \to \ell_2^n,\ell^*(R) \le 1\right\},
  \end{equation}
  for any $Q:X^*\to X$ such that $-Q \succ 0$. Since $f^*$ is a
  a proper lower-semicontinuous function, it is continuous on any
  line segment contained in its domain by Corollary
  7.5.1.~in~\cite{Rockafellar}. Therefore, \eqref{eq:conjugate}
  holds for any $Q \succeq 0$ as well. 
  
  By \eqref{eq:lagrange} and \eqref{eq:conjugate}, 
  \begin{multline}
  \lambda(U) = 
  \sup \biggl\{-\sum_{i = 1}^m{q_i} 
  + \frac{3}{2^{2/3}} \tr\Bigl(\Bigl(-RU\Bigl(\sum_{i = 1}^m{q_i  x_i\otimes  x_i}\Bigr)U^*R^*\Bigr)^{1/3}\Bigr):\\
  R:Y \to \ell_2^n,\ell^*(R) \le 1,
  q_1, \ldots, q_m \ge 0\biggr\}.
  \end{multline}
  Let us write $q = tp$ where $t \ge 0$ is a real number, and $p_1,
  \ldots, p_m \ge 0$ satisfy $\sum_i p_i = 1$. Then we can rewrite the
  equation above as
  \[
  \lambda(U) = 
  \sup\left\{-t + 
    \frac{3t^{1/3}}{2^{2/3}} \tr\Bigl(\Bigl(-RU\Bigl(\sum_{i = 1}^m{p_i  x_i\otimes x_i}\Bigr)U^*R^*\Bigr)^{1/3}\Bigr):
    t, p_1, \ldots, p_m \ge 0, \sum_{i=1}^m p_i = 1\right\}.
  \]
  Maximizing over $t$ finishes the proof. 
\end{proof}

\section{Algorithm for the Factorization Constant}
\label{sect:fact-alg}

In this section we use our convex formulation
\eqref{eq:fact-obj}--\eqref{eq:fact-psd} of the $\lambda(U)$
factorization constant in order to compute an approximately optimal
factorization. In order to use known results in convex optimization,
we need to make sure that we are optimizing a Lipschitz function over
a sufficiently bounded feasible region, and, moreover, that we have a
strictly feasible point. These conditions are not automatically
satisfied for \eqref{eq:fact-obj}--\eqref{eq:fact-psd}, but we can
modify the optimization problem so that they are, at the cost of a
small constant factor approximation to the optimum. 

In this section we assume that both normed spaces $X$ and $Y$ are
defined over $\R^n$. We assume that the unit ball of $X$ is $B_X =
\conv\{\pm x_1, \ldots, \pm x_m\}$, and that $X$ is specified by
giving the points $x_1, \ldots, x_m$ as input to the algorithm. We
assume that $Y$ is specified by an evaluation oracle, which takes a
point $y \in \R^n$ and returns $\|y\|_Y$. Moreover, we will assume
that $U$ is the identity map; otherwise we can define a new norm $Z$
by $\|z\|_Z = \|Uz\|_Y$ and we have $\lambda(U) = \lambda(I)$ for the
identity map $I:X \to Z$. An evaluation oracle for $Y$ easily gives an
evaluation oracle for $Z$.

As a first step, we transform the problem so that $B_X$ is
well-rounded, which makes our problem well-conditioned in a sense that
we will make precise later. To round $B_X$, we use a classical
algorithm of Khachiyan.

\begin{theorem}[\cite{Khachiyan96}]\label{thm:khach-round}
  There exists an algorithm running in time $O(mn^2(\log n + \log \log
  m)$ that, given a set of points $x_1, \ldots, x_m \in \R^n$,
  computes a linear map $T$ such that 
  \[
  \frac{1}{2\sqrt{n}} B_2^n \subseteq T(\conv\{\pm x_1, \ldots, \pm  x_m\})
  \subseteq
  B_2^n.
  \]
\end{theorem}

We compute the linear map $T$ for the extreme points $\pm x_1, \ldots,
\pm x_m$, using the algorithm guaranteed by
Theorem~\ref{thm:khach-round}, and apply $T$ to both $B_X$ and
$B_Y$. I.e.~we replace $X$ with the space whose unit ball is $TB_X$,
and $Y$ with the space whose unit ball is $TB_Y$. This does not change
$\lambda(I)$, $\vb(I)$, or the volume lower bound. With this
transformation, we can assume that 
\begin{equation}\label{eq:rounded}
  \frac{1}{2\sqrt{n}} B_2^n \subseteq B_X \subseteq B_2^n.
\end{equation}

In the rest of this section, we use the notation $A \succeq B$ for two symmetric matrices $A$ and $B$ to denote the fact
that $A-B$ is positive semidefinite. The
notation $A\preceq B$ is equivalent to $B \succeq A$. We also
repeatedly use the fact that if $AA^\T \preceq BB^\T$ for two matrices
$A$ and $B$, then, for a standard Gaussian $Z$ in $\R^n$ and any norm
$Y$ defined on $\R^n$, $\E \|AZ\|^2_Y \leq \E \|BZ\|_Y^2$. This is
well-known, and is due to the fact that $BZ$ is distributed
identically to $AZ + CZ'$
for a standard Gaussian $Z'$ independent from $Z$. Then, by Jensen's inequality,
\[
\E\|AZ\|^2_Y = \E\|AZ + \E[CZ']\|^2_Y
\le \E\|AZ + CZ'\|_Y^2 = \E\|BZ\|_Y^2.
\]


Our first lemma shows that we can strengthen the constraints
\eqref{eq:fact-constr}--\eqref{eq:fact-psd} without affecting the
value of the optimization problem significantly. The stronger
constraints will be helpful in showing that the objective is Lipschitz
over the feasible region. 

\begin{lemma}\label{lm:psd-lb}
  Assume that $X$ and $Y$ are normed spaces over $\R^n$ such that $B_X
  = \conv\{\pm x_1, \ldots, \pm x_m\}$, and equation
  \eqref{eq:rounded} holds, and let $I:X \to Y$ be the identity
  map. Then the value of the following convex optimization problem
  over positive definite matrices $A$ is at least $\lambda(I)$ and at
  most $\sqrt{2} \lambda(I)$:
  \begin{align}
    &\inf  (\E\|A^{-1/2} Z\|^2_Y)^{1/2}  \label{eq:alg-obj}\\
    &\text{s.t.}\notag\\
    &x_i^T A x_i \le 1 \ \ \ \forall i \in [m],\label{eq:alg-constr}\\
    &A \succeq \frac12 I.\label{eq:alg-psd}
  \end{align}
  Above $A^{-1/2}$ is the unique positive definite matrix such that
  $(A^{-1/2})^2 = A^{-1}$, and $Z$ is a standard Gaussian random
  variable in $\R^n$.

  Moreover, any positive definite matrix $A$ satisfying
  \eqref{eq:alg-constr} also satisfies $A \preceq 4n I$. 
\end{lemma}
\begin{proof}
  The objective function \eqref{eq:alg-obj} equals
  \eqref{eq:fact-obj}, the constraints \eqref{eq:alg-constr} are
  equivalent to \eqref{eq:fact-constr}, and \eqref{eq:alg-psd}
  implies \eqref{eq:fact-psd}, so, trivially, the value of
  \eqref{eq:alg-obj}--\eqref{eq:alg-psd} is at least the value of
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd}, which, by
  Lemma~\ref{lm:fact-convex}, equals $\lambda(I)$. In the reverse
  direction, let us take an operator $A:X\to X^*$ achieving the
  optimal value $\lambda(I)$ in
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd}, and let us identify $A$
  with its matrix in the standard basis. Let $\tilde{A} \eqdef \frac12(A +
  I)$. Since $B_X \subseteq B_2^n$ by assumption, and $A$ satisfies
  \eqref{eq:fact-constr}, we have 
  \[
  \frac12 x_i^T (A + I) x_i = \frac{x_i^T A x_i + \|x_i \|_2^2 }{2} 
  \le 1,
  \]
  and, therefore, $\tilde{A}$ satisfies \eqref{eq:alg-constr}. Because
  $A$ is positive definite, we have $\tilde{A} \succeq \frac12 I$, and
  \eqref{eq:alg-psd} is also satisfied, and $\tilde{A}$ is feasible. Because
  $\tilde{A} \succeq \frac12 A$, we have $\tilde{A}^{-1} \preceq
  2A^{-1}$, so
  \[
  \E\|\tilde{A}^{-1/2} Z\|^2_Y \le 2 \E\|A^{-1/2} Z\|^2_Y =
  2\lambda(I)^2.
  \]
  This shows that the value of
  \eqref{eq:alg-obj}--\eqref{eq:alg-psd} is at most
  $\sqrt{2}\lambda(I)$. 

  The statement after ``moreover'' follows because if there exists
  some $x \in \R^n$ for which $x^\T A x > 4n \|x\|_2^2$ then for $y =
  \frac{x}{2\sqrt{n}\|x\|_2}$ we have $y \in B_X = \conv\{\pm x_1,
  \ldots, \pm x_m\}$ but $x^\T A x > 1$. This would imply that for at
  least one extreme point $x_i$ of $B_X$ we have $x^\T A x > 1$, in
  contradiction with \eqref{eq:alg-constr}. 
\end{proof}

Our second lemma shows that the objective function \eqref{eq:alg-obj}
is Lipschitz over the feasible region
\eqref{eq:alg-constr}--\eqref{eq:alg-psd}.
\begin{lemma}
  Under the assumptions of Lemma~\ref{lm:psd-lb}, for any $A, B$
  satisfying \eqref{eq:alg-constr}--\eqref{eq:alg-psd} we have
  \[
  (\E\|A^{-1/2}Z\|_Y^2)^{1/2} -   (\E\|B^{-1/2}Z\|_Y^2)^{1/2} \le 
  16\sqrt{n}\lambda(I) \|A  - B\|_{op},
  \]
  where $\|A - B\|_{op}$ is the largest singular value of $A-B$.
\end{lemma}
\begin{proof}
  Observe that, because any $B$ which is feasible for
  \eqref{eq:alg-constr}--\eqref{eq:alg-psd} satisfies $\frac12 I
  \preceq A \preceq 4nI$, we have
  \[
  \frac{1}{2\sqrt{n}}(\E\|Z\|_Y^2)^{1/2} 
  \le (\E\|B^{-1/2}Z\|_Y^2)^{1/2}  \le \sqrt{2} (\E\|Z\|_Y^2)^{1/2}. 
  \]
  Therefore, for any feasible $B$, by Lemma~\ref{lm:psd-lb}, we have
  \[
  \lambda(I)\le (\E\|A^{-1/2}Z\|_Y^2)^{1/2}
  \le 4 \sqrt{n} \lambda(I).
  \]
  
  Let $\delta \eqdef \|A - B\|_{op}$, and consider first the case
  $\delta > \frac14$.  Then, 
  \[
  (\E\|A^{-1/2}Z\|_Y^2)^{1/2} -   (\E\|B^{-1/2}Z\|_Y^2)^{1/2} 
  \le 
  (\E\|A^{-1/2}Z\|_Y^2)^{1/2} \le 
  4 \sqrt{n} \lambda(I)
  \le 16 \sqrt{n} \lambda(I) \delta.
  \]

  Consider now the case $\delta < \frac14$. Then, $A - B \succeq
  -\delta I$, and we have
  \[
  A = B + (A-B) \succeq B - \delta I \succeq (1 - 2\delta) B.
  \]
  Therefore, 
  \[
  (\E\|A^{-1/2}Z\|_Y^2)^{1/2} 
  \le (1-2\delta)^{-1/2} (\E\|B^{-1/2}Z\|_Y^2)^{1/2}
  \le (1+4\delta) (\E\|B^{-1/2}Z\|_Y^2)^{1/2}.
  \]
  Finally,
  \[
  (\E\|A^{-1/2}Z\|_Y^2)^{1/2} -   (\E\|B^{-1/2}Z\|_Y^2)^{1/2} 
  \le
  4\delta (\E\|B^{-1/2}Z\|_Y^2)^{1/2} 
  \le
  16\sqrt{n} \lambda(I) \delta .
  \]
  This completes the proof
\end{proof}

We are now ready to prove our main algorithmic result. 
\begin{theorem}
  There exists an algorithm that, given $x_1, \ldots, x_m \in \R^n$,
  an evaluation oracle for a norm $Y$ on $\R^n$, and
  a linear operator $U:X \to Y$ specified by its matrix, where $X$ is
  the space with unit ball $B_X = \conv\{\pm x_1, \ldots, \pm x_m\}$,
  computes in time polynomial in $m$ and $n$ a factorisation $U = ST$,
  $S:\ell_2^n \to Y$, $T:X \to \ell_2^n$,  such that 
  \[
  \ell(S) \|T\| \le 2\lambda(U).
  \]
\end{theorem}


\section{The Volume Lower Bound and Convex Hulls}
\label{sec:conv-hulls}

Here we show that the volume lower bound is maximized at the extreme
points of a convex set.

\begin{theorem}\label{thm:conv-hull}
  Let $v_1, \ldots, v_m$ be points in $\R^n$, and let $C \eqdef
  \mathrm{conv}\{\pm v_1, \ldots, \pm v_m\}$. Then, for any $k \in
  \mathbb{N}$, $1 \le k \le n$, and any symmetric convex body $K$ in
  $\R^n$,
  \[
  \sup_{u_1, \ldots, u_k \in C}\vollb((u_i)_{i = 1}^k, K)
  \le
  \vollb^{\rm h}_k((v_i)_{i = 1}^m, K).
  \]
\end{theorem}

We will use a theorem of K.~Ball~\cite{Ball88}, which allows us to
define a norm associated with an arbitrary logarithmically concave
function $f$.
\begin{theorem}\label{thm:ball-logconcave}
  Let $f: \R^k \to [0, \infty)$ be an even logarithmically concave
  function such that $0 < \int_{\R^k} f < \infty$. Then, for any $p
  \ge 1$, 
  \[
  \|x\|_{f,p} \eqdef 
  \begin{cases}
    \left(\int_0^\infty f(rx) r^{p-1}dr\right)^{-1/p}, &x \neq 0,\\
    0, &x = 0.
  \end{cases}
  \]
  defines a norm on $\R^n$. 
\end{theorem}


\begin{proof}[Proof of Theorem~\ref{thm:conv-hull}]
  Let us fix a a sequence of linearly independent vectors $u_1,
  \ldots, u_{k-1} \in C$ for the remainder of the proof,
  and, for any $x\in \R^n$, define the matrix $U_x = (u_1, \ldots
  u_{k-1}, x)$.  To prove the theorem, it is enough to show that the
  function $g: \R^n \to [0, \infty)$ defined by
  \[
  %g(x) = \frac{\det(U_x^\T U_x)^{1/2}}{\vol_k(K \cap \lspan\{u_1,
  %  \ldots, u_{k-1}, x\})}
  g(x) = \vollb((u_1, \ldots, u_{k-1}, x), K)^k 
  = \frac{1}{\vol_k(\{a \in \R^k: U_x a \in K\})}
  \]
  achieves its maximum on $C$ at one of the extreme points $\pm v_1,
  \ldots, \pm v_m$. This follows immediately if $g$ is
  convex. Below, we use Theorem~\ref{thm:ball-logconcave} to prove the
  convexity of $g$.
  
  Let  $W = \lspan\{u_1, \ldots, u_{k-1}\}$ and let $\pi$ be
  the orthogonal projection onto the orthogonal complement $W^\perp$
  of $W$. Notice that 
  \[
  \vol_k(\{a \in \R^k: U_x a \in K\}) 
  = \vol_k(\{a \in \R^k: U_{\pi x} a \in K\}).
  \]
  To show that that $g$ is convex, it is, therefore, enough to show
  that it is convex on $W^\perp$.  For any $x \in W^\perp$, define 
  \begin{align*}
    L_x &= \{b \in \R^{k-1}: Ub + x \in K\},
  \end{align*}
  where $U$ is the matrix $(u_1, \ldots, u_{k-1})$. It is
  easy to check that for any two $x, x' \in W^\perp$, and any $\alpha
  \in [0,1]$, $\alpha L_{x} + (1-\alpha) L_{x'} \subseteq L_{\alpha x
    + (1-\alpha)x'}$. Therefore, by the Brunn-Minkowski inequality,
  the function $h(x) = \vol(L_x)$ is logarithmically
  concave. Moreover, by the symmetry of $K$, $L_{-x} = -L_x$, so $h(x)
  = h(-x)$.  By Theorem~\ref{thm:ball-logconcave} it follows that 
  \begin{align*}
  g(x) &=\frac{1}{\int_{-\infty}^\infty h(tx) dt}
  = \frac{1}{2\int_{0}^\infty h(tx) dt}
  = \frac{1}{2}\|x\|_{h,1}.
  \end{align*}
  is a norm on $W^\perp$, and, therefore, convex. The theorem follows.
\end{proof}

\snote{Do we need to state Brunn-Minkowski anywhere?}

\cut{
Let $v_1, \ldots, v_m \in \R^n$ and define a norm $X$ on $\R^n$ with
unit ball $B_X = \mathrm{conv}\{\pm v_1, \ldots, \pm v_m\}$. By
Theorem~\ref{thm:tightness}, we have that 
\[
1 \le \frac{\beta(X, Y)}{\sup_{u_1, \ldots, u_n \in
    B_X}\vollb((u_i)_{i = 1}^n, Y)} \le C K(Y)(1+\log n).
\]
Theorem~\ref{thm:conv-hull} allows us to write the stronger inequality
\[
1 \le \frac{\beta(X, Y)}{\vollb((v_i)_{i = 1}^m, Y)} \le C K(Y)(1+\log n).
\]
This also implies that
\begin{equation}\label{eq:beta-conv-hull}
\beta(X, Y) \le CK(Y)(1+\log n) \beta((v_i)_{i = 1}^m, Y),
\end{equation}
i.e.~the vector balancing constant does not increase much when we take
convex hulls. 

\medskip\noindent
\textbf{Questions}:
\begin{itemize}
\item Does \eqref{eq:beta-conv-hull} hold with $CK(Y)(1+\log n)$
  replaced by a fixed constant?
\item Is there a more direct proof of \eqref{eq:beta-conv-hull}?
\end{itemize}
}

\bibliographystyle{alpha}
\bibliography{Discrepancy}
\end{document}
