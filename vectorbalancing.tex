\documentclass[12pt]{article}

\usepackage{amsfonts,amsmath,amsthm,amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%\newif\ifbigfont\bigfonttrue
\newif\ifbigfont\bigfontfalse

\ifbigfont

\usepackage[right=.5in,left=.5in,top=.6in,bottom=.6in]{geometry}
\usepackage[17pt]{extsizes}
\usepackage{newpxmath,newpxtext}

\else

\usepackage[margin=1in]{geometry}
\usepackage{newpxmath,newpxtext}

\fi

\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\heading}[1]{\vspace{1ex}\par\noindent{\bf\boldmath #1}}

\newcommand{\cut}[1]{}
\newcommand{\ceil}[1]{\lceil #1 \rceil}

\newcommand{\R}{{\mathbb{R}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\N}{{\mathbb{N}}}
\renewcommand{\S}{\mathbb{S}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\T}{\mathsf T}

\newcommand\eps{\varepsilon}
\newcommand{\eqdef}{\triangleq}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\newcommand{\set}[1]{\left\{ #1 \right\}}

\DeclareMathOperator{\vollb}{volLB}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\hd}{hd}
\DeclareMathOperator{\vb}{vb}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\lspan}{span}
\DeclareMathOperator{\speclb}{specLB}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\bnds}{bounds}

\DeclarePairedDelimiter\norm{\lVert}{\rVert}

% MARGIN NOTES
\newif\ifnotes\notestrue
%\newif\ifnotes\notesfalse

\ifnotes
\usepackage{color}
%\definecolor{mygrey}{gray}{0.50}

\ifbigfont

\newcommand{\notename}[2]{{\textcolor{red}{{\bf (#1:} {#2}{\bf ) }}}}

\else

\newcommand{\notename}[2]{{\textcolor{red}{\footnotesize{\bf (#1:} {#2}{\bf ) }}}}

\fi

\newcommand{\noteswarning}{{\begin{center} {\Large WARNING: NOTES ON}\end{center}}}
\newcommand{\knote}[1]{{\notename{Kunal}{#1}}}
\newcommand{\nnote}[1]{{\notename{Nicole}{#1}}}
\newcommand{\dnote}[1]{{\notename{Daniel}{#1}}}
\newcommand{\snote}[1]{{\notename{Sasho}{#1}}}


\else

\newcommand{\notename}[2]{{}}
\newcommand{\noteswarning}{{}}
\newcommand{\knote}[1]{}
\newcommand{\nnote}[1]{}
\newcommand{\dnote}[1]{}
\newcommand{\snote}[1]{}

\fi



\begin{document}
\title{Vector Balancing}
\maketitle

\noteswarning


\section{Definitions and Statement of the Volume Lower Bound}

In what follows, we use we will use $\inner{\cdot}{\cdot}$ for the standard
inner product on $\R^n$. Duality and traces are defined with respect to this
inner product. $K^\circ$ denotes the polar of a set $K \subseteq \R^n$. We use
$\vol_n(K)$ for the $n$-dimensional volume (standard Lebesgue measure in $\R^n$)
of $K$ and $\kappa_n$ for the volume $B_2^n$, the unit Euclidean ball. For a
symmetric convex body $K$, we define $\norm{x}_K = \min \set{s \geq 0: x \in sK}$
to be the norm induced by $K$.

Let $C$ and $K$ be two symmetric convex bodies in $\R^n$. We define the vector
balancing constant of $C$ and $K$ by 
\[
\vb(C, K) \eqdef \sup\Bigg\{ 
\min_{\eps_1, \ldots,\eps_N \in \{-1, 1\}} \Bigl\|\sum_{i = 1}^N \eps_i
u_i \Bigr\|_{K}: N \in \mathbb{N}, u_1, \ldots, u_N \in C \Bigg\}.
\]
Given a sequence $(u_1, \ldots, u_N)$ of vectors in $\R^n$, we define the
discrepancy and hereditary discrepancy with respect to $K$ as follows:
\begin{align*}
\disc((u_i)_{i = 1}^N,K) &\eqdef \min_{\eps_1, \ldots,\eps_N \in
  \{-1, 1\}}
\Bigl\|\sum_{i = 1}^N{\eps_i u_i}\Bigr\|_{K};\\
\hd((u_i)_{i = 1}^N, K) &\eqdef \max_{S \subseteq  [N]}{\disc((u_i)_S, K)}.
\end{align*}
It follows trivially from the definitions that 
\[
\vb(C, K) = \sup\{\hd((u_i)_{i = 1}^N, K)\},
\]
where the supremum is taken over all finite sequence of vectors in $C$.

% For a non-singular matrix $A \in \R^{n \times k}$, we define $\detb(A) :=
% \det(A^\T A)^{1/2}$.  
% Let $G$ be the Gram matrix of $u_1, \ldots, u_N$, i.e. $g_{ij} =
% \langle u_i, u_j \rangle$, where $\langle \cdot, \cdot \rangle$ is
% the standard inner product on $\R^n$.  Banaszczyk~\cite{Bana93}
% established the following lower bound on $\beta((u_i)_{i = 1}^N, Y)$:
% \[
% \beta((u_i)_{i = 1}^N, Y) \ge \frac{\det(G)^{1/(2n)}}{\vol_n(B_Y)^{1/n}}.
% \]
% It is easy to see that this inequality is not always tight: for
% example, it is possible that $\beta((u_i)_{i = 1}^N, Y) > 0$, but the
% vectors $u_1, \ldots, u_N$ are not linearly independent, so $\det(G) =
% 0$. However, the inequality can be  strengthened by
% maximizing over subsequences of $(u_i)_{i = 1}^N$:
% \begin{equation}
%   \label{eq:vol-lb}
%   \beta((u_i)_{i = 1}^N, Y) \ge \vollb((u_i)_{i = 1}^N, Y) \eqdef
%   \max_{k = 1}^n \max_{S \subseteq [N]:|S| = k} \frac{\det(G_{S,S})^{1/(2k)}}{\vol_k(B_Y \cap \lspan\{u_i: i \in S\})^{1/k}}.
% \end{equation}
% Here $G_{S,S}$ is the principle submatrix of $G$ whose rows and
% columns are indexed by the set $S$.

For vectors $x,y \in \R^n$, we define $\inner{x}{y} = x^\T y$ to be the standard
inner product in $\R^n$. For a matrix $A \in \R^{n \times k}$, we will be
interested in the volume of the parallelepiped generated by the columns of $A$,
which can be computed by $\vol_k(A[0,1)^k) = \det(A^\T A)^{1/2}$.  Banaszczyk
established the following volumetric lower bound on hereditary discrepancy:

\begin{lemma}[\cite{Bana93}]
\label{eq:vol-lb}
Let $U = (u_1,\dots,u_N) \in \R^{n \times N}$ and $K \subseteq \R^n$ be a
symmetric convex body. For $S \subseteq [N]$, $|S| = k \in [n]$, where $(u_i)_{i
\in S}$ are linearly independent, define  
\begin{equation}
\label{eq:vol-lb}
\vollb((u_i)_{i \in S}, K) 
                         \eqdef \frac{\det(U_S^\T U_S)^{1/(2k)}}{\vol_{k}(K \cap
\lspan\{u_i: i \in S\})^{1/k}}.
\end{equation}
where $U_S$ denotes the columns of $U$ indexed by $S$. Then, letting
\[
\vollb_{\rm h}((u_i)_{i=1}^N, K) \eqdef \max_{\substack{S \subseteq [N] \\ {\rm
rank}(U_S) = |S|}} \vollb((u_i)_{i \in S},K),
\]
we have that
\[
\hd((u_i)_{i=1}^N,K) \geq \frac{1}{2} \vollb_{\rm h}((u_i)_{i=1}^N,K) .
\]
\end{lemma}

\section{Preliminaries}

We define $\gamma_n$ to be the standard Gaussian measure on $\R^n$, that is
$\gamma_n(A) = \frac{1}{\sqrt{2\pi}^n} \int_A e^{-\|x\|^2/2}$. We will often use
the $k$-dimensional Gaussian measure restricted to $k$-dimensional linear
subspace $H$ of $\R^n$, for which we use the notation $\gamma_H$.

For an $n \times n$ positive definite matrix $A$, we define the ellipsoid $E(A)
= \set{x \in \R^n: x^\T A x \leq 1}$. We recall that the polar ellipsoid
$E(A)^\circ = E(A^{-1})$, and that $\vol_n(E(A)) = \kappa_n \det(A)^{-1/2}$.

For a linear subspace $W \subseteq \R^n$, we denote the orthogonal projection
onto $W$ by $\pi_W$. For $S \subseteq [n]$, we write $\pi_S$ to denote the
projection onto the coordinate subspace $\lspan(e_i: i \in S)$.

For a symmetric convex body $K \subseteq \R^n$ and subset $S \subseteq [n]$, we
denote the coordinate section of $K$ on $S$ by $K_S \eqdef \set{x \in K: x_i =
0, ~\forall i \notin S}$. We use $B_p^n$, $p \in [1,\infty]$, to denote the unit
$\ell_p$ bal in dimension $n$, $B_p^S := (B_p^n)_S$ and $B_p^W := (B_p^n) \cap W$
for corresponding coordinate and general sections. For a vector $x \in
[-1,1]^n$, we write $\bnds(x) = \set{i \in [n]: x_i \in \set{-1,1}}$.

\section{Tightness of the Volume Lower Bound}

In this section, we will show that the volume lower bound \eqref{eq:vol-lb} is
tight within a logarithmic factor. 

\begin{theorem}\label{thm:tightness}
There exists a universal constant $C \geq 1$ such that the following
holds. Let $K \subseteq \R^n$ be a convex body $\R^n$, and let $u_1,
\ldots, u_N \in \R^n$. Then,
\[
1 \le \frac{\hd((u_i)_{i = 1}^N, K)}{\vollb_{\rm h}((u_i)_{i = 1}^N, K) } \le C(1+\log n).
\]
Furthermore, there exists a polynomial time algorithm which can compute
colorings matching the upper bound. 
\end{theorem}

The main technical result of this section is that the volume lower bound,
restricted to subsets of size at least $\Omega(n)$, is in fact an upper bound on
the discrepancy of so-called partial colorings. This allows us to easily recover
Theorem~\ref{thm:tightness} using $O(\log n)$ partial coloring phases in the
standard way. We state our technical result below, restricted to the case where
the vectors are aligned with the standard basis. We note that since the output
norm is general, this is essentially without loss of generality. 

\begin{lemma}[Partial Colorings via Volume] \label{lem:partial-via-volume}
There exists a universal constants $C \geq 1, \eps_0 \in (0,1), \delta \in (0,1)$, such that
for any $y \in (-1,1)^n$ and symmetric convex body $K \subseteq \R^n$ satisfying
$\forall S \subseteq [n]$, $|S| = \ceil{\delta n}$, $\vol_{|S|}(K_S) \geq 1$, there
exists a polynomial time algorithm which with high probability finds $x \in
[-1,1]^n$ with $|\bnds(x)| \geq \ceil{\eps_0 n}$ and
$x-y \in C K$.  
\end{lemma}

We now give the straightforward reduction from Theorem~\ref{thm:tightness} to
Lemma~\ref{lem:partial-via-volume}.

\begin{proof}[Proof of Theorem~\ref{thm:tightness}]
By~\eqref{eq:vol-lb}, we may restrict attention to the upper bound. In particular,
given $u_1,\dots,u_N \in \R^n$ and a convex body $K$ in $\R^n$, it suffices to
show that $\disc((u_i)_{i=1}^N) \leq C(1+\log n)\vollb_{\rm h}((u_i)_{i=1}^N,K)$.  

To begin, we compute a basic solution to the linear program $\sum_{i=1}^N x_i
u_i = 0$, $x \in [-1,1]^N$. After relabeling, we may assume the variables not
hitting their $\set{-1,1}$ bounds are $x_1,\dots,x_l$, noting that if there are
no such variables we already have a $0$ discrepancy coloring. Since
$x$ is basic, we know that the vectors $u_1,\dots,u_l$ must be linearly
independent. Therefore, we may apply a invertible linear
transformation $T:\R^n \rightarrow \R^n$ sending $u_1,\dots,u_l$ to
$e_1,\dots,e_l$. In particular, letting $K' := TK$, we have that 
\begin{align*}
\disc((u_i)_{i=1}^N,K) &\leq \min_{z \in \set{-1,1}^l} \|\sum_{i=1}^l z_i e_i +
\sum_{i=l+1}^N x_i T u_i\|_{K'} \\
                &= \min_{z \in \set{-1,1}^l} \|\sum_{i=1}^l (z_i-x_i)e_i\|_{K'}, 
\end{align*}
Furthermore, a direct computation shows that
\[
\vollb_{\rm h}((u_i)_{i=1}^l,Y) = \vollb_{\rm h}((e_i)_{i=1}^l,K') = \max_{S \subseteq [l]}
\vol_{|S|}(K'_S)^{-1/|S|} \text{ .}
\] 
Let us assume that we have computed $M > 0$ satisfying 
\[
M/2 \leq \vollb_{\rm h}((e_i)_{i=1}^l,K') \leq M. 
\]
From here, it suffices to compute $z \in
\set{-1,1}^l$ such that $\sum_{i=1}^l (z_i-x_i) e_i \in O(\log n M) K'$. Note
that by assumption on $M$, $\vol(MK'_S) \geq 1, \forall S \subseteq [l]$.
Therefore, repeatedly applying Lemma~\ref{lem:partial-via-volume} on $MK'$, letting $x^0 := x$ we compute a sequence
$x^1,\dots,x^T \in [-1,1]^l$, $T = \lceil \log l/\eps_0
\rceil = O(\log n)$, such that $\forall t \in [T]$, we have that

\begin{enumerate}
\item $\sum_{i=1}^l (x^t_i-x^{t-1}_i)e_i \in O(M) K'$.
\item $|\set{i \in [l]: x^t_i \in (-1,1)}| \leq (1-\eps_0)|\set{i \in [l]:
x^{t-1}_i \in (-1,1)}|$.
\end{enumerate}

By our choice of $T$, it is direct to check that $x^T \in \set{-1,1}^l$ and by
the triangle inequality that $\sum_{i=1}^l (x^T_i-x^0_i)e_i \in T M K' = O(\log n
M) K'$. Thus, setting $z = x^T$ satisfies the requirements.

We now discuss the computation of $M$. We first note that 
\[
M_1 := \max_{i \in [l]} \vol_1(K'_i)^{-1} = \max_{i \in [l]} \|e_i\|_{K'} =
\max_{i \in [l]} \|u_i\|_K \text{ ,}
\]
and thus the restricted maximum can be efficiently computed. Note that by
construction ${\rm conv}(\pm e_1,\dots, \pm e_l)/M_1 \subseteq K'_{[l]}$. Thus,
for any $S \subseteq [l]$,$|S|=k$, we see that
\[
\vol_k(l M_1 K'_S) \geq \vol_{k}(l \cdot {\rm conv}(\pm e_i: i \in S)) = \frac{(2l)^k}{k!}
 \geq 1 \text{ .}
\]
In particular, we get that $M_1 \leq \vollb_{\rm h}((e_i)_{i=1}^l,K') \leq l M_1$.
Hence, as input to the stage above we may successively try the values $M_1
2^k$, $k \in \set{0,\dots,\log_2 l}$, stopping the first time we find a valid
coloring. 
\end{proof}

Lemma~\ref{lem:partial-via-volume} should be viewed as a volumetric analogue of
a theorem of Rothvoss~\cite{rothvoss-giann}, who both extended and made
algorithmic vector balancing results of Giannopolous~\cite{giannop}. \dnote{I
would like to say something about how you probably can't get this type of result
using the determinant lower bound. In particular, you lose a log there even for
partial coloring in $\ell_\infty$. I think this is better save for the intro,
where its easier to compare things.} We state a slight variant of~\cite[Lemma 9]{rothvoss-giann} below.

\begin{theorem}[Partial Colorings via Gaussian Measure]\label{thm:roth-giann}
Let $0 < \eps \leq 1/60000$ and $\delta = \frac{3}{2}\eps \log_2
\frac{1}{\eps}$. Let $K \subseteq \R^n$ be a symmetric convex body and assume
that for some subspace $H \subseteq \R^n$ of dimension at least $(1-\delta)n$,
we have that $\gamma_H(K) \geq e^{-\delta n}$. Then for any $y \in (-1,1)^n$,
there exists a polynomial time algorithm which with high probability finds $x
\in [-1,1]^n$ satisfying $|\bnds(y)| \geq \eps n/2$ and
$x-y \in CK$.
\end{theorem}

We recall that Rothvoss' algorithm, for the special case $y = 0$ (the general
case is similar), works by computing the Euclidean projection of a Gaussian
random vector onto $K \cap [-1,1]^n$. The above statement deviates from the
corresponding Lemma in~\cite{rothvoss-giann} in that it does not assume that $K
\subseteq H$ or that $H$ is known to the algorithm. It is not hard to verify
however that this condition is not needed in analysis, so we defer discussion of
the proof of this statement to the full version. The flexibility gained by not
needing to know the subspace in advance will be very useful in the sequel. We
note that one can also adapt the analysis of the algorithm of Singh and
Eldan~\cite{ES14}, which maximizes over $K \cap [-1,1]^n$ using the Gaussian as
the objective vector instead of projecting it, to work in the above setting.   

Our proof of Lemma~\ref{lem:partial-via-volume} will in fact be a direct
reduction to Rothvoss' theorem. The core of our reduction is the following
geometric theorem, which shows that if all the coordinate sections of $K$ of
proportional dimension have large volume, then there exists a subspace $H$ of
proportional dimension on which $K$ has large Gaussian measure. 

\begin{theorem}[Gaussian Measure via Volume]
\label{thm:gauss-via-volume}
There exists a decreasing function $\eta: (0,1) \rightarrow \R_+$ such that the
following holds. For any $n \in \N$, $K \subseteq \R^n$ symmetric convex body,
$2 \leq k \leq n-1$, $\alpha = k/n$, such that $\forall S \subseteq [n]$,
$|S| = \alpha n$, $\vol_{\delta n}(K_S) \geq 1$, there exists a linear
subspace $H$ of dimension $(1-\delta)n$ for which $\gamma_H(\eta(\delta) K) \geq
e^{-\delta n}$.
\end{theorem}

We note that the above theorem is primarily interesting in the case where
$\delta$ is a fixed constant, as $\eta(\delta) = e^{O(1/\delta)}$ blows up quite
quickly as $\delta \rightarrow 0$. Lemma~\ref{lem:partial-via-volume} now
follows directly combining the above with Theorem~\ref{thm:roth-giann}, as shown
below.

\begin{proof}[Proof of Lemma~\ref{lem:partial-via-volume}] 
Let $\eps = 1/60000$ and $\delta = (3/2) \eps \log_2(1/\eps)$. By
Theorem~\ref{thm:gauss-via-volume}, $\eta(\delta) K$ satisfies the conditions
for applying Theorem~\ref{thm:roth-giann} on any $x \in (-1,1)^n$ with
`parameters $\eps$ and $\delta$ as in the last sentence. This yieds
Lemma~\ref{lem:partial-via-volume} with $\eps_0 = \eps/2$, $\delta = \delta$ and
$C = O(\eta(\delta)) = O(1)$, as needed.   
\end{proof}

While there are examples where the volume lower bound is a $O(\log n)$ factor
off from hereditary discrepancy (e.g.~$3$ permutations), we conjecture that the
volume lower bound actually characterizes a hereditary version of partial
coloring discrepancy. Namely, if the volume lower bound is $D$, we conjecture
that there exists a subset vectors for which every (fractional) partial coloring
has discrepancy $\Omega(D)$. Recall that Lemma~\ref{lem:partial-via-volume}
gives the other direction, i.e.~that there always exist partial colorings of
discrepancy $O(D)$. If this conjecture were true, then Rothvoss' algorithm,
which we use as a blackbox, would in a weak sense be optimal for finding
partial colorings. We discuss this conjecture in more detail in the next
subsection. 

Comparing to prior works, Theorem~\ref{thm:gauss-via-volume} provides a useful
and different route for proving that a body (or at least a large section of it)
has exponentially small Gaussian measure. In the context of discrepancy, to the
authors knowledge, only two main techniques were used to prove such bounds,
neither of which is directly applicable in the above setting. The first
technique consists of combining chaining techniques and moment bounds, which can
generally only measure when the body has Gaussian measure close to $1/2$.  This
approach loses the leverage we have in only needing exponentially small bounds
and thus often incurs additional logarithmic factors. The second strategy is
based on the positive correlation properties of Gaussian measure, first proved
for the intersection of symmetric slabs (i.e.~Sidak's lemma), and with the
recent resolution of the Gaussian correlation conjecture~\cite{Royen14}, for the
intersection of arbitrary symmetric convex bodies. More precisely, one tries to
show that $K$ contains (or equals) the intersection of ``simpler'' symmetric
convex bodies $K_1,\dots,K_T$ (most often slabs) to deduce that $\gamma_n(K)
\geq \prod_{i=1}^T \gamma_n(K_i)$, from which one can usefully get exponentially
small bounds.

In contrast, our proof of the $e^{-\delta n}$ lower bounds on Gaussian measure
in the above theorem proceeds via a direct covering argument. Namely, if one can
show that $e^{\delta n-1}$ translates of $K$ cover the Euclidean ball of radius
$\sqrt{n}$, which has Gaussian measure $\geq 1/2$, then one can directly deduce
that 
\[
1 \leq 2\gamma_n(\sqrt{n}B_2^n) \leq 2N(\sqrt{n} B_2^n,K) \max_{t \in \R^n}
\gamma_n(K+t) \leq e^{\delta n} \gamma_n(K) ,
\]
where we have used that $\max_{t \in \R^n} \gamma_n(K+t) = \gamma_n(K)$ for a
centrally symmetric convex body, which follows by the symmetry and logconcavity
of Gaussian measure. We will adopt this strategy on a section of $K$, which is
chosen to align with the longest axes of a so-called regular M-ellipsoid for
$K$. The volumetric condition in Lemma~\ref{lem:partial-via-volume} will in fact
be used to guarantee that these axes have length $\Omega(\sqrt{n})$, which makes
the above strategy plausible. We recall that an M-ellipsoid~ $E$ of $K$ is an
ellipsoid which approximates $K$ well from the perspective of covering,
i.e.~$2^{O(n)}$ shifts of $K$ suffice to cover $E$ and vice versa. The existence
of such ellipsoids was first proven by Milman~\cite{Milman86-reverseBM}. We give
precise definitions below. 

For any two sets $A,B \subseteq \R^n$, let 
\[
N(A,B) = \min \set{|\Lambda|: \Lambda \subset \R^n, A \subseteq \Lambda+B} ,
\]
denote the minimum number of shifts of $B$ needed to cover $A$. The following
theorem of Pisier~\cite{Pisier-book}, gives the existence of M-ellipsoids whose
covering estimates have polynomial decay. The decay estimate will be used to
make the Gaussian measure of the large section of $K$ we find as close to $1$ as
we like after a sufficient scaling. 

\begin{theorem}[Regular M-ellipsoid]
There exists an absolute constant $c_0 > 0$, such that any $0 < \alpha < 2$,
letting $\sigma(\alpha) = c_0(2-\alpha)^{-1/2}$, $n \in \N$, and symmetric
convex body $K \subseteq \R^n$, there exists an ellipsoid $E \subseteq \R^n$,
$\vol_n(E)=\vol_n(K)$, such that for all $t \geq 1$
\[
\max \set{N(K,tE),N(E,tK),N(K^\circ,tE^\circ),N(E^\circ,tK^\circ)} \leq
e^{\sigma(\alpha) n / t^\alpha} \text{ .}
\]
\end{theorem}

To relate volumes of sections of $K$ to corresponding projection of $K^\circ$,
we will need the following well-known inequality:

\begin{theorem}[Blashke-Santal{\'o}]\label{thm:santalo} 
Let $K \subseteq \R^n$ be a symmetric convex body. Then $\vol_n(K)
\vol_n(K^\circ) \leq \kappa_n^2$, where equality holds if and only if $K$ is
an origin centered ellipsoid. 
\end{theorem}

To show that the axes of the M-ellipsoid of $K$ are long, we will need to relate
the axis lengths to the volumes of coordinate projections of the polar
ellipsoid. For this, purpose we will require the following formula for
coordinate projection volumes. 

\begin{lemma}\label{lem:ellipsoid-volumes}
Let $E := E(Q) \subseteq \R^n$ be an origin center ellipsoid. Then, for any
$S \subseteq [n]$, $|S| = k$, we have that 
\[
\vol_k(\pi_S(E^\circ)) = \kappa_k \det(Q_{S,S})^{1/2} \text{ .}
\]
\end{lemma}
\begin{proof}
Recall that $E^\circ = E(Q^{-1}) = Q^{1/2} B_2^n$, where $Q^{1/2}$ is the
positive definite square root of $Q$. To begin, we recall that the support
function of $E^\circ$ can be computed by
\begin{align*}
h_{E^\circ}(w) &\eqdef \max_{y \in E^\circ} \inner{w}{y} 
               = \max_{z \in B_2^n} \inner{w}{Q^{1/2} z} 
               = \|Q^{1/2} w\| = \sqrt{w^\T Q w}.
\end{align*}
Let $W_S = \lspan(e_i: i \in S)$. Note that by construction, $\pi_S(E^\circ)
\subseteq W_S$ and $h_{\pi_S(E^\circ)}(w) = h_{E}(w)$, $\forall w \in W_S$.
Furthermore, by duality, among convex bodies these conditions uniquely define $\pi_S(E^\circ)$. 

Let $s_1 < s_2 < \dots < s_k$ be the elements of $S$ and let $P_S =
(e_{s_1},\dots,e_{s_k})$, noting that $P_S P_S^\T = \pi_S$. Let $T = P_S
(Q_{S,S})^{1/2} \in \R^{n \times k}$. We now show that $TB_2^k = \pi_S(E^\circ)$
using the aforementioned conditions. Clearly $TB_2^k \subseteq W_S$ since
$\lspan(P_S) = W_S$. For $w \in W_S$, letting $w_S = (w_{s_1},\dots,w_{s_k})^\T$
denote the restriction to the coordinates in $S$, we have that 
\begin{align*}
h_{TB_2^k}(w) &= \max_{z \in B_2^k} \inner{T^\T w}{z} 
              = \max_{z \in B_2^k} \inner{(Q_{S,S})^{1/2} w_S}{x} \\ 
              &= \sqrt{w_S^\T Q_{S,S} w_S} = \sqrt{w^\T Q w} ,
\end{align*} 
where the last equality follows since $w_i = 0$ for $i \notin S$. Thus
$\pi_S(E^\circ) = TB_2^k$ as claimed. The volume can now be computed as follows:
\begin{align*}
\vol_k(TB_2^k) &= \kappa_k \det(T^\T T)^{1/2} 
               = \kappa_k \det((Q_{S,S})^{1/2} (P_S^\T P_S)
(Q_{S,S})^{1/2})^{1/2} \\
               &= \kappa_k \det(Q_{S,S})^{1/2} \text{ ,}
\end{align*}
as needed.
\end{proof}

The next lemma we will need is a simple determinantal analogue of the restricted
invertibility principle.

\begin{lemma}\label{lm:rip-det}
  Let $Q$ be an $n\times n$ real positive semi-definite matrix with
  eigenvalues $\lambda_1 \ge \ldots \ge \lambda_n$. For any integer
  $k$, $1 \le k \le n$, there exists a set $S \subseteq [n]$ of size $k$
  such that
  \[\prod_{i=1}^k \lambda_i \leq \binom{n}{k} \det(Q_{S,S}).\]
\end{lemma}
\begin{proof}
To prove the lemma, we will rely on the classical identity for applying the
elementary symmetric polynomials to the eigen values of $Q$:
\begin{equation*}
\sum_{S \in [n],|S|=k} \prod_{i \in S} \lambda_i \eqdef p_k(\lambda) = \sum_{S \subset [n]: |S| = k}\det(Q_{S,S}).
\end{equation*}
To verify this equation, consider the coefficient of $t^{n-k}$ in the polynomial
$\det(Q + tI)$. Calculating the coefficient using the Leibniz formula for the
determinant gives the right hand side; calculating it using $\det(Q + tI) =
(\lambda_1 + t)\ldots(\lambda_n + t)$ gives the left hand side. Since the eigen
values are all non-negative, we get that
\[
\prod_{i=1}^k \lambda_i \leq p_k(\lambda) =  
 \sum_{S \subseteq [n]: |S|=k} \det(Q_{S,S}) \leq \binom{n}{k} \max_{S
\subseteq [n]: |S|=k} \det(Q_{S,S}),
\]
as needed.
\end{proof}

We now have all the ingredients needed to prove our main geometric estimate.

\begin{proof}[Proof of Theorem~\ref{thm:gauss-via-volume}]
Let $E := E(Q)$ denote a $1$-regular M-ellipsoid for $K$ and let $\sigma :=
\sigma(1)$. Let $l_1 \geq \dots \geq l_n > 0$ denote
the length of the principal axes of $E$, where we recall that
$l_n^{-2}\geq \dots \geq l_1^{-2} > 0$ are then the eigen values of $Q$.

By the Blashke-Santal{\'o} inequality, for all $S \subseteq
[n]$, $|S|=\delta n$, we have that 
\[
\vol_{\delta n}(K_S)\vol_{\delta n}((K_S)^\circ) = 
\vol_{\delta n}(K_S) \vol_{\delta n}(\pi_S(K^\circ)) \leq
\kappa_{\delta n}^2 \text{ .}
\]
Since we assume $\vol_{\delta n}(K_S) \geq 1$, the above implies that
$\vol_{\delta n}(\pi_S(K^\circ)) \leq \kappa_{\delta n}^2$. The
coordinate projections of $E^\circ$ thus have volume at most 
\[
\vol_{\delta n}(\pi_S(E^\circ)) \leq N(E^\circ,K^\circ)
\vol_{ \delta n}(\pi_S(K^\circ)) \leq e^{\sigma n} \kappa_{\delta n}^2 \text{ .}
\]
Combining Lemma~\ref{lm:rip-det} and~\ref{lem:ellipsoid-volumes}, we have that
\begin{align*}
\prod_{i=(1-\delta)n+1}^n l_i^{-1} &\leq \binom{n}{\delta n}^{1/2} \max_{S \subseteq
[n],|S|=\delta n} \det(Q_{S,S})^{1/2} \\
&= \binom{n}{\delta n}^{1/2} \max_{S \subseteq [n],|S|=\delta n}
\vol_{\delta n}(\pi_S(E^\circ)) \kappa_{\delta n}^{-1} \\
&\leq \binom{n}{\delta n}^{1/2} e^{\sigma n} \kappa_{\delta n} \text{ .}
\end{align*}
From here, we conclude that 
\[
l_{(1-\delta)n} \geq \prod_{i=(1-\delta)n+1}^n l_i^{\frac{1}{\delta
n}} \geq \binom{n}{\delta 
n}^{-\frac{1}{2\delta n}} e^{-\frac{\sigma}{\delta }} \kappa_{\delta
n}^{-\frac{1}{\delta n}} \geq 
\frac{1}{e^{2\frac{\sigma}{\delta }}} \cdot \sqrt{\frac{\delta n}{2\pi e}} :=
c(\delta)^{-1} \sqrt{n} \text{ .}
\] 
Letting $H$ be the span of the first $(1-\delta)n$ principal
axes of $E$, we thus conclude that 
\[
\sqrt{n} (B_2^n \cap H) \subseteq c(\delta) (E \cap H). 
\]
Using the $1$-regularity of $E$, letting $t = 2 \sigma / \delta$, we derive the
following covering estimate 
\begin{align*}
N(\sqrt{n} (B_2^n \cap H), 2 t c(\delta) (K \cap H))
&\leq N(c(\delta)(E \cap H), 2 t c(\delta) (K \cap H)) \\
&\leq N(E, t K) \leq e^{\sigma n / t} = e^{\delta n/2} \text{ .}
\end{align*}
Since $\gamma_H(\sqrt{n} B_2^n \cap H) \geq 1/2$, setting $\eta := \eta(\delta)
=\frac{2c(\delta)\sigma}{\delta}$, we get that $\gamma_H(\eta K \cap H) \geq
\frac{1}{2} e^{-\delta n/2} \geq e^{-\delta n}$, as needed. Lastly, $\eta$ as
defined above is easily checked to be decreasing in $\delta$.
\end{proof}

\subsection{The Discrepancy of Partial Colorings}

In this section we discuss a geometric conjecture which would imply that a tight
relationship between the discrepancy of partial colorings and the volume lower
bound, and thus a weak form of optimality for Rothvoss' partial coloring
algorithm. For this purpose, we formally define the partial coloring discrepancy
as well as its hereditary version. Given $(u_i)_{i=1}^N \in \R^n$, symmetric
convex body $K \subseteq \R^n$ and $\alpha \in (0,1]$,  we define  
\begin{equation}
\label{def:partial-disc}
\begin{split}
&\disc_\alpha(( u_i)_{i=1}^N,K) \eqdef \min_{\substack{x \in [-1,1]^N \\
|\bnds(x)| \geq \alpha N}} \Bigl\|\sum_{i=1}^N x_i u_i\Bigr\|_K .
\end{split}
\end{equation}
and
\begin{equation}
\label{def:partial-hd}
\hd_\alpha((u_i)_{i=1}^N,K) \eqdef \max_{S \subseteq [N]} \disc_\alpha((u_i)_{i \in S}, K) .
\end{equation}

We recall that (repeated applications) of Lemma~\ref{lem:partial-via-volume}
implies the upper bound
\[
\hd_{1/2}((u_i)_{i=1}^N,K) \leq O(1) \vollb_{\rm h}((u_i)_{i=1}^N,K).
\]
Here we conjecture that the reverse inequality should also hold.

\begin{conjecture}
\label{conj:partial-volume}
There exists a universal constant $c \geq 1$, such that for any $n \in \N$,
$u_1,\dots,u_n \in \R^n$ linearly independent and symmetric convex body $K
\subseteq \R^n$: 
\begin{equation}
\vollb_{\rm h}((u_i)_{i=1}^n,K) \leq c \hd_{1/2}((u_i)_{i=1}^n,K)
\end{equation}
\end{conjecture}

Note that we restrict above to linear independent subsets of vectors, but as is
well-known (e.g.~see proof of Theorem~\ref{thm:tightness}), this is without loss
of generality. As a pathway to prove the conjecture, we suggest the following
natural geometric analog of the so-called spectral lower bound for discrepancy
into $\ell_2$.

\begin{lemma}
\label{lem:speclb}
Let $(u_i)_{i=1}^n \in \R^n$ be linearly independent, $K \subseteq \R^n$ be a
symmetric convex body, and $\alpha \in [0,1]$. For any subset $S \subseteq [n]$,
$|S|=k$, letting $W_S := \lspan(u_i \in S)$, define
\[
\speclb((u_i)_{i \in S},K) := 
\max \set{r \geq 0: r K \cap W_S \subseteq k \conv(\pm u_i: i \in S)}.
\]
Then, we have that
\begin{equation}
\label{eq:spec-lb}
\disc_\alpha((u_i)_{i \in S},K) \geq \alpha \speclb((u_i)_{i \in S},K).
\end{equation}
In particular, defining
\[
\speclb_{\rm h}((u_i)_{i=1}^n,K) := \max_{S \subseteq [n]} \speclb((u_i)_{i \in
S},K)
\]
we have that
\begin{equation}
\label{eq:hd-spec-lb}
\hd_{\alpha}((u_i)_{i=1}^n,K) \geq \alpha \speclb_{\rm h}((u_i)_{i=1}^n,K).
\end{equation}
\end{lemma}
\begin{proof}
We prove only~\eqref{eq:spec-lb}, since then \eqref{eq:hd-spec-lb} follows
trivially. For~\eqref{eq:spec-lb}, by replacing $K$ by $K \cap W_S$, we may wlog
assume that $|S|=n$ and $W_S = \R^n$. 

Let $x \in [-1,1]^n$, $|\bnds(x)| \geq \alpha n$, and $u_x = \sum_{i=1}^n x_i
u_i$. Our goal is to show that $\beta := \norm{u_x}_K \geq \alpha r$, where $r
:= \speclb((u_i)_{i=1}^n,K)$.

Let $(u_i^*)_{i=1}^n$ denote the corresponding dual basis of
$(u_i)_{i=1}^n$, i.e.~satisfying $\inner{u_i^*}{u_j} = 1$ if $i=j$ and $0$
otherwise, which exists by linear independence. Now letting $v_x =
\sum_{i=1}^n {\rm sign}(x_i) u_i^*$, it is easy to check that 
\[
\inner{v_x}{u_x} = \sum_{i=1}^n |x_i| \geq |\bnds(x)| \geq \alpha n .
\]
Since $u_x \in \beta K$ and $r K \subseteq n \conv(\pm u_i: i
\in [n])$, we have that
\begin{align*}
\alpha n \leq \beta \max_{z \in K} \inner{v_x}{z} 
         \leq \frac{\beta}{r} n 
               \max_{z \in \conv(\pm u_i: i \in [n])} \inner{v_x}{z}
          = \frac{\beta}{r} n .
\end{align*}
The desired inequality now follows by rearranging.
\end{proof}

We note that as with $\vollb_{\rm h}$, one may extend the $\speclb_{\rm h}$ to
an arbitrary sequence of vectors $(u_i)_{i=1}^N$, however one must take care to
optimize only over subsets of linearly independent vectors, since otherwise the
conclusion of Lemma~\ref{lem:speclb} is false.

Given the above, it suffices to prove Conjecture~\ref{conj:partial-volume} with
$\hd_{1/2}$ replaced by $\speclb_{1/2}$. The resulting stronger conjecture has a
very natural geometric interpretation which we expand on below. For
$(u_i)_{i=1}^n \in \R^n$ linearly independent and $K \subseteq \R^n$ a symmetric
convex body, letting $T$ denote the linear map sending $(u_i)_{i=1}^n$ to
$(e_i)_{i=1}^n$, it is direct to check that $\tau((u_i)_{i=1}^n,K) =
\tau((e_i)_{i=1}^n,TK)$ for $\tau \in \set{\speclb_{\rm h},\vollb_{\rm h}}$.
Thus, for the purpose of the conjecture, it suffices to consider the setting
where the vectors are the standard basis. In this setting, we see that
\[
\speclb_{\rm h}((e_i)_{i=1}^n,K) = \max_{S \subseteq [n]} \max \set{r \geq
0: r K_S \subseteq |S| B_1^S} 
\]
and that 
\[
\vollb_{\rm h}((e_i)_{i=1}^n,K) = \max_{S \subseteq [n]}
\vol_{|S|}(K_S)^{-1/|S|} \text{ .} 
\]
The goal is now to show that for every $S_0 \subseteq [n]$, there exists $S_1
\subseteq [n]$, such that 
\begin{equation}
\label{eq:conj-simple}
\vol_{|S_0|}(K_{S_0})^{-1/|S_0|} \leq c \max \set{r \geq 0: r K_{S_1} \subseteq
|S_1| B_1^{|S_1|}} \text{ .}
\end{equation}
Since this must hold for every symmetric convex body $K$, we may assume that
$S_0 = [n]$ (and thus $S_1 \subseteq S_0$). Furthermore, by homogeneity, we may
also assume that $\vol_n(K)=1$. In this case~\eqref{eq:conj-simple}, and hence
Conjecture~\ref{conj:partial-volume}, directly reduces to the following
geometric conjecture.

\begin{conjecture}[Restricted Isometry Principle for Convex Bodies]
\label{conj:conv-restr-iso} There exists an absolute constant $c \geq 1$, such
for any $n \in \N$, symmetric convex body $K \subseteq \R^n$ of volume $1$,
there exists $S \subseteq [n]$ such that $K_S \subseteq c |S| B_1^S$.
\end{conjecture}

Another natural, and again stronger conjecture, would be to ask for containment
inside $c \sqrt{|S|} B_2^S \subseteq c |S| B_1^S$. We suspect that this version
should also hold, and indeed, our evidence for the conjecture supports this
belief. 

Two natural weakenings of Conjecture~\ref{conj:conv-restr-iso} are to ask
whether (a) it holds for ellipsoids and (b) whether it holds for general bodies
but with coordinate sections replaced by arbitrary sections. As our main
evidence for the conjecture, we show that both assertions indeed hold with the
stronger containment relation with respect to $B_2^n$. We note that (a) indeed
implies Conjecture~\ref{conj:partial-volume} when $K$ is an ellipsoid.  

Before formally stating our partial results, we give a last simplification of
Conjecture~\ref{conj:conv-restr-iso}. In particular, when attempting to prove
the conjecture we may additionally assume that $\vol_{|S|}(K_S) \geq 1$ for all
$S \subset [n]$. This follows by noting that if there exists $S \subset [n]$
with $\vol_{|S|}(K_S) < 1$, we may simply apply induction on $K_S$, after
scaling it up to have volume $1$ (which only makes the task more difficult). The
main two lemmas we can prove, from which the above results are easily follow,
are stated below. 

\begin{lemma}
\label{lem:axis-m-ell}
Let $K \subseteq \R^n$ be a symmetric convex body of volume $1$ satisfying
for all $S \subseteq [n]$, $\vol_{|S|}(K_S) \geq 1$. Let $E \subseteq \R^n$ be a
$1$-regular M-ellipsoid of $K$. Then there exists $S \subseteq [n]$, $|S| =
\Theta(n)$, such that $E_S \subseteq O(\sqrt{|S|}) B_2^S$.
\end{lemma}

\begin{lemma}
\label{lem:cover-to-section}
Let $K \subseteq \R^n$ be a symmetric convex body such that $N(K,\sqrt{n}B_2^n)
\leq 2^{O(n)}$. Then there exists a linear subspace $W \subseteq \R^n$, $\dim(W) =
\Theta(n)$, such that $K \cap W \subseteq O(\sqrt{|W|}) B_2^W$.  
\end{lemma}

To derive the conjecture when $K$ is an ellipsoid, we simply apply
Lemma~\ref{lem:axis-m-ell} with $E=K$. To derive the conjecture for general
sections, we first apply Lemma~\ref{lem:axis-m-ell} to $K$, noting that the
produced section $K_S$ now satisfies the conditions of
Lemma~\ref{lem:cover-to-section}, from which we derive the result.  

For the proof of Lemma~\ref{lem:axis-m-ell}, we use the volumetric conditions to
show that the central axes of $E$ all have length $\Theta(\sqrt{n})$
(complementing the $\Omega(\sqrt{n})$ lower bound from
Theorem~\ref{thm:gauss-via-volume}), and derive the existence of the
corresponding section from the Bourgain-Tzafriri restricted isometry principle
applied to the quadratic form defining $E$. The proof of
Lemma~\ref{lem:cover-to-section} follows relatively directly from Milman's
quotient of subspace (QS) theorem, which states that there exists a projection
of a section of $K$ of proportional dimension which is constant factor
isomorphic to a Euclidean ball. We remark that the main step that seems to be
missing for the proof of Conjecture~\ref{conj:conv-restr-iso} is a
corresponding ``coordinate version'' of the QS theorem. As the proofs of the
above lemmas require a fair amount of machinery, and only yield partial evidence
for a conjecture, we defer detailed proofs to the full version.  

\section{Convex Hulls}

Here we show that the volume lower bound is maximized at the extreme
points of a convex set.

\begin{theorem}\label{thm:conv-hull}
  Let $v_1, \ldots, v_m$ be points in $\R^n$, and let $K \eqdef
  \mathrm{conv}\{\pm v_1, \ldots, \pm v_m\}$. Then, for any normed
  space $Y$ on $\R^n$, 
  \[
  \sup_{u_1, \ldots, u_n \in K}\vollb((u_i)_{i = 1}^N, Y)
  \le
  \vollb((v_i)_{i = 1}^m, Y).
  \]
\end{theorem}

We will use a theorem of K.~Ball~\cite{Ball88}.

\begin{theorem}\label{thm:ball-logconcave}
  Let $f: \R^k \to [0, \infty)$ be an even logarithmically concave
  function such that $0 < \int_{\R^k} f < \infty$. Then, for any $p
  \ge 1$, 
  \[
  \|x\|_{f,p} \eqdef 
  \begin{cases}
    \left(\int_0^\infty f(rx) r^{p-1}dr\right)^{-1/p}, &x \neq 0,\\
    0, &x = 0.
  \end{cases}
  \]
  defines a norm on $\R^k$. 
\end{theorem}


\begin{proof}[Proof of Theorem~\ref{thm:conv-hull}]
  It is enough to show that, for any integer $k$ between $1$ and $n$,
  if we keep $u_1, \ldots, u_{k-1} \in \R^n$ fixed, and for any $x\in
  \R^n$ define $G_x$ to be the Gram matrix of the vectors $u_1,
  \ldots, u_{k-1}, x$, then the function $g: \R^n \to [0, \infty)$
  defined by
  \[
  g(x) = \frac{\det(G_x)^{1/(2)}}{\vol_k(B_Y \cap \lspan\{u_1,
    \ldots, u_{k-1}, x\})}
  \]
  achieves its maximum on $K$ at an extreme point. Furthermore, this
  follows immediately if $g$ is convex. Below we prove the convexity
  of $g$.

  Let $G$ be the Gram matrix of $u_1, \ldots, u_{k-1}$, and let $W =
  \lspan\{u_1, \ldots, u_{k-1}\}$. Denote by $P_{W^\perp}$ the
  orthogonal projection onto the orthogonal complement of $W$.
  Observe first that
  \[
  g(x) = \frac{\det(G)^{1/2}\|P_{W^\perp}x\|_2}{\vol_k(B_Y \cap (W +
    \lspan\{x\}))}. 
  \]
  To show that that $g$ is convex, it is enough to show that it is
  convex on $W^\perp$.  For $x \in W^\perp$, define $f(x) \eqdef
  \vol_{k}(B_Y \cap (W + x))$. By the Brunn-Minkowski inequality and
  symmetry of $B_Y$, $f$ is en even log-concave function on
  $W^\perp$. Notice that
  \[
  \vol_{k+1}(B_Y \cap  (W + \lspan\{x\}))
  = 
  \int_{-\infty}^\infty{f(tx/\|x\|_2)dt} 
  = 
  2\|x\|_2 \int_{0}^\infty{f(tx)dt}.
  \]
  Therefore, 
  \[
  g(x) = 
  \frac{\|x\|_2}{\vol_{k+1}(B_Y \cap  (W + \lspan\{x\}))}
  = 
  \frac{1}{2\int_{0}^\infty{f(tx)dt}}
  = \frac{1}{2}\|x\|_{f,1}
  \]
  is a norm on $W^\perp$, by Theorem~\ref{thm:ball-logconcave} (after
  identifying $W^\perp$ with $\R^{n-k+1}$), and, therefore, convex. The
  theorem follows.
\end{proof}


Let $v_1, \ldots, v_m \in \R^n$ and define a norm $X$ on $\R^n$ with
unit ball $B_X = \mathrm{conv}\{\pm v_1, \ldots, \pm v_m\}$. By
Theorem~\ref{thm:tightness}, we have that 
\[
1 \le \frac{\beta(X, Y)}{\sup_{u_1, \ldots, u_n \in
    B_X}\vollb((u_i)_{i = 1}^n, Y)} \le C K(Y)(1+\log n).
\]
Theorem~\ref{thm:conv-hull} allows us to write the stronger inequality
\[
1 \le \frac{\beta(X, Y)}{\vollb((v_i)_{i = 1}^m, Y)} \le C K(Y)(1+\log n).
\]
This also implies that
\begin{equation}\label{eq:beta-conv-hull}
\beta(X, Y) \le CK(Y)(1+\log n) \beta((v_i)_{i = 1}^m, Y),
\end{equation}
i.e.~the vector balancing constant does not increase much when we take
convex hulls. 

\medskip\noindent
\textbf{Questions}:
\begin{itemize}
\item Does \eqref{eq:beta-conv-hull} hold with $CK(Y)(1+\log n)$
  replaced by a fixed constant?
\item Is there a more direct proof of \eqref{eq:beta-conv-hull}?
\end{itemize}

\section{The Factorization Approach}

In the inductive proof of Theorem~\ref{thm:tightness} we need to keep
alternating between computing Gaussian estimates based on volume
information, and applying Theorem~\ref{thm:roth-giann}, as we induct on
lower dimensional subspaces. One downside of this (except the
complexity of the proof) is that the proof is not too useful in
computing $\beta(X, Y)$. Next we explore a different approach, which
aims at separating computing a Gaussian estimate from volume
information, on one hand, and the vector balancing argument on the
other. The approach is a (significant) generalization of the one used
in~\cite{disc-gamma2}. 

Recall that for an operator $S:\ell_2^n \to Y$, where $Y$ is a Banach
space, we define the $\ell$-norm as
\[
\ell(S) \eqdef \left( \int \|u(x)\|_Y^2 d\gamma_n(x) \right)^{1/2},
\]
where $\gamma_n$ is the standard Gaussian measure on $\R^n$. The dual
norm is defined for an opartor $R: Y \to \ell_2^n$ by 
\[
\ell^*(R) = \sup\{\tr(RS): S: \ell_2^n \to Y, \ell(S) \le 1\}.
\]

Let $X$ and $Y$ be normed spaces defined on $\R^n$, and let $U:X \to
Y$ be a linear operator. We define a factorization constant
\[
\lambda(U) \eqdef \inf \{\ell(S)\|T\|: T: X \to \ell_2^n, S: \ell_2^n
\to Y, U = ST\}.
\]

\medskip\noindent
\textbf{Questions}:
\begin{itemize}
\item Is $\lambda$ a norm? (Probably not.) A quasi-norm? (Probably yes.)
\end{itemize}


We have the following theorem.

\begin{theorem}\label{thm:factorization}
  There exists a constant $C$ such that for any two $n$-dimensional
  normed spaces $X, Y$ we have
  \[
  \beta(X,Y) \le C\lambda(I_{X,Y}),
  \]
  where $I_{X,Y}:X \to Y$ is the formal identity operator. 
\end{theorem}

The theorem is a consequence of the following result by Banaszczyk~\cite{bana}.
\begin{theorem}\label{thm:bana}
  Let $K$ be a convex body in $\R^n$ such that $\gamma_n(K) \ge
  \frac12$. Then, for any sequence of vectors $v_1, \ldots, v_N \in
  B_2^n$, there exist signs $\eps_1, \ldots, \eps_N$ such that
  \[
  \sum_{i = 1}^N{\eps_i v_i}\in 5K.
  \]
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:factorization}]
  Let $u_1, \ldots, u_N \in B_X$ be arbitrary, and let $T:X \to
  \ell_2^n$, $S:\ell_2^n \to Y$ be such that $\ell(S)\le
  \lambda(I_{X,Y}) + \eps$ and $\|T\| \le 1$. For $i \in \{1, \ldots,
  N\}$, define $v_i \eqdef Tu_i$; by assumption, $v_i \in B_2^n$ for
  all $i$. Let $K = \sqrt{2}\ell(S) S^{-1}(B_Y)$, and let
  $\|\cdot\|_K$ be the norm with unit ball $K$. Observe that for any
  $x \in \R^n$, $\|x\|_K = \frac{1}{\sqrt{2}\ell(S)}\|Sx\|_Y$. By
  Chebyshev's inequality,
  \[
  1 - \gamma_n(K) \le \int \|x\|_K^2 d\gamma_n(x)
  =
  \frac{1}{2\ell(S)^2} \int \|Sx\|_Y^2 d\gamma_n(x)
  = \frac{1}{2}.
  \]
  We can, therefore, apply Theorem~\ref{thm:bana}, and have that there
  exist signs $\eps_1, \ldots, \eps_N$ such that 
  \[
  \sum_{i =1}^N{\eps_i v_i} \in 5K
  \iff
  \sum_{i = 1}^N{\eps_i ST u_i} \in (5\sqrt{2}\ell(S))B_Y
  \iff 
  \left\|\sum_{i = 1}^N{\eps_i u_i}\right\|_Y \le 5\sqrt{2}(\lambda(I_{X,Y})
    + \eps).
  \]
  Sending $\eps$ to $0$ and taking a supremum over
  $u_1, \ldots, u_N \in B_X$  finishes the proof.
\end{proof}

We would like to prove that the inequality
Theorem~\ref{thm:factorization} holds in the reverse direction as
well, up to a poly-logarithmic factor, i.e.~to show that
$\lambda(I_{X,Y}) \le (\log n)^{O(1)}\beta(X,Y)$.

\subsection{Convex Formulation}

The factorization constant $\lambda(I_{X,Y})$ can be formulated as the
solution to a convex optimization problem, i.e.~as the infimum of a
convex function over a convex domain. This fact is useful in that it
allows applying generic convex optimization techniques to compute
$\lambda(I_{X,Y})$ given appropriate access to the unit balls of $X$
and $Y$. It will also be useful in deriving a dual formulation of
$\lambda(I_{X,Y})$ in the subsequent section. 

We will use the notation $\nu(A)$ for the nuclear norm of an operator
$A:\ell_2^n \to \ell_2^n$, i.e.~the sum of its singular values. We say
an operator $M:\R^n \to \R^n$ is positive definite if $\langle x,
Mx\rangle = \langle Mx, x\rangle > 0$ for all nonzero $x \in \R^n$,
and we denote $M \succ 0$.  


In the following lemma we define a
function which playes a key role in our development, and we derive
some of its properties.

\begin{lemma}\label{lm:obj-f}
  Let $X$ and $Y$ be two normed spaces defined on $\R^n$. Let $f$ be
  the function that maps a positive definite operator $M: X \to X^*$
  to
  \[
  f(M) \eqdef  \ell(I_{X,Y}M^{-1/2}),
  \]
  where $M^{-1/2}:\ell_2^n \to X$ is the unique positive definite operator such that
  $M^{-1/2}(M^{-1/2})^* = M^{-1}$. Then
  \begin{itemize}
  \item $f$ is a differentiable convex function on $\{M: M \succ 0\}$;
  \item $f$ is given by the formula
    \begin{equation}\label{eq:f-formula}
    f(M) = \sup\{\tr((RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2}): 
    R: Y \to \ell_2^n, \ell^*(R) \le 1\};
    \end{equation}
  \item the derivative of $f$ at $M$ is 
    \begin{equation}\label{eq:f-derivative}
    \nabla f(M) = 
    -\frac{1}{2} (RI_{X,Y})^{-1} ((RI_{X,Y})^{-*} M (RI_{X,Y})^{-1})^{-3/2} (RI_{X,Y})^{-*},
    \end{equation}
    where $R: Y \to \ell_2^n$, $\ell^*(R) \le 1$ is such that $f(M) = 
    \tr((RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2})$.
  \end{itemize}
\end{lemma}

First we need an auxiliary lemma. 

\begin{lemma}\label{lm:invertible}
  Let $Y$ be an $n$-dimensional normed space, and let $S:\ell_2^n \to
  Y$ be an invertible linear operator. Then any operator $R:Y \to
  \ell^2_n$ such that $\tr(RS) = \ell(S)$ is invertible.
\end{lemma}
\begin{proof}
  Assume for contradiction that $R$ is not invertible, i.e.~it has a
  non-trivial kernel. Then, if $P$ is the orthogonal projection onto
  the kernel of $R$, we have $\ell(S) = \tr(RS) = \tr(R(I-P)S) \le
  \ell((I-P)S)$. Let $k$ be the dimension of the kernel of $R$, $W$ be
  the $k$-dimensional linear subspace such that $S(W)$ is the kernel
  of $R$, and let $K = S^{-1}(B_Y)$. Using integration by parts,
  we have
  \begin{align*}
  \ell((I-P)S)^2 &= \int_{t = 0}^\infty (1-\gamma_{n-k}(\sqrt{t}K\cap W^\perp))dt\\
  \ell(S)^2 &= \int_{t = 0}^\infty (1-\gamma_n(\sqrt{t}K))dt\\
  &= \int_{t = 0}^\infty \int_{W}(1-\gamma_{n-k}(\sqrt{t}K \cap (y + W^\perp))d\gamma_{k}(y))dt
  \end{align*}
  By the logconcavity of Gaussian measure and the symmetry of $K$,
  $\gamma_{n-k}(\sqrt{t}K \cap (y + W^\perp)) \le \gamma_{n-k}(\sqrt{t}K \cap
  W^\perp)$ for all $y \in W$, and, for all $y$ outside a
  compact set we have $\gamma_{n-k}(\sqrt{t}K \cap (y + W^\perp)) = 0 <
  \gamma_{n-k}(\sqrt{t}K \cap W^\perp)$. Therefore, $\ell((I-P)S) < \ell(S)$, a
  contradiction.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lm:obj-f}]
  We begin with the proof of differentiability. Since $\| \cdot \|_Y$
  is Lipschitz, by Rademacher's theorem it is almost everywhere
  differentiable. Therefore, for any invertible operator $A:\ell_2^n
  \to Y$, the function $g_x(A) = \|Ax\|_Y$ is differentiable in $A$
  for almost all $x$. Also, the function $h$ that maps positive
  definite operators $M$ on $\R^n$ to the principle square root
  $M^{-1/2}$ of $M^{-1}$ has a Frechet derivative $Dh$. Using the
  dominated convergence theorem and the chain rule, we can show that
  the derivative of $f$ at $M$ is
  \[
  \nabla f(M) = \frac{1}{f(M)} \int Dh(M)^*(\nabla g_x(M^{-1/2})) d\gamma_n(x).
  \]
  Above, $Dh(M)^*$ is the dual of the linear operator $Dh(M)$. This
  completes the proof of differentiability. 

  Next we prove the identity \eqref{eq:f-formula}. Observe that
  for any orthogonal transformation $U$ and any transformation $A:
  \ell_2^n \to Y$, $\ell(AU) = \ell(A)$ by the rotational invariance
  of the Gaussian measure. Therefore, 
  \begin{align*}
  \ell(I_{X,Y}M^{-1/2}) &= \sup\{\ell(I_{X,Y}M^{-1/2}U): U \text{ orthogonal}\}\\
  &= \sup\{\tr(RI_{X,Y}M^{-1/2}U): R:Y\to\ell_2^n, \ell^*(R) \le 1, U \text{ orthogonal}\}\\
  &= \sup\{\nu(RI_{X,Y}M^{-1/2}): R:Y\to\ell_2^n, \ell^*(R) \le 1\}\\
  &= \sup\{\tr((RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2}): R:Y\to\ell_2^n, \ell^*(R) \le  1\}. 
  \end{align*}
  Given this formula, in order to prove convexity, it is enough to
  prove that for any operator $A:X \to \ell^n_2$, the function
  $\tr((AM^{-1}A^*)^{1/2})$ is convex in $M$ for $M$ positive
  definite. We use a standard argument based on majorization, which we
  sketch next.  By continuity, we may assume that $A$ is invertible,
  so that $AM^{-1}A^* = (A^{-*}MA^{-1})^{-1}$; let us denote $B =
  A^{-1}$. Let $\alpha \in (0,1)$ be arbitrary, and let $M_1, M_2$ be
  two positive definite operators on $\R^n$. Let $\mu$ map a
  self-adjoint operator on $\R^n$ to the vector of its eigenvalues. It
  is well-known that $\mu(B^*(\alpha M_1 + (1-\alpha)M_2)B)$ is
  majorized by $\alpha \mu(B^*M_1B) + (1-\alpha) \mu(B^*M_2B)$. Let
  $g$ be the function defined on vectors $x \in \R^n$ with positve
  coordinates by $g(x) = \sum_{i=1}^n{x_i^{-1/2}}$. It is easy to
  verify that $g$ is convex and Schur-convex, so
  \[
  g(\mu(B^*(\alpha M_1 + (1-\alpha)M_2)B))
  \le
  g(\alpha \mu(B^*M_1B) + (1-\alpha) \mu(B^*M_2B))
  \le 
  \alpha g(\mu(B^*M_1B)) + (1-\alpha) g(\mu(B^*M_2B)).
  \]
  Since the left hand side above equals   
  \[
  \tr((B^*(\alpha M_1 + (1-\alpha)M_2)B)^{-1/2})
  = 
  \tr((A(\alpha M_1 + (1-\alpha)M_2)^{-1}A^*)^{1/2}),
  \]
  and the right hand side equals
  \[
  \alpha \tr((B^*M_1B)^{-1/2}) +  (1- \alpha) \tr((B^*M_2B)^{-1/2})
  = 
  \alpha \tr((AM_1^{-1}A^*)^{1/2}) +  (1- \alpha) \tr((AM_2^{-1}A^*)^{1/2}),
  \]
  we have established convexity.   

  From \eqref{eq:f-formula}, we can see that the subgradient of
  $f$ at $M$ is
  \begin{align*}
  \partial f(M) &= \mathrm{conv}
  \{\nabla_M \tr((RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2}):\\
  &\hspace{10em}
  R: Y \to \ell_2^n, \ell^*(R) \le 1,
  f(M) = \tr((RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2})\}\\
  &= 
  \mathrm{conv}\{
  -\frac{1}{2} 
  (RI_{X,Y})^{-1} ((RI_{X,Y})^{-*} M (RI_{X,Y})^{-1})^{-3/2} (RI_{X,Y})^{-*}:\\ 
  &\hspace{10em}
  R: Y \to \ell_2^n, \ell^*(R) \le 1, 
  f(M) = \tr((RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2})\}.
  \end{align*}
  Above we used the fact that $\nabla \tr(X^{-1/2}) =
  -\frac{1}{2}X^{-3/2}$ for any positive definite $X$
  (see~\cite{Lewis95}), and Lemma~\ref{lm:invertible}, which implies
  that any $R$ satisfying $f(M) =
  \tr((RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2})$ is invertible. Since $f$
  is differentiable, we have that $\partial f(M)$ is a singleton set,
  i.e.~
  \[
  \nabla f(M) = 
  -\frac{1}{2} (RI_{X,Y})^{-1} ((RI_{X,Y})^{-*} M (RI_{X,Y})^{-1})^{-3/2} (RI_{X,Y})^{-*}
  \]
  for the an invertible operator $R: Y \to \ell_2^n$, such that
  $\ell^*(R) \le 1$ and $f(M) =
  \tr((RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2})$. This finishes the proof
  of the lemma.
\end{proof}

We are now ready to formulate $\lambda(I_{X,Y})$ as a convex
minimization problem. 

\begin{theorem}\label{thm:fact-convex}
  For any two $n$-dimensional normed spaces $X$ and $Y$, and the
  formal identity $I_{X,Y}:X \to Y$, $\lambda(I_{X,Y})$ equals
  \begin{align}
    &\inf %\ell(M^{-1/2})
    f(M)  \label{eq:fact-obj}\\
    &\text{s.t.}\notag\\
    &M: X \to X^*,\|M\| \le 1\label{eq:fact-constr}\\
    &M \succ 0.\label{eq:fact-psd}
  \end{align}
  The function $f$ is the one defined in Lemma~\ref{lm:obj-f}.
  %$M^{-1/2}$ is the inverse of the principle square root
  %$M^{1/2}$ of $M$, and is taken as an operator from $\ell_2$ to $Y$. 

  Moreover, the objective \eqref{eq:fact-obj} and the constraints
  \eqref{eq:fact-constr}--\eqref{eq:fact-psd} are convex in $M$.
\end{theorem}
\begin{proof}
  The convexity of the constraints
  \eqref{eq:fact-constr}--\eqref{eq:fact-psd} is apparent from the
  definition, and the convexity of the objective was proved in
  Lemma~\ref{lm:obj-f}. We proceed to show that the value of
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd} equals $\lambda(I_{X,Y})$.

  Let $T:X \to \ell_2^n$ and $S:\ell_2^n \to Y$, $ST = I_{X,Y}$ be a
  factorization achieving $\lambda(I_{X,Y})$ such that $\|T\| = 1$ and
  $\ell(S) = \lambda(I_{X,Y})$. Then we claim that $M \eqdef T^*T$
  satisfies \eqref{eq:fact-constr}--\eqref{eq:fact-psd} and achieves
  value $\ell(S)$, so the value of
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd} is at most
  $\ell(S) = \lambda(I_{X,Y})$. Indeed, $M$ is clearly positive semidefinite,
  and must be positive definite, as $T$ is
  invertible. Furthermore, since $\|T\|\le 1$, we have that for any $x
  \in B_X$,
  \[\langle x, Mx \rangle = \langle Tx, Tx\rangle =
  \|Tx\|^2_2 \le 1.\] 
  On the other hand, since $M$ is self-adjoint,
  $\|M\| = \sup\{\langle x, Mx\rangle: x \in B_X\}$, and we have shown
  that $\|M\| \le 1$. Since $M^{-1} = T^{-1}T^{-*}$, we have that
  there exists an orthogonal transformation $U$ such that $M^{-1/2} =
  T^{-1}U$. By the rotational invariance of the Gaussian measure, 
  \[
  \ell(I_{X,Y}M^{-1/2}) = 
  \ell(I_{X,Y}  T^{-1}U) = 
  \ell(I_{X,Y}  T^{-1})  = \ell(S) = \lambda(I_{X,Y}).
  \]
  This finishes the proof of the claim that the value of
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd} is at most $\lambda(I_{X,Y})$
  
  Next we prove the reverse inequality. Given a feasible solution $M$
  to \eqref{eq:fact-obj}--\eqref{eq:fact-psd}, we can construct a
  factorization $S \eqdef I_{X,Y}M^{-1/2}$, $T \eqdef M^{1/2}$ of
  $I_{X,Y}$, where $M^{1/2}$ is the principle square root of $M$ and
  $M^{-1/2}$ is its inverse. By
  \eqref{eq:fact-constr}--\eqref{eq:fact-psd}, for any $x \in B_X$ we have
  \[
  \|Tx\|^2_2 = \langle Tx, Tx\rangle = \langle x, Mx \rangle  \le
  1,\] so $\|T\| \le 1$. Moreover, $\ell(S) = f(M)$ by
  definition. This proves that the value of
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd} is at least
  $\lambda(I_{X,Y})$, and, since we already showed that it is also at
  most $\lambda(I_{X,Y})$, the two are equal.
\end{proof}

\subsection{Dual Formulation}


Next we give a dual formulation of $\lambda(I_{X,Y})$ as a
supremum over ``dual certificates''. Such a formulation is very useful
in reversing the inequality in Theorem~\ref{thm:factorization},
because it allows us to reduce such a proof to relating the dual
certificates to the terms in the volume lower bound~\eqref{eq:vol-lb}.

\begin{theorem}\label{thm:dual}
  Let $X$ and $Y$ be two $n$-dimensional normed
  spaces, such that the unit ball of $X$ is $B_X = \mathrm{conv}\{\pm
  v_1, \ldots, \pm v_m\}$. Then $\lambda(I_{X,Y})$ equals
  \begin{align}
    &\sup \tr((RI_{X,Y}(\sum_{i = 1}^m{p_i v_i \otimes  v_i})I_{Y^*,X^*}R^*)^{1/3})^{3/2}\label{eq:dual-obj}\\
    \text{s.t.}\notag\\
    &R: Y \to \ell_2^n, \ell^*(R) \le 1 \label{eq:dual-ellstar}\\
    &\sum_{i = 1}^m{p_i} = 1\label{eq:dual-prob}\\
    &p_1, \ldots, p_m \ge 0. \label{eq:dual-nonneg}
  \end{align}
\end{theorem}
\begin{proof}
  Since $B_X = \mathrm{conv}\{\pm v_1, \ldots, \pm v_m\}$, we can can
  rewrite \eqref{eq:fact-obj}--\eqref{eq:fact-psd} as
  \begin{align}
    &\inf f(M)\ \ \ 
    \text{s.t.}\label{eq:poly-obj}\\
    &\langle v_i, Mv_i\rangle \le 1 \ \ \forall i \in [m]\\
    &M \succ 0\label{eq:poly-psd},
  \end{align}
  By Theorem~\ref{thm:fact-convex} this is a convex minimization problem
  and its value equals $\lambda(I_{X,Y})$.

  Recall that the conjugate function $f^*$ is defined on self-adjoint
  linear operators $Q:\R^n \to \R^n$ by:
  \[
  f^*(Q) \eqdef \sup\{\tr(QM) - f(M): M \succ 0\}.
  \]
  Since $f^*(Q)$ is the supremum of affine functions, it is convex and
  lower semicontinuous. The set on which $f^*$ takes a finite value is
  called its domain. The significance of $f^*$ is the fact
  \begin{equation}\label{eq:lagrange}
  \lambda(I_{X,Y}) = 
  \sup\left\{-\sum_{i = 1}^m{q_i} - 
    f^*\left(-\sum_{i = 1}^m{q_i v_i\otimes v_i}\right):
      q_1, \ldots, q_m \ge 0\right\}.
  \end{equation}
  This follows from the duality theory of convex optimization, since
  \eqref{eq:poly-obj}--\eqref{eq:poly-psd} satisfies Slater's
  condition, which in this case reduces to just checking the existence
  of a feasible $M$ (see~\cite[Chapter 5]{BoydV04}). We proceed to
  compute $f^*(Q)$.

  It is easy to see that unless $-Q \succeq 0$, $f^*(Q) = \infty$,
  and, conversely, if $-Q \succeq 0$, $f^*(Q) \le 0 <
  \infty$. Therefore, the domain of $f^*$ is $\{Q: -Q \succeq 0\}$. We
  will first handle the case $-Q \succ 0$, and then we will extend our
  formula for $f^*(Q)$ to $-Q \succeq 0$ by continuity. Assume then
  that $-Q \succ 0$. As $f$ is a differentiable convex function, the
  range of the gradient mapping $\nabla f$ includes the relative
  interior of the domain of $f^*$, i.e.~the set of linear operators
  $\{X: -X \succ 0\}$ (see Corollary~26.4.1.~in~\cite{Rockafellar});
  from \eqref{eq:f-derivative} it is also apparent that $\nabla f(M)$
  is negative definite for any positive definite $M$, so the range of
  the gradient mapping is exactly $\{X: -X \succ 0\}$. This means that
  the equation $\nabla f(M) = Q$ has a solution over $M \succ 0$, and
  $f^*(Q)$ is achieved at this solution. Let $M$ be the solution, and
  let $R:Y \to \ell_2^n$ be the invertible map such that $\ell^*(R)
  \le 1$ and $f(M) = \tr((RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2})$. Let
  $A = RI_{X,Y}$; $\nabla f(M) = Q$ implies
  \[
  (A^{-*} M A^{-1})^{-3/2} = -2 AQA^*,
  \]
  and, therefore,
  \begin{align*}
    f^*(Q) &= \tr(QM) - \tr((AM^{-1}A^*)^{1/2})\\
    &= \tr((AQA^*)(A^{-*}MA^{-1})) -   \tr((A^{-*}MA^{-1})^{-1/2})\\
    &= -\frac{3}{2^{2/3}} \tr((-AQA^*)^{1/3}).
  \end{align*}
  We have proved that
  \begin{equation}\label{eq:conjugate-lb}
  f^*(Q) \ge 
  \inf\left\{ -\frac{3}{2^{2/3}} \tr((-RI_{X,Y}QI_{Y^*,X^*}R^*)^{1/3}):
  R:Y \to \ell_2^n,\ell^*(R) \le 1\right\},
  \end{equation}
  for any $Q:X^*\to X$ such that $-Q \succ 0$.  Let $\mathcal{D}$ be
  the set of invertible maps $R:Y \to \ell_2^n$ such that $\ell^*(R)
  \le 1\}$. By \eqref{eq:f-formula} and the definition of the
  conjugate function $f^*$ we also have
  \begin{align*}
  f^*(Q) &= 
  \sup_{M: M \succ 0} 
  \inf_{R \in \mathcal{D}}
  \tr(QM) - \tr((RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2})\\
  &\le
  \inf_{R \in \mathcal{D}}
  \sup_{M: M \succ 0} 
  \tr(QM) - \tr((RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2}).
  \end{align*}
  For any $R \in \mathcal{D}$, the supremum on the right hand side
  equals the value at $Q$ of the conjugate $h^*_R$ of the
  function $h_R(M) = \tr((RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2})$. A
  calculation analogous to the one above shows that $h_R^*(Q) =
  -\frac{3}{2^{2/3}}
  \tr((-RI_{X,Y}QI_{Y^*,X^*}R^*)^{1/3})$. Therefore,
  \[
    f^*(Q) \le
    \inf\left\{ -\frac{3}{2^{2/3}} \tr((-(RI_{X,Y}QI_{Y^*,X^*}R^*)^{1/3}):
    R:Y \to \ell_2^n,\ell^*(R) \le 1\right\},
  \]
  and, together with \eqref{eq:conjugate-lb}, we have established 
  \begin{equation}
    \label{eq:conjugate}
    f^*(Q) =
    \inf\left\{ -\frac{3}{2^{2/3}} \tr((-(RI_{X,Y}QI_{Y^*,X^*}R^*)^{1/3}):
    R:Y \to \ell_2^n,\ell^*(R) \le 1\right\},
  \end{equation}
  for any $Q:X^*\to X$ such that $-Q \succ 0$. Since $f^*$ is a
  a proper lower-semicontinuous function, it is continuous on any
  line segment contained in its domain by Corollary
  7.5.1.~in~\cite{Rockafellar}. Therefore, \eqref{eq:conjugate}
  holds for any $Q \succeq 0$ as well. 
  
  By \eqref{eq:lagrange} and \eqref{eq:conjugate}, 
  \[
  \lambda(I_{X,Y}) = 
  \sup\left\{-\sum_{i = 1}^m{q_i} 
    + \frac{3}{2^{2/3}} \tr((-(RI_{X,Y}(\sum_{i = 1}^m{q_i  v_i\otimes v_i})I_{Y^*,X^*}R^*)^{1/3}):
    R:Y \to \ell_2^n,\ell^*(R) \le 1,
    q_1, \ldots, q_m \ge 0\right\}.
  \]
  Let us write $q = tp$ where $t \ge 0$ is a real number, $p_1,
  \ldots, p_m \ge 0$, and $\sum_i p_i = 1$. Then we can rewrite the
  equation above as
  \[
  \lambda(I_{X,Y}) = 
  \sup\left\{-t + 
    \frac{3t^{1/3}}{2^{2/3}} \tr((-(RI_{X,Y}(\sum_{i = 1}^m{p_i  v_i\otimes v_i})I_{Y^*,X^*}R^*)^{1/3}):
    t, p_1, \ldots, p_m \ge 0, \sum_{i=1}^m p_i = 1\right\}.
  \]
  Maximizing over $t$ finishes the proof. 
\end{proof}

\cut{
Let, as before, $X$ be an $n$-dimensional normed spaces, and
let $U:X\to \ell_2^n$ be an invertible linear tranformation. Consider the
function
\begin{align}
  f(X) \eqdef &\inf \tr((UM^{-1}U^*)^{1/2}) \label{eq:single-obj}\\
  &\text{s.t.}\notag\\
  &M: X \to X^*,\|M\| \le 1\label{eq:single-constr}\\
  &M \succ 0.\label{eq:single-psd}.
\end{align}
In the proof of Theorem~\ref{thm:fact-convex} we already established
that this is a convex optimization problem, i.e.~that the objective
function $\tr((UM^{-1}U^*)^{1/2})$ and the constraints
\eqref{eq:single-constr}--\eqref{eq:single-psd} are convex in $M$. We
can formulate $f(X)$ dually as a supremum using standard
techniques. 

\begin{lemma}\label{lm:duality-single}
  Let $X$ be an $n$-dimensional normed spaces, whose unit ball $B_X$
  is the convex hull of $\pm v_1, \ldots, \pm v_m$. Then,
  \[
  f(X) = \sup\left\{\tr((U(\sum_{i = 1}^m{p_i v_i \otimes  v_i})U^*)^{1/3})^{3/2}:
  p_1, \ldots, p_m \ge 0, \sum_{i = 1}^m{p_i} = 1\right\}. 
  \]
\end{lemma}
\begin{proof}
  Since $B_X = \mathrm{conv}\{\pm v_1, \ldots, \pm v_m\}$, we can
  rewrite \eqref{eq:single-constr}--\eqref{eq:single-psd} as 
  \begin{align}
    f(X) \eqdef &\inf \tr((UM^{-1}U^*)^{1/2}) \label{eq:sing2-obj}\\
    &\text{s.t.}\notag\\
    &\langle v_i, Mv_i\rangle \le 1 \ \ \forall i \in [m]\label{eq:sing2-constr}\\
    &M \succ 0\label{eq:sing2-psd}.
  \end{align}
  
  Define the function $g(M) = \tr((UM^{-1}U^*)^{1/2})$ on positive
  definite $M$. Recall that the conjugate function $g^*$ is defined
  by:
  \[
  g^*(Q) \eqdef \sup\{\tr(QM) - g(M): M \succ 0\}.
  \]
  Since $g^*(Q)$ is the supremum of affine functions, it is
  convex. The set on which $g^*$ takes a finite value is called its
  domain. The significance of $g^*$ is the fact
  \[
  f(X) = \sup\left\{-\sum_{i = 1}^m{q_i} - g^*\left(-\sum_{i = 1}^m{q_i
      v_i\otimes v_i}\right):
      q_1, \ldots, q_m \ge 0\right\}.
  \]
  This follows from the duality theory of convex optimization, and the fact
  that \eqref{eq:sing2-obj}--\eqref{eq:sing2-psd} satisfy Slater's
  condition (see~\cite[Chapter 5]{BoydV04}). We proceed to compute
  $g^*(Q)$.

  Let us consider again the function $h(x) = \sum_{i =
    1}^m{x^{-1/2}}$, defined for vectos $x \in \R^n$ with positive
  coordinates (denoted $x > 0$ below). By an elementary calculation,
  the conjugate function
  \[
  h^*(y) = \sup\{\langle x, y \rangle - h(x): x >0\}
  \]
  equals $h^*(y) = -\frac{3}{2^{2/3}} \sum_{i = 1}^m{(-y_i)^{1/3}}$ for
  vectors $y$ with non-positive coordinates, and $\infty$ for all other
  $y$. Let, as before, the function $\mu$ map a self-adjoint operator
  to its vector of eigenvalues. Then, by an observation of Lewis,
  generalizing a theorem of von Neumann, $(h\otimes \mu)^* = h^*
  \otimes \mu$, where $h\otimes \mu$ is the composition of $h$ and
  $\mu$, defined over positive definite operators. We have thus
  established that for any positive semidefinite $Q$, 
  \[
  \sup\{-\tr(QM) - tr(M^{-1/2}): M \succ 0\}
  = 
 -\frac{3}{2^{2/3}} \tr((-Q)^{1/3}). 
  \]
  We are now ready to compute $g^*$. Since $U$ is invertible, we can
  write $UM^{-1}U^* = (U^{-*}MU^{-1})^{-1}$; let us denote $V =
  U^{-1}$ from now on. Then, for any positive semidefinite $Q$,
  \begin{align*}
    g^*(-Q) &= \sup\{ -\tr(QM) - g(M): M \succ 0\}\\
    &= \sup\{ -\tr((UQU^*)(V^*MV)) - tr((V^*MV)^{-1/2}): M \succ 0\}\\
    &=  \frac{3}{2^{2/3}} \tr((-UQU^*)^{1/3}). 
  \end{align*}
  Therefore,
  \[
  f(X) = \sup\left\{-\sum_{i = 1}^m{q_i} 
    + \frac{3}{2^{2/3}} \tr((U(\sum_{i = 1}^m{q_i  v_i\otimes v_i})U^*)^{1/3}):
    q_1, \ldots, q_m \ge 0\right\}.
  \]
  Finally, let us write $q = tp$ where $t \ge 0$ is a real number,
  $p_1, \ldots, p_m \ge 0$, and $\sum_i p_i = 1$. Then we can rewrite
  the equation above as
  \[
  f(X) = \sup\left\{-t
    + \frac{3t^{1/3}}{2^{2/3}} \tr((U(\sum_{i = 1}^m{p_i  v_i\otimes v_i})U^*)^{1/3}):
    t, p_1, \ldots, p_m \ge 0, \sum_{i=1}^m p_i = 1\right\}.
  \]
  Optimizing over $t$ finishes the proof. 
\end{proof}

By Theorem~\ref{thm:fact-convex}, 
\[
\lambda(I_{X,Y}) = 
\inf_{M \in \mathcal{D}_X} \sup_{R \in \mathcal{D}_X} 
\tr(RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2}),
\]
where $\mathcal{D}_X \eqdef \{M: X \to X^*, \|M\|\le 1, M \succ 0\}$
and $\mathcal{D}_Y \eqdef \{R: Y \to \ell_2^n, \ell^*(R) \le
1\}$. \emph{Suppose} that the following min-max equality holds
\begin{equation}
  \label{eq:minmax}
  \inf_{M \in \mathcal{D}_X} \sup_{R \in \mathcal{D}_X} 
  \tr(RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2})
  = 
  \sup_{R \in \mathcal{D}_X} \inf_{M \in \mathcal{D}_X} 
  \tr(RI_{X,Y}M^{-1}I_{Y^*,X^*}R^*)^{1/2}).
\end{equation}
(\textbf{I haven't been able to prove this: it does not seem to follow
  from standard minmax theorems like Sion's. Ideas?}) Then, by
Lemma~\ref{lm:duality-single}, the right hand side above equals 
\begin{align}
  &\sup \tr((U(\sum_{i = 1}^m{p_i v_i \otimes  v_i})U^*)^{1/3})^{3/2}\label{eq:dual-obj}\\
  \text{s.t.}\notag\\
  &R: Y \to \ell_2^n, \ell^*(R) \le 1 \label{eq:dual-ellstar}\\
  &\sum_{i = 1}^m{p_i} = 1\label{eq:dual-prob}\\
  &p_1, \ldots, p_m \ge 0. \label{eq:dual-nonneg}
\end{align}
In summary, we have the following lemma.

\begin{lemma}
  Let $X$ and $Y$ be two $n$-dimensional normed
  spaces, such that the unit ball of $X$ is $B_X = \mathrm{conv}\{\pm
  v_1, \ldots, \pm v_m\}$. Then $\lambda(I_{X,Y})$ equals
  the value of \eqref{eq:dual-obj}--\eqref{eq:dual-nonneg} if and only
  if equation \eqref{eq:minmax} holds. 
\end{lemma} }%end of cut

\subsection{Characterizing $\beta(X,Y)$ in terms of $\lambda(I_{X,Y})$}

Here we use Theorem~\ref{thm:dual} to prove the reverse of the
inequality in Theorem~\ref{thm:factorization}. 

\begin{theorem}\label{thm:fact-vollb}
  There exists a constant $C$ such that the following holds. Let $X$
  and $Y$ be two $n$-dimensional normed spaces, such that the unit
  ball of $X$ is $B_X = \mathrm{conv}\{\pm v_1, \ldots, \pm v_m\}$. Then
  \[
  \lambda(I_{X,Y}) \le C K(Y) (1 + \log n)^{3/2} \vollb((v_i)_{i =
    1}^m, Y),
  \]
  where $I_{X,Y}:X \to Y$ is the formal identity operator, and $K(Y)$
  is the $K$-convexity constant of $Y$. 
\end{theorem}

In the proof of Theorem~\ref{thm:fact-vollb} we use covering/packing
numbers and Sudakov's inequality. For two compact sets $K$ and $L$, we
use $N(K,L)$ for the covering number, i.e.~the minimum number of translates of
$L$ needed to cover $K$. We also recall the definition of the entropy
number $e_k(u)$ of an operator $u:X \to Y$:
\[
e_k(u) = \inf\{\varepsilon: N(u(B_X), \varepsilon B_Y) \le 2^{k-1}\}. 
\]
In this notation, the dual Sudakov inequality says that there exists a
constant $C$ such that for any $u:\ell_2^n \to X$
\begin{equation}
  \label{eq:sudakov}
  \max_{k = 1}^n \sqrt{k}e_k(u) \le C\ell(u). 
\end{equation}

We also use the basic volumetric lower estimate on entropy numbers
\begin{equation}
  \label{eq:entropy-vol}
  e_k(u) \ge \frac{\vol_k(Pu(B_X))^{1/k}}{2\vol_k(P(B_Y))^{1/k}},
\end{equation}
valid for any $u:X \to Y$ and any rank $k$ orthogonal projection $P$.

We also make use of a weighted version of
Lemma~\ref{lm:rip-det}.  

\begin{lemma}\label{lm:rip-det-weighted}
  Let $u_1, \ldots, u_m \in \R^n$, and let $c_1, \ldots, c_m \ge 0$,
  $\sum_{i = 1}^mc_i = 1$. Let $\lambda_1 \ge \ldots \ge \lambda_n$ be
  the eigenvalues of $\sum_{i=1}^m{c_i u_i \otimes u_i}$, and let $G$
  be the Gram matrix of $u_1, \ldots, u_m$. For any
  integer $k$ such that $1 \le k \le n$, there exists a set $S
  \subseteq [m]$ of size $k$ such that 
  \[
  \frac{\det(G_{S,S})}{k!} \ge\lambda_1 \ldots \lambda_k. 
  \]
\end{lemma}
\begin{proof}
  Consider the matrix $H = (\sqrt{c_ic_j}\langle u_i, u_j
  \rangle)_{i,j = 1}^m$. This matrix has the same nonzero eigenvalues as
  $\sum_{i=1}^m{c_i u_i \otimes u_i}$, and, therefore,
  \[
  \sum_{S \subseteq [m]: |S| = k}{\left(\prod_{i \in S}{c_i}\right)\det(G_{S,S})}
  = \sum_{S \subseteq [m]: |S| = k}\det(H_{S,S}) = p_{k,n}(\lambda),
  \]
  where $p_{k,n}$ is the degree $k$ elementary symmetric polynomial in $n$
  variables. (See the proof of Lemma~\ref{lm:rip-det} for a
  justification of the final equality.) Therefore, 
  \[
  \max_{S \subseteq [m]: |S| = k}\det(G_{S,S}) 
  \ge \frac{p_{k,n}(\lambda)}{p_{k,m}(c)}. 
  \]
  We can show that $p_{k,n}(\lambda) \ge \lambda_1 \ldots \lambda_k$
  as in the proof of Lemma~\ref{lm:rip-det}. I.e.~we define a
  vector $\mu \in \R^n_{\ge 0}$ that majorizes $\lambda$ by $\mu_i
  \eqdef \lambda_i + \nu$ for $i \in [k]$ and $\mu_i = 0$ for $i > k$,
  where $\nu \eqdef \frac{1}{k}\sum_{i = k+1}^n{\lambda_i}$. Then, by
  the Schur concavity of $p_{k,n}$,
  \[
  p_{k,n}(\lambda) \ge p_{k,n}(\mu) \ge \lambda_1 \ldots \lambda_k.
  \]
  Similarly, observe that the vector $d \in \R^m_{\ge 0}$ with coordinates $d_i =
  \frac{1}{m}$ is majorized by $c$, and, therefore, 
  \[
  p_{k,m}(c) \le p_{k,m}(d) = \frac{1}{m^k} {m \choose k} \le \frac{1}{k!}.
  \]
  Combining the inequalities finishes the proof. 
\end{proof}

Finally, we use the following elementary inequality, valid for an
absolute constant $C$, and any sequence $x_1 \ge x_2 \ge \ldots \ge
x_n \ge 0$ of non-negative reals
\begin{equation}
  \label{eq:DFE} %Daniel's Favorite Inequality
  \sum_{i = 1}^n{x_i} \le C(1+\log n) 
  \max_{k = 1}^n k \left(\prod_{i = 1}^k{x_i}\right)^{1/k}.
\end{equation}

\begin{proof}[Proof of Theorem~\ref{thm:fact-vollb}]
  By Theorem~\ref{thm:dual}, there exists an operator $R: \ell_2^n \to
  Y$, $\ell^*(R) \le 1$ and non-negative reals $p_1, \ldots, p_m \ge
  0$, $\sum_{i = 1}^m{p_i} = 1$, such that 
  \[
  \lambda(I_{X,Y})^{2/3} 
  = \tr((RI_{X,Y}(\sum_{i = 1}^m{p_i v_i  \otimes  v_i})I_{Y^*,X^*}R^*)^{1/3})
  = \tr((\sum_{i = 1}^m{p_i RI_{X,Y}(v_i)  \otimes  RI_{X,Y}(v_i)})^{1/3}).
  \]
  Let $u_i \eqdef RI_{X,Y}(v_i)$, and let $\lambda_1 \ge \ldots \ge
  \lambda_n \ge 0$ be the eigenvalues of $\sum_{i = 1}^m{p_i u_i
    \otimes u_i}$, so that $\lambda(I_{X,Y})^{2/3} = \sum_{i =
    1}^n{\lambda_i^{1/3}}$. Applying \eqref{eq:DFE} to $x_i \eqdef
  \lambda_i^{1/3}$, we have that there exists an integer $k$, $1 \le k
  \le n$, such that
  \[
  \lambda(I_{X,Y})^{2/3} \le C_0 (1+\log n) k (\lambda_1 \ldots \lambda_k)^{1/(3k)},
  \]
  for an absolute constant $C_0$. Now, by
  Lemma~\ref{lm:rip-det-weighted}, we have that there exists a set $S
  \subseteq [m]$ of size $k$ such that
  \[
  (\lambda_1 \ldots \lambda_k)^{1/k} \le
  \frac{\det(H_{S,S})^{1/k}}{(k!)^{1/k}},
  \]
  where $H$ is the Gram matrix of $u_1, \ldots, u_m$. By Stirling's
  estimate, this implies that 
  \begin{equation}\label{eq:fact-det}
  \lambda(I_{X,Y})^{2/3} \le C_1 (1+\log n) k^{2/3} \det(H_{S,S})^{1/(3k)},
  \end{equation}
  for an absolute constant $C_1$. 

  To finish the proof, we relate the right hand side of
  \eqref{eq:fact-det} to the volume lower bound. Let $V:\R^S \to W$ be
  the operator defined by $V(x) \eqdef \sum_{i \in S}{x_i v_i}$ and let $T$
  be the restriction of $RI_{X,Y}$ to $W \eqdef \lspan\{v_i: i \in
  S\}$. Then, 
  \begin{equation}\label{eq:det-product}
  \det(H_{S,S}) = \det(TVV^*T^*) = \det(V)^2\det(T^*)^2 =
  \det(G_{S,S}) \det(T^*)^2,
  \end{equation}
  where $G$ is the Gram matrix of $v_1, \ldots, v_m$. Furthermore,
  \begin{align*}
  |\det(T^*)|^{1/k} =
  \frac{\vol_k(P_WR^*(B_2^n))^{1/k}}{\vol_k(B_2^k)^{1/k}}
  &= \frac{\vol_k(P_WR^*(B_2^n))^{1/k}}{\vol_k(P_WB_Y^\circ)^{1/k}} \cdot 
  \frac{\vol_k(P_WB_Y^\circ)^{1/k}}{\vol_k(B_2^k)^{1/k}}\\
  &\le 2e_k(R^*) \cdot 
  \frac{\vol_k(P_WB_Y^\circ)^{1/k}}{\vol_k(B_2^k)^{1/k}},
  \end{align*}
  where $P_W:Y^* \to Y^*/W^\perp$ is the orthogonal projection onto
  the subspace $W$, which we identify with $Y^*/W^\perp$ in the natural
  way. The last inequality follows from the volume
  estimate~\eqref{eq:entropy-vol}. Applying \eqref{eq:sudakov} to the
  first term on the right hand side, and Santalo's inequality to the
  second, we have that, for an absolute constants $C_2$,
  \[
  |\det(T^*)|^{1/k} \le C_2 \frac{\ell(R^*)}{\sqrt{k}} \cdot
  \frac{\vol_k(B_2^k)^{1/k}}{\vol_k(B_Y \cap W)^{1/k}}. 
  \]
  Finally, by $K$-convexity $\ell(R^*) \le K(Y) \ell^*(R) \le
  K(Y)$. Together with a basic estimate for $\vol_k(B_2^k)^{1/k}$, we
  have that 
  \[
  |\det(T^*)|^{1/k} \le C_3\ \frac{K(Y)}{k\vol_k(B_Y \cap W)^{1/k}},
  \]
  for to an absolute constant $C_3$. Combining with \eqref{eq:fact-det} and
  \eqref{eq:det-product}, we have, for an absolute constant $C_4$,
  \[
  \lambda(I_{X,Y}) \le C_4K(Y) (1+\log n)^{3/2}  
  \frac{\det(G_{S,S})^{1/(2k)}}{\vol_k(B_Y \cap W)^{1/k}}
  \le C_4 K(Y) (1+\log n)^{3/2}  
  \vollb((v_i)_{i =  1}^m, Y),
  \]
  as required.
\end{proof}

Combining \eqref{eq:vol-lb} and
Theorems~\ref{thm:factorization}~and~\ref{thm:fact-vollb},
we have the following

\begin{corollary}
  Let $Y$ be a normed space defined on $\R^n$, $u_1, \ldots, u_N \in
  \R^n$, and let $X$ be the norm on $\R^n$ with unit ball $B_X
  \eqdef\mathrm{conv}\{\pm u_1, \ldots, \pm u_N\}$. Then, for absolute
  constants $C, C'$,
  \[
  \beta((u_i)_{i = 1}^N, Y) \le \beta(X,Y)
  \le C \lambda(I_{X,Y})
  \le C' K(Y) (1+\log n)^{3/2}  \beta((u_i)_{i = 1}^N, Y).
  \]
\end{corollary}

\bibliographystyle{alpha}
\bibliography{Discrepancy}
\end{document}
