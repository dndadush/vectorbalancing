\documentclass[12pt]{article}

\usepackage{amsfonts,amsmath,amsthm,amssymb}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{authblk}

%\newif\ifbigfont\bigfonttrue
\newif\ifbigfont\bigfontfalse

\ifbigfont

\usepackage[right=.5in,left=.5in,top=.6in,bottom=.6in]{geometry}
\usepackage[17pt]{extsizes}
\usepackage{newpxmath,newpxtext}

\else

\usepackage[margin=1in]{geometry}
%\usepackage{newpxmath,newpxtext}

\fi

\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\heading}[1]{\vspace{1ex}\par\noindent{\bf\boldmath #1}}

\newcommand{\cut}[1]{}
\newcommand{\ceil}[1]{\lceil #1 \rceil}

\newcommand{\R}{{\mathbb{R}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\N}{{\mathbb{N}}}
\renewcommand{\S}{\mathbb{S}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\T}{\mathsf T}

\newcommand\eps{\varepsilon}
\newcommand{\eqdef}{\triangleq}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\newcommand{\set}[1]{\left\{ #1 \right\}}

\DeclareMathOperator{\vollb}{volLB}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\hd}{hd}
\DeclareMathOperator{\vb}{vb}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\lspan}{span}
\DeclareMathOperator{\speclb}{specLB}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\bnds}{bounds}

\DeclarePairedDelimiter\norm{\lVert}{\rVert}

% MARGIN NOTES
\newif\ifnotes\notestrue
%\newif\ifnotes\notesfalse

\ifnotes
\usepackage{color}
%\definecolor{mygrey}{gray}{0.50}

\ifbigfont

\newcommand{\notename}[2]{{\textcolor{red}{{\bf (#1:} {#2}{\bf ) }}}}

\else

\newcommand{\notename}[2]{{\textcolor{red}{\footnotesize{\bf (#1:} {#2}{\bf ) }}}}

\fi

\newcommand{\noteswarning}{{\begin{center} {\Large WARNING: NOTES ON}\end{center}}}
\newcommand{\knote}[1]{{\notename{Kunal}{#1}}}
\newcommand{\nnote}[1]{{\notename{Nicole}{#1}}}
\newcommand{\dnote}[1]{{\notename{Daniel}{#1}}}
\newcommand{\snote}[1]{{\notename{Sasho}{#1}}}


\else

\newcommand{\notename}[2]{{}}
\newcommand{\noteswarning}{{}}
\newcommand{\knote}[1]{}
\newcommand{\nnote}[1]{}
\newcommand{\dnote}[1]{}
\newcommand{\snote}[1]{}

\fi



\begin{document}
\title{Balancing Vectors in Any Norm}
\author[1]{Daniel Dadush
\thanks{Email: \href{mailto:dadush@cwi.nl}{dadush@cwi.nl}.
Supported by NWO Veni grant 639.071.510.}} 
\author[2]{ 
Aleksandar Nikolov
\thanks{Email: \href{mailto:anikolov@cs.toronto.edu}{anikolov@cs.toronto.edu}.}}
\author[3]{
Kunal Talwar
\thanks{Email: \href{mailto:kunal@kunaltalwar.org}{kunal@kunaltalwar.org}.}} 
\author[4]{
Nicole Tomczak-Jaegermann
\thanks{Email: 
\href{mailto: nicole.tomczak@ualberta.ca}{ nicole.tomczak@ualberta.ca}.}} 

\affil[1]{\small Centrum Wiskunde \& Informatica.}
\affil[2]{\small University of Toronto.}
\affil[3]{\small Google Brain.}
\affil[4]{\small University of Alberta.}

\maketitle

\noteswarning

\begin{abstract}
In the vector balancing problem, we are given $N$ vectors $v_1,..., v_N$ in an
$n$-dimensional normed space, and our goal is to assign signs to them, so that
the norm of their signed sum is as small as possible. The balancing constant of
the vectors is the smallest number $\beta$, such that any subset of the vectors
can be balanced so that their signed sum has norm at most $\beta$. The vector
balancing constant generalizes combinatorial discrepancy, and is related to
rounding problems in combinatorial optimization, and to the approximate
Caratheodory theorem. We study the question of efficiently approximating the
vector balancing constant of any set of vectors, with respect to an arbitrary
norm. We show that the vector balancing constant can be approximated in
polynomial time to within factors logarithmic in the dimension, and is
characterized by (an appropriately optimized version of) a known volumetric
lower bound. Our techniques draw on results from geometric functional analysis
and the theory of Gaussian processes.  Our results also imply an improved
approximation algorithm for hereditary discrepancy. 
\end{abstract}

\newpage

\section{Introduction}

In classical combinatorial discrepancy, one studies for various set systems,
over the set of $\pm 1$ colorings of the elements, what is the minimum
worst-case imbalance, known as discrepancy, one guarantee between the number of
$+1$ and $-1$ elements in every set? The tools developed for understanding the
discrepancy of set systems have found many applications in mathematics and
computer science~\cite{Matousek,Chazelle}. 

\paragraph{\bf Vector Balancing.} In many instances, the best known techniques
for finding good bounds in combinatorial discrepancy were derived by working
with more general vector balancing problems, where convex geometric techniques
can be applied. Given symmetric convex bodies $C,K \subseteq \R^n$, the vector
balancing constant of $C$ into $K$ is defined as 
% i.e.~which induce the unit ball of a norm, and the
% goal is to understand the minimum number $M \geq 0$ such that for any sequence
% $(v_i)_{i=1}^N \in C$, there exists signs $(x_i)_{i=1}^N \in \set{-1,1}$ such
% that $\sum_{i=1}^N x_i v_i \in M D$. The minimum $M$ is denoted $\vb(C,D)$,
% the vector balancing constant of $C$ into $D$. 
\[
\vb(C, K) \eqdef \sup\Bigg\{ 
\min_{x_1, \ldots,x_N \in \{-1, 1\}} \Bigl\|\sum_{i = 1}^N x_i
u_i \Bigr\|_{K}: N \in \mathbb{N}, u_1, \ldots, u_N \in C \Bigg\}.
\]

As an example, one may consider Spencer's theorem~\cite{Spencer}, independently
obtained by Gluskin~\cite{gluskin}, which states that every set system on $n$
points and $n$ sets can be colored with discrepancy at most $O(\sqrt{n})$. In
the vector balancing context, the more general statement is that
$\vb(B_\infty^n,B_\infty^n) = O(\sqrt{n})$ (also proved
in~\cite{Spencer,gluskin}), where we use the notation $B_p^n = \set{x \in \R^n:
\norm{x}_p \leq 1}$, $p \in [1,\infty]$, to denote the unit ball of the $\ell_p$
norm. To encode Spencer's theorem, we simply represent the set system using its
incidence matrix $U \in \set{0,1}^{n \times n}$, where $U_{ji} = 1$ if element
$i$ is in set $j$ and $0$ otherwise. Here the columns of $U$ have $\ell_\infty$
norm $1$, and thus the sign vector $x \in \set{-1,1}^n$ satisfying $\norm{U
x}_\infty = O(\sqrt{n})$ indeed yields the desired coloring. 

Many fundamental discrepancy bounds, as well as conjectured bounds, can be
stated in terms of vector balancing constants. The Beck-Fiala theorem, which
bounds the discrepancy of any $t$-sparse set system by $2t-1$, i.e.~where each
element appears in at most $t$-sets, can be recovered from the bound
$\vb(B_1^n,B_\infty^n) < 2$~\cite{beckfiala}. The infamous Beck-Fiala conjecture,
which asks whether the bound for $t$-sparse set systems can be improved to
$O(\sqrt{t})$, is generalized by the K{\'o}mlos conjecture~\cite{spencer-lectures},
which asks whether $\vb(B_2^n,B_\infty^n) = O(1)$. One of the most important
vector balancing bounds is due to Banaszczyk~\cite{bana}, who proved that for
any convex body $K \subseteq \R^n$ of Gaussian measure $1/2$, one has the bound
$\vb(B_2^n,K) \leq 5$. In particular, this implies the bound of
$\vb(B_2^n,B_\infty^n) = O(\sqrt{\log n})$ for the K{\'o}mlos conjecture. 

\paragraph{\bf Discrepancy Minimization.} The original proofs of many of the
aforementioned discrepancy upper bounds were existential, and did not come with
efficient algorithms capable of constructing the requisite low discrepancy
colorings. Over the last eight years, starting with the breakthrough work of
Bansal~\cite{Bansal10}, who gave a constructive version of Spencer's theorem
using random walk and semidefinite programming techniques, almost all known
vector balancing bounds have been made algorithmic. 

One of the most important discrepancy minimization techniques is known as the
partial coloring method, which covers most of the above discrepancy results
apart from Banaszczyk's vector balancing theorem. Given a discrepancy
minimization problem $\min_{x \in \set{-1,1}^n} \norm{\sum_{i=1}^n x_i v_i}_\infty$,
this method ``colors'' (i.e.~sets to $\pm 1$) at least a constant fraction of
the remaining variables at every stage. This yields $O(\log n)$ partial coloring
phases, where the discrepancy of the full coloring is generally upper bounded by
the sum of discrepancies incurred in each phase. The existence of low
discrepancy partial colorings was initially established via the pigeon hole
principle and certain entropy bounds for random $\pm 1$ combinations of the
vectors, known as Beck's entropy method. This method was both generalized and
made constructive by Lovett and Meka~\cite{lovettmeka} using random walk
techniques. The entropy method was further generalized by
Giannopoulos~\cite{giann} to the vector balancing setting using Gaussian
measure. Precisely, he showed that if a symmetric convex body $K \subseteq \R^n$
has Gaussian measure at least $e^{-\delta n}$, for $\delta$ small enough, then
for any sequence vector $v_1,\dots,v_n \in B_2^n$, there exists a partial
coloring $x \in \set{-1,0,1}^n$, having support at least $n/2$, such that
$\sum_{i=1}^n x_i v_i \in O(1) K$. This method was both extended and made
constructive by Rothvoss~\cite{rothvoss-giann}, using a novel random projection
algorithm, and a simplified algorithm was given by Eldan and Singh~\cite{ES14}.
An important difference between the constructive and existential partial
coloring methods, is that the constructive methods only guarantee that the
``uncolored'' coordinates of partial coloring $x$ are in $(-1,1)$ instead of
equal to $0$. This relaxation seems to make the constructive more versatile,
i.e.~the conditions under which such ``fractional'' partial colorings are
somewhat milder, without having any noticeable drawbacks in the applications. 

The main alternative to the partial coloring method comes from Banaszczyk's
vector balancing theorem~\cite{bana}. Comparing to Giannopoulos, Banaszczyk can
find a full coloring when $K$ has gaussian measure $1/2$ instead of a partial
coloring when the measure is $2^{-O(n)}$. Banaszczyk's method was only very
recently made constructive in the sequence of works~\cite{BDG16,DGLN16,BDGL18}.
In particular,~\cite{DGLN16} showed an equivalence of Banaszcyzk's theorem to
the existence of certain subgaussian signing distributions, and~\cite{BDGL18}
gave a random walk algorithm to build such distributions.

\subsection{Nearly Optimal Vector Balancing and Hereditary Discrepancy}

Given the powerful tools that have been developed for discrepancy minimization,
a natural question is whether they can be used to get optimal or nearly optimal
bounds for any vector balancing problem. That is, can we efficiently compute
colorings achieving discrepancy $\approx \vb(C,K)$, for any two symmetric convex
bodies $C,K$? Furthermore, can we approximate the value $\vb(C,K)$ directly?

These questions have indeed been studied, however most directly in the context
of hereditary discrepancy, which we presently define. Given a sequence $(u_1,
\ldots, u_N)$ of vectors in $\R^n$, we define the discrepancy and hereditary
discrepancy with respect to $K$ as follows:
\begin{align*}
\disc((u_i)_{i = 1}^N,K) &\eqdef \min_{\eps_1, \ldots,\eps_N \in
  \{-1, 1\}}
\Bigl\|\sum_{i = 1}^N{\eps_i u_i}\Bigr\|_{K};\\
\hd((u_i)_{i = 1}^N, K) &\eqdef \max_{S \subseteq  [N]}{\disc((u_i)_S, K)}.
\end{align*}
It follows trivially from the definitions that 
\[
\vb(C, K) = \sup\{\hd((u_i)_{i = 1}^N, K)\},
\]
where the supremum is taken over all finite sequence of vectors in $C$.

\subsection{Approximating Hereditary Discrepancy}

\subsection{Application to Approximate Caratheodory} 

\section{Definitions and Statement of the Volume Lower Bound}

In what follows, we use we will use $\inner{\cdot}{\cdot}$ for the standard
inner product on $\R^n$. Duality and traces are defined with respect to this
inner product. $K^\circ$ denotes the polar of a set $K \subseteq \R^n$. We use
$\vol_n(K)$ for the $n$-dimensional volume (standard Lebesgue measure in $\R^n$)
of $K$ and $\kappa_n$ for the volume $B_2^n$, the unit Euclidean ball. For a
symmetric convex body $K$, we define $\norm{x}_K = \min \set{s \geq 0: x \in sK}$
to be the norm induced by $K$.

Let $C$ and $K$ be two symmetric convex bodies in $\R^n$. We define the vector
balancing constant of $C$ and $K$ by 
\[
\vb(C, K) \eqdef \sup\Bigg\{ 
\min_{\eps_1, \ldots,\eps_N \in \{-1, 1\}} \Bigl\|\sum_{i = 1}^N \eps_i
u_i \Bigr\|_{K}: N \in \mathbb{N}, u_1, \ldots, u_N \in C \Bigg\}.
\]
Given a sequence $(u_1, \ldots, u_N)$ of vectors in $\R^n$, we define the
discrepancy and hereditary discrepancy with respect to $K$ as follows:
\begin{align*}
\disc((u_i)_{i = 1}^N,K) &\eqdef \min_{\eps_1, \ldots,\eps_N \in
  \{-1, 1\}}
\Bigl\|\sum_{i = 1}^N{\eps_i u_i}\Bigr\|_{K};\\
\hd((u_i)_{i = 1}^N, K) &\eqdef \max_{S \subseteq  [N]}{\disc((u_i)_S, K)}.
\end{align*}
It follows trivially from the definitions that 
\[
\vb(C, K) = \sup\{\hd((u_i)_{i = 1}^N, K)\},
\]
where the supremum is taken over all finite sequence of vectors in $C$.

% For a non-singular matrix $A \in \R^{n \times k}$, we define $\detb(A) :=
% \det(A^\T A)^{1/2}$.  
% Let $G$ be the Gram matrix of $u_1, \ldots, u_N$, i.e. $g_{ij} =
% \langle u_i, u_j \rangle$, where $\langle \cdot, \cdot \rangle$ is
% the standard inner product on $\R^n$.  Banaszczyk~\cite{Bana93}
% established the following lower bound on $\beta((u_i)_{i = 1}^N, Y)$:
% \[
% \beta((u_i)_{i = 1}^N, Y) \ge \frac{\det(G)^{1/(2n)}}{\vol_n(B_Y)^{1/n}}.
% \]
% It is easy to see that this inequality is not always tight: for
% example, it is possible that $\beta((u_i)_{i = 1}^N, Y) > 0$, but the
% vectors $u_1, \ldots, u_N$ are not linearly independent, so $\det(G) =
% 0$. However, the inequality can be  strengthened by
% maximizing over subsequences of $(u_i)_{i = 1}^N$:
% \begin{equation}
%   \label{eq:vol-lb}
%   \beta((u_i)_{i = 1}^N, Y) \ge \vollb((u_i)_{i = 1}^N, Y) \eqdef
%   \max_{k = 1}^n \max_{S \subseteq [N]:|S| = k} \frac{\det(G_{S,S})^{1/(2k)}}{\vol_k(B_Y \cap \lspan\{u_i: i \in S\})^{1/k}}.
% \end{equation}
% Here $G_{S,S}$ is the principle submatrix of $G$ whose rows and
% columns are indexed by the set $S$.

For vectors $x,y \in \R^n$, we define $\inner{x}{y} = x^\T y$ to be the standard
inner product in $\R^n$. For a matrix $A \in \R^{n \times k}$, we will be
interested in the volume of the parallelepiped generated by the columns of $A$,
which can be computed by $\vol_k(A[0,1)^k) = \det(A^\T A)^{1/2}$. Banaszczyk
established the following volumetric lower bound on hereditary discrepancy:

\begin{lemma}[\cite{Bana93}]
\label{lem:vol-lb}
Let $U = (u_1,\dots,u_N) \in \R^{n \times N}$ and $K \subseteq \R^n$ be a
symmetric convex body. For $S \subseteq [N]$, $|S| = k \in [n]$, where $(u_i)_{i
\in S}$ are linearly independent, define \dnote{realized that we can simplify
the formula.}  
\begin{equation}
\label{eq:vol-lb}
\vollb((u_i)_{i \in S}, K) 
                         \eqdef \vol_k(\set{x \in \R^k: U_S x \in K})^{-1/k}.
\end{equation}
where $U_S$ denotes the columns of $U$ indexed by $S$. Then, letting
\begin{align*}
\vollb^{\rm h}_k((u_i)_{i=1}^N, K) &\eqdef \max_{\substack{S \subseteq [N] \\ {\rm
rank}(U_S) = k}} \vollb((u_i)_{i \in S},K),~\forall k \in [n], \\
\vollb^{\rm h}((u_i)_{i=1}^N, K) &\eqdef \max_{k \in [n]}
\vollb^{\rm h}_k((u_i)_{i=1}^N,K),
\end{align*}
we have that
\[
\hd((u_i)_{i=1}^N,K) \geq \vollb^{\rm h}((u_i)_{i=1}^N,K) .
\]
\end{lemma}

\section{Preliminaries}

We define $\gamma_n$ to be the standard Gaussian measure on $\R^n$, that is
$\gamma_n(A) = \frac{1}{\sqrt{2\pi}^n} \int_A e^{-\|x\|^2/2}$. We will often use
the $k$-dimensional Gaussian measure restricted to $k$-dimensional linear
subspace $H$ of $\R^n$, for which we use the notation $\gamma_H$.

For an $n \times n$ positive definite matrix $A$, we define the ellipsoid $E(A)
= \set{x \in \R^n: x^\T A x \leq 1}$. We recall that the polar ellipsoid
$E(A)^\circ = E(A^{-1})$, and that $\vol_n(E(A)) = \kappa_n \det(A)^{-1/2}$.

For a linear subspace $W \subseteq \R^n$, we denote the orthogonal projection
onto $W$ by $\pi_W$. For $S \subseteq [n]$, we write $\pi_S$ to denote the
projection onto the coordinate subspace $\lspan\{e_i: i \in S)\}$.

For a symmetric convex body $K \subseteq \R^n$ and subset $S \subseteq [n]$, we
denote the coordinate section of $K$ on $S$ by $K_S \eqdef \set{x \in K: x_i =
0, ~\forall i \notin S}$. We use $B_p^n$, $p \in [1,\infty]$, to denote the unit
$\ell_p$ ball in dimension $n$, $B_p^S := (B_p^n)_S$ and $B_p^W := (B_p^n) \cap W$
for corresponding coordinate and general sections. For a vector $x \in
[-1,1]^n$, we write $\bnds(x) = \set{i \in [n]: x_i \in \set{-1,1}}$.

\section{Tightness of the Volume Lower Bound}

In this section, we will show that the volume lower bound \eqref{eq:vol-lb} is
tight within a logarithmic factor. 

\begin{theorem}\label{thm:tightness}
There exists a universal constant $C \geq 1$ such that the following holds. Let
$K \subseteq \R^n$ be a convex body $\R^n$, and let $u_1, \ldots, u_N \in \R^n$.
Then, the following holds:
\begin{enumerate}
\item $1 \le \frac{\hd((u_i)_{i = 1}^N, K)}{\vollb^{\rm h}((u_i)_{i = 1}^N, K)} 
         \le C(1+\log n)$.\\
\item $\hd((u_i)_{i = 1}^N, K) \leq C \sum_{k=1}^n \frac{1}{k} \vollb^{\rm
h}_k((u_i)_{i=1}^N,K)$.
\end{enumerate}
Furthermore, there exists a polynomial time algorithm which can compute a
coloring of $(u_i)_{i=1}^N$ of $K$-discrepancy at most $C(\log n+1)\vollb^{\rm
h}((u_i)_{i=1}^N,K)$. \dnote{Fix the proof to show the above.} 
\end{theorem}

The main technical result of this section is that the volume lower bound,
restricted to subsets of size at least $\Omega(n)$, is in fact an upper bound on
the discrepancy of so-called partial colorings. This allows us to easily recover
Theorem~\ref{thm:tightness} using $O(\log n)$ partial coloring phases in the
standard way. We state our technical result below, restricted to the case where
the vectors are aligned with the standard basis. We note that since the output
norm is general, this is essentially without loss of generality. 

\begin{lemma}[Partial Colorings via Volume] \label{lem:partial-via-volume}
There exists a universal constants $C \geq 1, \eps_0 \in (0,1), \delta \in (0,1)$, such that
for any $y \in (-1,1)^n$ and symmetric convex body $K \subseteq \R^n$ satisfying
$\forall S \subseteq [n]$, $|S| = \ceil{\delta n}$, $\vol_{|S|}(K_S) \geq 1$, there
exists a polynomial time algorithm which with high probability finds $x \in
[-1,1]^n$ with $|\bnds(x)| \geq \ceil{\eps_0 n}$ and
$x-y \in C K$.  
\end{lemma}

We now give the straightforward reduction from Theorem~\ref{thm:tightness} to
Lemma~\ref{lem:partial-via-volume}.

\begin{proof}[Proof of Theorem~\ref{thm:tightness}]
By Lemma~\ref{lem:vol-lb}, we may restrict attention to the upper bound. In
particular, given $u_1,\dots,u_N \in \R^n$ and a convex body $K$ in $\R^n$, it
suffices to show that $\disc((u_i)_{i=1}^N) \leq C(1+\log n)\vollb^{\rm
h}((u_i)_{i=1}^N,K)$.  

To begin, we compute a basic solution to the linear program $\sum_{i=1}^N x_i
u_i = 0$, $x \in [-1,1]^N$. After relabeling, we may assume the variables not
hitting their $\set{-1,1}$ bounds are $x_1,\dots,x_l$, noting that if there are
no such variables we already have a $0$ discrepancy coloring. Since
$x$ is basic, we know that the vectors $u_1,\dots,u_l$ must be linearly
independent. Therefore, we may apply a invertible linear
transformation $T:\R^n \rightarrow \R^n$ sending $u_1,\dots,u_l$ to
$e_1,\dots,e_l$. In particular, letting $K' := TK$, we have that 
\begin{align*}
\disc((u_i)_{i=1}^N,K) &\leq \min_{z \in \set{-1,1}^l} \|\sum_{i=1}^l z_i e_i +
\sum_{i=l+1}^N x_i T u_i\|_{K'} \\
                &= \min_{z \in \set{-1,1}^l} \|\sum_{i=1}^l (z_i-x_i)e_i\|_{K'}, 
\end{align*}
Furthermore, a direct computation shows that
\[
\vollb^{\rm h}((u_i)_{i=1}^l,Y) = \vollb^{\rm h}((e_i)_{i=1}^l,K') = \max_{S \subseteq [l]}
\vol_{|S|}(K'_S)^{-1/|S|} \text{ .}
\] 
Let us assume that we have computed $M > 0$ satisfying 
\[
M/2 \leq \vollb^{\rm h}((e_i)_{i=1}^l,K') \leq M. 
\]
From here, it suffices to compute $z \in
\set{-1,1}^l$ such that $\sum_{i=1}^l (z_i-x_i) e_i \in O(\log n M) K'$. Note
that by assumption on $M$, $\vol(MK'_S) \geq 1, \forall S \subseteq [l]$.
Therefore, repeatedly applying Lemma~\ref{lem:partial-via-volume} on $MK'$, letting $x^0 := x$ we compute a sequence
$x^1,\dots,x^T \in [-1,1]^l$, $T = \lceil \log l/\eps_0
\rceil = O(\log n)$, such that $\forall t \in [T]$, we have that

\begin{enumerate}
\item $\sum_{i=1}^l (x^t_i-x^{t-1}_i)e_i \in O(M) K'$.
\item $|\set{i \in [l]: x^t_i \in (-1,1)}| \leq (1-\eps_0)|\set{i \in [l]:
x^{t-1}_i \in (-1,1)}|$.
\end{enumerate}

By our choice of $T$, it is direct to check that $x^T \in \set{-1,1}^l$ and by
the triangle inequality that $\sum_{i=1}^l (x^T_i-x^0_i)e_i \in T M K' = O(\log n
M) K'$. Thus, setting $z = x^T$ satisfies the requirements.

We now discuss the computation of $M$. We first note that 
\[
M_1 := \max_{i \in [l]} \vol_1(K'_i)^{-1} = \max_{i \in [l]} \|e_i\|_{K'} =
\max_{i \in [l]} \|u_i\|_K \text{ ,}
\]
and thus the restricted maximum can be efficiently computed. Note that by
construction ${\rm conv}(\pm e_1,\dots, \pm e_l)/M_1 \subseteq K'_{[l]}$. Thus,
for any $S \subseteq [l]$,$|S|=k$, we see that
\[
\vol_k(l M_1 K'_S) \geq \vol_{k}(l \cdot {\rm conv}(\pm e_i: i \in S)) = \frac{(2l)^k}{k!}
 \geq 1 \text{ .}
\]
In particular, we get that $M_1 \leq \vollb^{\rm h}((e_i)_{i=1}^l,K') \leq l M_1$.
Hence, as input to the stage above we may successively try the values $M_1
2^k$, $k \in \set{0,\dots,\log_2 l}$, stopping the first time we find a valid
coloring. 
\end{proof}

Lemma~\ref{lem:partial-via-volume} should be viewed as a volumetric analogue of
a theorem of Rothvoss~\cite{rothvoss-giann}, who both extended and made
algorithmic vector balancing results of Giannopoulos~\cite{giannop}. We state a
slight variant of~\cite[Lemma 9]{rothvoss-giann} below.

% \dnote{I
% would like to say something about how you probably can't get this type of result
% using the determinant lower bound. In particular, you lose a log there even for
% partial coloring in $\ell_\infty$. I think this is better save for the intro,
% where its easier to compare things.} 

\begin{theorem}[Partial Colorings via Gaussian Measure]\label{thm:roth-giann}
Let $0 < \eps \leq 1/60000$ and $\delta = \frac{3}{2}\eps \log_2
\frac{1}{\eps}$. Let $K \subseteq \R^n$ be a symmetric convex body and assume
that for some subspace $H \subseteq \R^n$ of dimension at least $(1-\delta)n$,
we have that $\gamma_H(K) \geq e^{-\delta n}$. Then for any $y \in (-1,1)^n$,
there exists a polynomial time algorithm which with high probability finds $x
\in [-1,1]^n$ satisfying $|\bnds(y)| \geq \eps n/2$ and
$x-y \in CK$.
\end{theorem}

We recall that Rothvoss' algorithm, for the special case $y = 0$ (the general
case is similar), works by computing the Euclidean projection of a Gaussian
random vector onto $K \cap [-1,1]^n$. The above statement deviates from the
corresponding Lemma in~\cite{rothvoss-giann} in that it does not assume that $K
\subseteq H$ or that $H$ is known to the algorithm. It is not hard to verify
however that this condition is not needed in analysis, so we defer discussion of
the proof of this statement to the full version. The flexibility gained by not
needing to know the subspace in advance will be very useful in the sequel. We
note that one can also adapt the analysis of the algorithm of Singh and
Eldan~\cite{ES14}, which maximizes over $K \cap [-1,1]^n$ using the Gaussian as
the objective vector instead of projecting it, to work in the above setting.   

Our proof of Lemma~\ref{lem:partial-via-volume} will in fact be a direct
reduction to Rothvoss' theorem. The core of our reduction is the following
geometric theorem, which shows that if all the coordinate sections of $K$ of
proportional dimension have large volume, then there exists a subspace $H$ of
proportional dimension on which $K$ has large Gaussian measure. 

\begin{theorem}[Gaussian Measure via Volume]
\label{thm:gauss-via-volume}
There exists a decreasing function $\eta: (0,1) \rightarrow \R_+$ such that the
following holds. For any $n \in \N$, $K \subseteq \R^n$ symmetric convex body,
$2 \leq k \leq n-1$, $\alpha = k/n$, such that $\forall S \subseteq [n]$,
$|S| = \alpha n$, $\vol_{\delta n}(K_S) \geq 1$, there exists a linear
subspace $H$ of dimension $(1-\delta)n$ for which $\gamma_H(\eta(\delta) K) \geq
e^{-\delta n}$.
\end{theorem}

We note that the above theorem is primarily interesting in the case where
$\delta$ is a fixed constant, as $\eta(\delta) = e^{O(1/\delta)}$ blows up quite
quickly as $\delta \rightarrow 0$. Lemma~\ref{lem:partial-via-volume} now
follows directly combining the above with Theorem~\ref{thm:roth-giann}, as shown
below.

\begin{proof}[Proof of Lemma~\ref{lem:partial-via-volume}] 
Let $\eps = 1/60000$ and $\delta = (3/2) \eps \log_2(1/\eps)$. By
Theorem~\ref{thm:gauss-via-volume}, $\eta(\delta) K$ satisfies the conditions
for applying Theorem~\ref{thm:roth-giann} on any $x \in (-1,1)^n$ with
`parameters $\eps$ and $\delta$ as in the last sentence. This yieds
Lemma~\ref{lem:partial-via-volume} with $\eps_0 = \eps/2$, $\delta = \delta$ and
$C = O(\eta(\delta)) = O(1)$, as needed.   
\end{proof}

While there are examples where the volume lower bound is a $O(\log n)$ factor
off from hereditary discrepancy (e.g.~$3$ permutations), we conjecture that the
volume lower bound actually characterizes a hereditary version of partial
coloring discrepancy. Namely, if the volume lower bound is $D$, we conjecture
that there exists a subset vectors for which every (fractional) partial coloring
has discrepancy $\Omega(D)$. Recall that Lemma~\ref{lem:partial-via-volume}
gives the other direction, i.e.~that there always exist partial colorings of
discrepancy $O(D)$. If this conjecture were true, then Rothvoss' algorithm,
which we use as a blackbox, would in a weak sense be optimal for finding
partial colorings. We discuss this conjecture in more detail in the next
subsection. 

Comparing to prior works, Theorem~\ref{thm:gauss-via-volume} provides a useful
and different route for proving that a body (or at least a large section of it)
has exponentially small Gaussian measure. In the context of discrepancy, to the
authors knowledge, only two main techniques were used to prove such bounds,
neither of which is directly applicable in the above setting. The first
technique consists of combining chaining techniques and moment bounds, which can
generally only measure when the body has Gaussian measure close to $1/2$.  This
approach loses the leverage we have in only needing exponentially small bounds
and thus often incurs additional logarithmic factors. The second strategy is
based on the positive correlation properties of Gaussian measure, first proved
for the intersection of symmetric slabs (i.e.~Sidak's lemma), and with the
recent resolution of the Gaussian correlation conjecture~\cite{Royen14}, for the
intersection of arbitrary symmetric convex bodies. More precisely, one tries to
show that $K$ contains (or equals) the intersection of ``simpler'' symmetric
convex bodies $K_1,\dots,K_T$ (most often slabs) to deduce that $\gamma_n(K)
\geq \prod_{i=1}^T \gamma_n(K_i)$, from which one can usefully get exponentially
small bounds.

In contrast, our proof of the $e^{-\delta n}$ lower bounds on Gaussian measure
in the above theorem proceeds via a direct covering argument. Namely, if one can
show that $e^{\delta n-1}$ translates of $K$ cover the Euclidean ball of radius
$\sqrt{n}$, which has Gaussian measure $\geq 1/2$, then one can directly deduce
that 
\[
1 \leq 2\gamma_n(\sqrt{n}B_2^n) \leq 2N(\sqrt{n} B_2^n,K) \max_{t \in \R^n}
\gamma_n(K+t) \leq e^{\delta n} \gamma_n(K) ,
\]
where we have used that $\max_{t \in \R^n} \gamma_n(K+t) = \gamma_n(K)$ for a
centrally symmetric convex body, which follows by the symmetry and logconcavity
of Gaussian measure. We will adopt this strategy on a section of $K$, which is
chosen to align with the longest axes of a so-called regular M-ellipsoid for
$K$. The volumetric condition in Lemma~\ref{lem:partial-via-volume} will in fact
be used to guarantee that these axes have length $\Omega(\sqrt{n})$, which makes
the above strategy plausible. We recall that an M-ellipsoid~ $E$ of $K$ is an
ellipsoid which approximates $K$ well from the perspective of covering,
i.e.~$2^{O(n)}$ shifts of $K$ suffice to cover $E$ and vice versa. The existence
of such ellipsoids was first proven by Milman~\cite{Milman86-reverseBM}. We give
precise definitions below. 

For any two sets $A,B \subseteq \R^n$, let 
\[
N(A,B) = \min \set{|\Lambda|: \Lambda \subset \R^n, A \subseteq \Lambda+B} ,
\]
denote the minimum number of shifts of $B$ needed to cover $A$. The following
theorem of Pisier~\cite{Pisier-book}, gives the existence of M-ellipsoids whose
covering estimates have polynomial decay. The decay estimate will be used to
make the Gaussian measure of the large section of $K$ we find as close to $1$ as
we like after a sufficient scaling. 

\begin{theorem}[Regular M-ellipsoid]
There exists an absolute constant $c_0 > 0$, such that any $0 < \alpha < 2$,
letting $\sigma(\alpha) = c_0(2-\alpha)^{-1/2}$, $n \in \N$, and symmetric
convex body $K \subseteq \R^n$, there exists an ellipsoid $E \subseteq \R^n$,
$\vol_n(E)=\vol_n(K)$, such that for all $t \geq 1$
\[
\max \set{N(K,tE),N(E,tK),N(K^\circ,tE^\circ),N(E^\circ,tK^\circ)} \leq
e^{\sigma(\alpha) n / t^\alpha} \text{ .}
\]
\end{theorem}

To relate volumes of sections of $K$ to corresponding projection of $K^\circ$,
we will need the following well-known inequality:

\begin{theorem}[Blashke-Santal{\'o}]\label{thm:santalo} 
Let $K \subseteq \R^n$ be a symmetric convex body. Then $\vol_n(K)
\vol_n(K^\circ) \leq \kappa_n^2$, where equality holds if and only if $K$ is
an origin centered ellipsoid. 
\end{theorem}

To show that the axes of the M-ellipsoid of $K$ are long, we will need to relate
the axis lengths to the volumes of coordinate projections of the polar
ellipsoid. For this, purpose we will require the following formula for
coordinate projection volumes. 

\begin{lemma}\label{lem:ellipsoid-volumes}
Let $E := E(Q) \subseteq \R^n$ be an origin center ellipsoid. Then, for any
$S \subseteq [n]$, $|S| = k$, we have that 
\[
\vol_k(\pi_S(E^\circ)) = \kappa_k \det(Q_{S,S})^{1/2} \text{ .}
\]
\end{lemma}
\begin{proof}
Recall that $E^\circ = E(Q^{-1}) = Q^{1/2} B_2^n$, where $Q^{1/2}$ is the
positive definite square root of $Q$. To begin, we recall that the support
function of $E^\circ$ can be computed by
\begin{align*}
h_{E^\circ}(w) &\eqdef \max_{y \in E^\circ} \inner{w}{y} 
               = \max_{z \in B_2^n} \inner{w}{Q^{1/2} z} 
               = \|Q^{1/2} w\| = \sqrt{w^\T Q w}.
\end{align*}
Let $W_S = \lspan\{e_i: i \in S\}$. Note that by construction, $\pi_S(E^\circ)
\subseteq W_S$ and $h_{\pi_S(E^\circ)}(w) = h_{E}(w)$, $\forall w \in W_S$.
Furthermore, by duality, among convex bodies these conditions uniquely define $\pi_S(E^\circ)$. 

Let $s_1 < s_2 < \dots < s_k$ be the elements of $S$ and let $P_S =
(e_{s_1},\dots,e_{s_k})$, noting that $P_S P_S^\T = \pi_S$. Let $T = P_S
(Q_{S,S})^{1/2} \in \R^{n \times k}$. We now show that $TB_2^k = \pi_S(E^\circ)$
using the aforementioned conditions. Clearly $TB_2^k \subseteq W_S$ since
$\lspan(P_S) = W_S$. For $w \in W_S$, letting $w_S = (w_{s_1},\dots,w_{s_k})^\T$
denote the restriction to the coordinates in $S$, we have that 
\begin{align*}
h_{TB_2^k}(w) &= \max_{z \in B_2^k} \inner{T^\T w}{z} 
              = \max_{z \in B_2^k} \inner{(Q_{S,S})^{1/2} w_S}{x} \\ 
              &= \sqrt{w_S^\T Q_{S,S} w_S} = \sqrt{w^\T Q w} ,
\end{align*} 
where the last equality follows since $w_i = 0$ for $i \notin S$. Thus
$\pi_S(E^\circ) = TB_2^k$ as claimed. The volume can now be computed as follows:
\begin{align*}
\vol_k(TB_2^k) &= \kappa_k \det(T^\T T)^{1/2} 
               = \kappa_k \det((Q_{S,S})^{1/2} (P_S^\T P_S)
(Q_{S,S})^{1/2})^{1/2} \\
               &= \kappa_k \det(Q_{S,S})^{1/2} \text{ ,}
\end{align*}
as needed.
\end{proof}

The next lemma we will need is a simple determinantal analogue of the restricted
invertibility principle.

\begin{lemma}\label{lm:rip-det}
  Let $Q$ be an $n\times n$ real positive semi-definite matrix with
  eigenvalues $\lambda_1 \ge \ldots \ge \lambda_n$. For any integer
  $k$, $1 \le k \le n$, there exists a set $S \subseteq [n]$ of size $k$
  such that
  \[\prod_{i=1}^k \lambda_i \leq \binom{n}{k} \det(Q_{S,S}).\]
\end{lemma}
\begin{proof}
To prove the lemma, we will rely on the classical identity for applying the
elementary symmetric polynomials to the eigen values of $Q$:
\begin{equation*}
\sum_{S \in [n],|S|=k} \prod_{i \in S} \lambda_i \eqdef p_k(\lambda) = \sum_{S \subset [n]: |S| = k}\det(Q_{S,S}).
\end{equation*}
To verify this equation, consider the coefficient of $t^{n-k}$ in the polynomial
$\det(Q + tI)$. Calculating the coefficient using the Leibniz formula for the
determinant gives the right hand side; calculating it using $\det(Q + tI) =
(\lambda_1 + t)\ldots(\lambda_n + t)$ gives the left hand side. Since the eigen
values are all non-negative, we get that
\[
\prod_{i=1}^k \lambda_i \leq p_k(\lambda) =  
 \sum_{S \subseteq [n]: |S|=k} \det(Q_{S,S}) \leq \binom{n}{k} \max_{S
\subseteq [n]: |S|=k} \det(Q_{S,S}),
\]
as needed.
\end{proof}

We now have all the ingredients needed to prove our main geometric estimate.

\begin{proof}[Proof of Theorem~\ref{thm:gauss-via-volume}]
Let $E := E(Q)$ denote a $1$-regular M-ellipsoid for $K$ and let $\sigma :=
\sigma(1)$. Let $l_1 \geq \dots \geq l_n > 0$ denote
the length of the principal axes of $E$, where we recall that
$l_n^{-2}\geq \dots \geq l_1^{-2} > 0$ are then the eigen values of $Q$.

By the Blashke-Santal{\'o} inequality, for all $S \subseteq
[n]$, $|S|=\delta n$, we have that 
\[
\vol_{\delta n}(K_S)\vol_{\delta n}((K_S)^\circ) = 
\vol_{\delta n}(K_S) \vol_{\delta n}(\pi_S(K^\circ)) \leq
\kappa_{\delta n}^2 \text{ .}
\]
Since we assume $\vol_{\delta n}(K_S) \geq 1$, the above implies that
$\vol_{\delta n}(\pi_S(K^\circ)) \leq \kappa_{\delta n}^2$. The
coordinate projections of $E^\circ$ thus have volume at most 
\[
\vol_{\delta n}(\pi_S(E^\circ)) \leq N(E^\circ,K^\circ)
\vol_{ \delta n}(\pi_S(K^\circ)) \leq e^{\sigma n} \kappa_{\delta n}^2 \text{ .}
\]
Combining Lemma~\ref{lm:rip-det} and~\ref{lem:ellipsoid-volumes}, we have that
\begin{align*}
\prod_{i=(1-\delta)n+1}^n l_i^{-1} &\leq \binom{n}{\delta n}^{1/2} \max_{S \subseteq
[n],|S|=\delta n} \det(Q_{S,S})^{1/2} \\
&= \binom{n}{\delta n}^{1/2} \max_{S \subseteq [n],|S|=\delta n}
\vol_{\delta n}(\pi_S(E^\circ)) \kappa_{\delta n}^{-1} \\
&\leq \binom{n}{\delta n}^{1/2} e^{\sigma n} \kappa_{\delta n} \text{ .}
\end{align*}
From here, we conclude that 
\[
l_{(1-\delta)n} \geq \prod_{i=(1-\delta)n+1}^n l_i^{\frac{1}{\delta
n}} \geq \binom{n}{\delta 
n}^{-\frac{1}{2\delta n}} e^{-\frac{\sigma}{\delta }} \kappa_{\delta
n}^{-\frac{1}{\delta n}} \geq 
\frac{1}{e^{2\frac{\sigma}{\delta }}} \cdot \sqrt{\frac{\delta n}{2\pi e}} :=
c(\delta)^{-1} \sqrt{n} \text{ .}
\] 
Letting $H$ be the span of the first $(1-\delta)n$ principal
axes of $E$, we thus conclude that 
\[
\sqrt{n} (B_2^n \cap H) \subseteq c(\delta) (E \cap H). 
\]
Using the $1$-regularity of $E$, letting $t = 2 \sigma / \delta$, we derive the
following covering estimate 
\begin{align*}
N(\sqrt{n} (B_2^n \cap H), 2 t c(\delta) (K \cap H))
&\leq N(c(\delta)(E \cap H), 2 t c(\delta) (K \cap H)) \\
&\leq N(E, t K) \leq e^{\sigma n / t} = e^{\delta n/2} \text{ .}
\end{align*}
Since $\gamma_H(\sqrt{n} B_2^n \cap H) \geq 1/2$, setting $\eta := \eta(\delta)
=\frac{2c(\delta)\sigma}{\delta}$, we get that $\gamma_H(\eta K \cap H) \geq
\frac{1}{2} e^{-\delta n/2} \geq e^{-\delta n}$, as needed. Lastly, $\eta$ as
defined above is easily checked to be decreasing in $\delta$.
\end{proof}

\subsection{The Discrepancy of Partial Colorings}

In this section we discuss a geometric conjecture which would imply that a tight
relationship between the discrepancy of partial colorings and the volume lower
bound, and thus a weak form of optimality for Rothvoss' partial coloring
algorithm. For this purpose, we formally define the partial coloring discrepancy
as well as its hereditary version. Given $(u_i)_{i=1}^N \in \R^n$, symmetric
convex body $K \subseteq \R^n$ and $\alpha \in (0,1]$,  we define  
\begin{equation}
\label{def:partial-disc}
\begin{split}
&\disc_\alpha(( u_i)_{i=1}^N,K) \eqdef \min_{\substack{x \in [-1,1]^N \\
|\bnds(x)| \geq \alpha N}} \Bigl\|\sum_{i=1}^N x_i u_i\Bigr\|_K .
\end{split}
\end{equation}
and
\begin{equation}
\label{def:partial-hd}
\hd_\alpha((u_i)_{i=1}^N,K) \eqdef \max_{S \subseteq [N]} \disc_\alpha((u_i)_{i \in S}, K) .
\end{equation}

We recall that (repeated applications) of Lemma~\ref{lem:partial-via-volume}
implies the upper bound
\[
\hd_{1/2}((u_i)_{i=1}^N,K) \leq O(1) \vollb^{\rm h}((u_i)_{i=1}^N,K).
\]
Here we conjecture that the reverse inequality should also hold.

\begin{conjecture}
\label{conj:partial-volume}
There exists a universal constant $c \geq 1$, such that for any $n \in \N$,
$u_1,\dots,u_n \in \R^n$ linearly independent and symmetric convex body $K
\subseteq \R^n$: 
\begin{equation}
\vollb^{\rm h}((u_i)_{i=1}^n,K) \leq c \hd_{1/2}((u_i)_{i=1}^n,K)
\end{equation}
\end{conjecture}

Note that we restrict above to linear independent subsets of vectors, but as is
well-known (e.g.~see proof of Theorem~\ref{thm:tightness}), this is without loss
of generality. As a pathway to prove the conjecture, we suggest the following
natural geometric analog of the so-called spectral lower bound for discrepancy
into $\ell_2$.

\begin{lemma}
\label{lem:speclb}
Let $(u_i)_{i=1}^n \in \R^n$ be linearly independent, $K \subseteq \R^n$ be a
symmetric convex body, and $\alpha \in [0,1]$. For any subset $S \subseteq [n]$,
$|S|=k$, letting $W_S := \lspan\{u_i: i \in S\}$, define
\[
\speclb((u_i)_{i \in S},K) := 
\max \set{r \geq 0: r K \cap W_S \subseteq k \conv(\pm u_i: i \in S)}.
\]
Then, we have that
\begin{equation}
\label{eq:spec-lb}
\disc_\alpha((u_i)_{i \in S},K) \geq \alpha \speclb((u_i)_{i \in S},K).
\end{equation}
In particular, defining
\[
\speclb^{\rm h}((u_i)_{i=1}^n,K) := \max_{S \subseteq [n]} \speclb((u_i)_{i \in
S},K)
\]
we have that
\begin{equation}
\label{eq:hd-spec-lb}
\hd_{\alpha}((u_i)_{i=1}^n,K) \geq \alpha \speclb^{\rm h}((u_i)_{i=1}^n,K).
\end{equation}
\end{lemma}
\begin{proof}
We prove only~\eqref{eq:spec-lb} since then \eqref{eq:hd-spec-lb} follows
trivially. For~\eqref{eq:spec-lb}, by replacing $K$ by $K \cap W_S$, we may wlog
assume that $|S|=n$ and $W_S = \R^n$. 

Let $x \in [-1,1]^n$, $|\bnds(x)| \geq \alpha n$, and $u_x = \sum_{i=1}^n x_i
u_i$. Our goal is to show that $\beta := \norm{u_x}_K \geq \alpha r$, where $r
:= \speclb((u_i)_{i=1}^n,K)$.

Let $(u_i^*)_{i=1}^n$ denote the corresponding dual basis of
$(u_i)_{i=1}^n$, i.e.~satisfying $\inner{u_i^*}{u_j} = 1$ if $i=j$ and $0$
otherwise, which exists by linear independence. Now letting $v_x =
\sum_{i=1}^n {\rm sign}(x_i) u_i^*$, it is easy to check that 
\[
\inner{v_x}{u_x} = \sum_{i=1}^n |x_i| \geq |\bnds(x)| \geq \alpha n .
\]
Since $u_x \in \beta K$ and $r K \subseteq n \conv(\pm u_i: i
\in [n])$, we have that
\begin{align*}
\alpha n \leq \beta \max_{z \in K} \inner{v_x}{z} 
         \leq \frac{\beta}{r} n 
               \max_{z \in \conv(\pm u_i: i \in [n])} \inner{v_x}{z}
          = \frac{\beta}{r} n .
\end{align*}
The desired inequality now follows by rearranging.
\end{proof}

We note that as with $\vollb^{\rm h}$, one may extend the $\speclb^{\rm h}$ to
an arbitrary sequence of vectors $(u_i)_{i=1}^N$, however one must take care to
optimize only over subsets of linearly independent vectors, since otherwise the
conclusion of Lemma~\ref{lem:speclb} is false.

Given the above, it suffices to prove Conjecture~\ref{conj:partial-volume} with
$\hd_{1/2}$ replaced by $\speclb^{\rm h}$. The resulting stronger conjecture has a
very natural geometric interpretation which we expand on below. For
$(u_i)_{i=1}^n \in \R^n$ linearly independent and $K \subseteq \R^n$ a symmetric
convex body, letting $T$ denote the linear map sending $(u_i)_{i=1}^n$ to
$(e_i)_{i=1}^n$, it is direct to check that $\tau((u_i)_{i=1}^n,K) =
\tau((e_i)_{i=1}^n,TK)$ for $\tau \in \set{\speclb^{\rm h},\vollb^{\rm h}}$.
Thus, for the purpose of the conjecture, it suffices to consider the setting
where the vectors are the standard basis. In this setting, we see that
\[
\speclb^{\rm h}((e_i)_{i=1}^n,K) = \max_{S \subseteq [n]} \max \set{r \geq
0: r K_S \subseteq |S| B_1^S} 
\]
and that 
\[
\vollb^{\rm h}((e_i)_{i=1}^n,K) = \max_{S \subseteq [n]}
\vol_{|S|}(K_S)^{-1/|S|} \text{ .} 
\]
The goal is now to show that for every $S_0 \subseteq [n]$, there exists $S_1
\subseteq [n]$, such that 
\begin{equation}
\label{eq:conj-simple}
\vol_{|S_0|}(K_{S_0})^{-1/|S_0|} \leq c \max \set{r \geq 0: r K_{S_1} \subseteq
|S_1| B_1^{|S_1|}} \text{ .}
\end{equation}
Since this must hold for every symmetric convex body $K$, we may assume that
$S_0 = [n]$ (and thus $S_1 \subseteq S_0$). Furthermore, by homogeneity, we may
also assume that $\vol_n(K)=1$. In this case~\eqref{eq:conj-simple}, and hence
Conjecture~\ref{conj:partial-volume}, directly reduces to the following
geometric conjecture.

\begin{conjecture}[Restricted Isometry Principle for Convex Bodies]
\label{conj:conv-restr-iso} There exists an absolute constant $c \geq 1$, such
for any $n \in \N$, symmetric convex body $K \subseteq \R^n$ of volume $1$,
there exists $S \subseteq [n]$ such that $K_S \subseteq c |S| B_1^S$.
\end{conjecture}

Another natural, and again stronger conjecture, would be to ask for containment
inside $c \sqrt{|S|} B_2^S \subseteq c |S| B_1^S$. We suspect that this version
should also hold, and indeed, our evidence for the conjecture supports this
belief. 

Two natural weakenings of Conjecture~\ref{conj:conv-restr-iso} are to ask
whether (a) it holds for ellipsoids and (b) whether it holds for general bodies
but with coordinate sections replaced by arbitrary sections. As our main
evidence for the conjecture, we show that both assertions indeed hold with the
stronger containment relation with respect to $B_2^n$. We note that (a) indeed
implies Conjecture~\ref{conj:partial-volume} when $K$ is an ellipsoid.  

Before formally stating our partial results, we give a last simplification of
Conjecture~\ref{conj:conv-restr-iso}. In particular, when attempting to prove
the conjecture we may additionally assume that $\vol_{|S|}(K_S) \geq 1$ for all
$S \subset [n]$. This follows by noting that if there exists $S \subset [n]$
with $\vol_{|S|}(K_S) < 1$, we may simply apply induction on $K_S$, after
scaling it up to have volume $1$ (which only makes the task more difficult). The
main two lemmas we can prove, from which the above results easily follow,
are stated below. 

\begin{lemma}
\label{lem:axis-m-ell}
Let $K \subseteq \R^n$ be a symmetric convex body of volume $1$ satisfying
for all $S \subseteq [n]$, $\vol_{|S|}(K_S) \geq 1$. Let $E \subseteq \R^n$ be a
$1$-regular M-ellipsoid of $K$. Then there exists $S \subseteq [n]$, $|S| =
\Theta(n)$, such that $E_S \subseteq O(\sqrt{|S|}) B_2^S$.
\end{lemma}

\begin{lemma}
\label{lem:cover-to-section}
Let $K \subseteq \R^n$ be a symmetric convex body such that $N(K,\sqrt{n}B_2^n)
\leq 2^{O(n)}$. Then there exists a linear subspace $W \subseteq \R^n$, $\dim(W) =
\Theta(n)$, such that $K \cap W \subseteq O(\sqrt{|W|}) B_2^W$.  
\end{lemma}

To derive the conjecture when $K$ is an ellipsoid, we simply apply
Lemma~\ref{lem:axis-m-ell} with $E=K$. To derive the conjecture for general
sections, we first apply Lemma~\ref{lem:axis-m-ell} to $K$, noting that the
produced section $K_S$ now satisfies the conditions of
Lemma~\ref{lem:cover-to-section}, from which we derive the result.  

For the proof of Lemma~\ref{lem:axis-m-ell}, we use the volumetric conditions to
show that the central axes of $E$ all have length $\Theta(\sqrt{n})$,
complementing the $\Omega(\sqrt{n})$ lower bound used in the proof of
Theorem~\ref{thm:gauss-via-volume} under the additional assumption that
$\vol_n(K)=1$, and derive the existence of the
corresponding section from the Bourgain-Tzafriri restricted isometry principle
applied to the quadratic form defining $E$. The proof of
Lemma~\ref{lem:cover-to-section} follows relatively directly from Milman's
quotient of subspace (QS) theorem, which states that there exists a projection
of a section of $K$ of proportional dimension which is constant factor
isomorphic to a Euclidean ball. We remark that the main step that seems to be
missing for the proof of Conjecture~\ref{conj:conv-restr-iso} is a
corresponding ``coordinate version'' of the QS theorem. As the proofs of the
above lemmas require a fair amount of machinery, and only yield partial evidence
for a conjecture, we defer detailed proofs to the full version.  

\section{Convex Hulls}

Here we show that the volume lower bound is maximized at the extreme
points of a convex set.

\begin{theorem}\label{thm:conv-hull}
  Let $v_1, \ldots, v_m$ be points in $\R^n$, and let $C \eqdef
  \mathrm{conv}\{\pm v_1, \ldots, \pm v_m\}$. Then, for symmetric
  convex body $K$ in $\R^n$,
  \[
  \sup_{u_1, \ldots, u_n \in C}\vollb((u_i)_{i = 1}^N, K)
  \le
  \vollb((v_i)_{i = 1}^m, K).
  \]
\end{theorem}

We will use a theorem of K.~Ball~\cite{Ball88}, which allows us to
define a norm associated with an arbitrary logarithmically concave
function $f$.
\begin{theorem}\label{thm:ball-logconcave}
  Let $f: \R^n \to [0, \infty)$ be an even logarithmically concave
  function such that $0 < \int_{\R^k} f < \infty$. Then, for any $p
  \ge 1$, 
  \[
  \|x\|_{f,p} \eqdef 
  \begin{cases}
    \left(\int_0^\infty f(rx) r^{p-1}dr\right)^{-1/p}, &x \neq 0,\\
    0, &x = 0.
  \end{cases}
  \]
  defines a norm on $\R^n$. 
\end{theorem}


\begin{proof}[Proof of Theorem~\ref{thm:conv-hull}]
  It is enough to show that, for any integer $k$ between $1$ and $n$,
  if we keep $u_1, \ldots, u_{k-1} \in \R^n$ fixed, and for any $x\in
  \R^n$ define $U_x = (u_1, \ldots u_{k-1}, x)$, then the function $g:
  \R^n \to [0, \infty)$ defined by
  \[
  g(x) = \frac{\det(U_x^\T U_x)^{1/(2)}}{\vol_k(K \cap \lspan\{u_1,
    \ldots, u_{k-1}, x\})}
  \]
  achieves its maximum on $C$ at one of the extreme points $\pm v_1,
  \ldots, \pm v_m$. Furthermore, this
  follows immediately if $g$ is convex. Below, we
  use Theorem~\ref{thm:ball-logconcave} to prove the convexity of $g$.

  Define the matrix $U = (u_1, \ldots, u_{k-1})$ and let $G = U^\T U$
  be the Gram matrix of $u_1, \ldots, u_{k-1}$. Let $W =
  \lspan\{u_1, \ldots, u_{k-1}\}$, and denote by $P_{W^\perp}$ the
  orthogonal projection onto the orthogonal complement $W^\perp$ of $W$.
  Since $\det(U_x^\T U_x)^{1/(2)}$ is the $k$-dimensional volume of
  the parallelepiped formed by $u_1, \ldots, u_{k-1}, x$, and
  $\det(G)^{1/2}$ is the $(k-1)$-dimensional volume of the
  parallelepiped formed by $u_1, \ldots, u_{k-1}$, we have
  \[
  g(x) = \frac{\det(G)^{1/2}\|P_{W^\perp}x\|_2}{\vol_k(K \cap (W +
    \lspan\{x\}))}. 
  \]
  Notice that both the numerator and the denominator on the right hand
  side are unchanged if we replace $x$ by $P_{W^\perp}x$.
  To show that that $g$ is convex, it is, therefore, enough to show that it is
  convex on $W^\perp$.  For $x \in W^\perp$, define $f(x) \eqdef
  \vol_{k}(K \cap (W + x))$. By the Brunn-Minkowski inequality, and the
  symmetry of $K$, $f$ is en even log-concave function on
  $W^\perp$. We have
  \[
  \vol_{k+1}(K \cap  (W + \lspan\{x\}))
  = 
  \int_{-\infty}^\infty{f(tx/\|x\|_2)dt} 
  = 
  2\|x\|_2 \int_{0}^\infty{f(tx)dt}.
  \]
  Therefore, 
  \[
  g(x) = 
  \frac{\det(G)^{1/2}\|x\|_2}{\vol_{k}(K \cap  (W + \lspan\{x\}))}
  = 
  \frac{\det(G)^{1/2}}{2\int_{0}^\infty{f(tx)dt}}
  = \frac{\det(G)^{1/2}}{2}\|x\|_{f,1}
  \]
  is a norm on $W^\perp$, by Theorem~\ref{thm:ball-logconcave} (after
  identifying $W^\perp$ with $\R^{n-k+1}$), and, therefore, convex. The
  theorem follows.
\end{proof}

\cut{
Let $v_1, \ldots, v_m \in \R^n$ and define a norm $X$ on $\R^n$ with
unit ball $B_X = \mathrm{conv}\{\pm v_1, \ldots, \pm v_m\}$. By
Theorem~\ref{thm:tightness}, we have that 
\[
1 \le \frac{\beta(X, Y)}{\sup_{u_1, \ldots, u_n \in
    B_X}\vollb((u_i)_{i = 1}^n, Y)} \le C K(Y)(1+\log n).
\]
Theorem~\ref{thm:conv-hull} allows us to write the stronger inequality
\[
1 \le \frac{\beta(X, Y)}{\vollb((v_i)_{i = 1}^m, Y)} \le C K(Y)(1+\log n).
\]
This also implies that
\begin{equation}\label{eq:beta-conv-hull}
\beta(X, Y) \le CK(Y)(1+\log n) \beta((v_i)_{i = 1}^m, Y),
\end{equation}
i.e.~the vector balancing constant does not increase much when we take
convex hulls. 

\medskip\noindent
\textbf{Questions}:
\begin{itemize}
\item Does \eqref{eq:beta-conv-hull} hold with $CK(Y)(1+\log n)$
  replaced by a fixed constant?
\item Is there a more direct proof of \eqref{eq:beta-conv-hull}?
\end{itemize}
}

\section{The Factorization Approach}

In the inductive proof of Theorem~\ref{thm:tightness} we need to keep
alternating between computing Gaussian estimates based on volume
information, and applying Theorem~\ref{thm:roth-giann}, as we induct
on lower dimensional subspaces. One downside of this indirect nature
of our proof is that it does not lead to an algorithm for computing
the vector balancing constant $\vb(C,K)$ of two symmetric convex
bodies $C$ and $K$, or of the hereditary discrepancy $\hd((u_i)_{i =
  1}^N, K)$. In this section, we explore a different approach, which
aims at separating computing a Gaussian estimate from volume
information, on one hand, and the vector balancing argument on the
other. The approach is based on recasting vector balancing in the
language of linear operators between normed spaces, and on factoring
such operators through Euclidean space. This factorization strategy is
a significant generalization of the connection between the $\gamma_2$
norm and hereditary discrepancy used in~\cite{disc-gamma2}.

Let $U:X \to Y$ be a linear operator between two finite-dimensional
normed spaces $X = (\R^n, \| \cdot\|_X)$ and $Y = (\R^n, \| \cdot
\|_Y)$. A natural way to define the vector balancing constant of $U$
is
\[
\vb(U) = \vb(U(B_X), B_Y),
\]
where $B_X$ and $B_Y$ are, respectively, the unit balls of $X$ and
$Y$. Equivalently, we have
\[
\vb(U) = \sup\{\hd((Uu_i)_{i = 1}^N, B_Y)\},
\]
where the supremum is over all finite sequences $(u_i)_{i = 1}^N$ of
vectors in $B_X$. If $C$ and $K$ are two centrally symmetric convex
bodies in $\R^n$, and we define the corresponding normed spaces $X_C =
(\R^n, \|\cdot\|_C)$ and $X_K = (\R^n, \|\cdot\|_K)$, then the vector
balancing constant $\vb(I)$ of the identity operator $I:X_C \to X_K$
recovers $\vb(C, K)$. 

We will relate $\vb(U)$ to an efficiently computable quantity
$\lambda(U)$, which will allow us to efficiently approximate $\vb(U)$
(given appropriate access to the domain and range $X$ and
$Y$). Moreover, we will see that for the operator $U:\ell_1^N \to X_K$
given by $Ue_i = u_i$, $\lambda(U)$ also approximates $\hd((u_i)_{i =
  1}^N, K)$.

\subsection{Preliminaries}

\snote{Put all prelims in one place, and add prelims for dual
  operator, Schur convexity, $K$-convexity.}

We will almost exclusively deal with normed spaces defined on $\R^n$.
For a normed space $X = (\R^n, \|\cdot\|_X)$ we denote by $X^*$ the
dual space with norm $\|x^*\|_{X^*} \eqdef \sup\{\langle x, x^*
\rangle: \|x\|_X \le 1\}$. Since we work only with finite dimensional
spaces, we always have that $X^{**} = X$.

We will use the tensor product notation $x^* \otimes y$ for the rank-1
linear operator from a normed space $X$ to a normed space $Y$, defined
by $(x^* \otimes y)(x) = \langle x, x^*\rangle y$, where $x^* \in X^*$
and $y \in Y$. Note that if $X$ and $Y$ are normed spaces over $\R^n$,
the matrix of $x^* \otimes y$ with respect to the standard basis is
$yx^\T$. Any linear operator $A:X \to X$ on an $n$-dimensional normed
space $X$ can be written as $A = \sum_{i = 1}^n{x^*_i \otimes y}$ for
$x_1^*, \ldots, x_n^* \in X^*$ and $y_1, \ldots, y_n \in X$. The trace
of $A$ is then defined by
\[
\tr(A) \eqdef \sum_{j = 1}^n{\langle y_j, x^*_j \rangle}.
\]
This abstract definition agrees with the usual one, i.e.~if the matrix
of $A$ is $M$, then $\tr(A) = \tr(M)$. We will identify linear
functionals  on operators $A:X \to Y$, where $X$ and $Y$ are
$n$-dimensional, with operators $B:Y \to X$ via $f_B(A) = \tr(BA)$. 

A linear operator $A: X \to X^*$ on normed space $X$ defines a
bilinear form $B$ on $X \times X$, given by $B(x,y) = \langle x, Ay
\rangle$. We will say that $A$ is positive definite if the
corresponding bilinear form is symmetric and positive definite,
i.e.~if $\langle x, Ay \rangle = \langle Ax, y\rangle$ for all $x,y
\in X$, and $\langle x, Ax \rangle >0$ for all nonzero $x \in X$; $A$
is positive semidefinite if instead we have $\langle x, Ax \rangle \ge
0.$ In the case of $\R^n$ this is equivalent to stating that the matrix
$M$ of $A$ with respect to the standard basis is positive definite,
i.e.~is symmetric and all its eigenvalues are positive. We write $A
\succ 0$ to denote that $A$ is positive definite, and $A \succeq 0$ to
denote that it is positive semidefinite. 

For a positive definite operator $A:\ell_2^n \to \ell_2^n$, and a
positive integer $k$, there exists a unique positive definite operator
$B:\ell_2^n \to \ell_2^n$ such that $B^k = A$. We use the notation
$A^{1/k}$ for $B$. We also use the shorthand notation $A^{\ell/k} \eqdef
(A^\ell)^{1/k}$ for (positive or negative) integers
$\ell$. Equivalently, we can derive $A^{\ell/k}$ by raising every
eigenvalue of $A$ to the power $\ell/k$ in the spectral decomposition
of $A$.

We also recall that the operator norm $\|A\|$ of a linear operator
$A:X \to Y$ is defined by 
\[
\|A\| = \sup\{\|Ax\|_Y: \|x\|_X \le 1\},
\]
where $X = (\R^n, \|\cdot\|_X)$ and $Y = (\R^n, \|\cdot\|_Y)$ are
normed spaces. 

A related norm on operarots is the nuclear norm. Here we only use the
nuclear norm $\nu(A)$ of an operator $A:\ell_2^n \to \ell_2^n$, which
equals the sum of its singular values. It is easy to see that
\[
\nu(A) = \tr((AA^*)^{1/2}).
\]
The nuclear norm is dual to the
operator norm with respect to trace duality, and in particular we have
the identity
\[
\nu(A) = \sup\{\tr(AO): O \text{ orthogonal transformation}\}.
\]



\subsection{The Factorization Constant $\lambda$}

We will use the $\ell$-norm, which has been extensively studied in the
theory of operator ideals, and in asymptotic convex geometry. For
a linear operator $S:\ell_2^n \to Y$, where $Y = (\R^n, \|\cdot\|_Y)$
is a normed space, the $\ell$-norm of $S$ is defined as
\[
\ell(S) \eqdef \left( \int \|S(x)\|_Y^2 d\gamma_n(x) \right)^{1/2},
\]
where $\gamma_n$ is the standard Gaussian measure on $\R^n$. I.e., if
$Z$ is a standard Gaussian random variable in $\R^n$, then $\ell(S) =
(\E \|S(Z)\|_Y^2)^{1/2}$. It is easy to verify that $\ell(\cdot)$ is a
norm on the space of linear operators from $\ell_2^n$ to $Y$, for any
normed space $Y$ as above. 

Moreover, we can define a dual norm
$\ell^*$ via trace duality: for any linear opartor $R:
Y \to \ell_2^n$, we define
\[
\ell^*(R) \eqdef \sup\{\tr(RS): S: \ell_2^n \to Y, \ell(S) \le 1\}.
\]
Then $\ell$ and $\ell^*$ form a dual pair, and in particular we have
\[
\ell(S) = \sup\{\tr(RS): R:Y\to\ell_2^n, \ell^*(R) \le 1\}.
\]

We now define the main object of study in this section. Let $X$ and
$Y$ be normed spaces defined on $\R^n$, and let $U:X \to Y$ be a
linear operator. We define a factorization constant
\[
\lambda(U) \eqdef \inf \{\ell(S)\|T\|: T: X \to \ell_2^n,\ S: \ell_2^n
\to Y, U = ST\}.
\]
In other words, $\lambda(U)$ is the minimum of $\ell(S)\|T\|$ over all
ways to factor $U$ through $\ell_2^n$ as $U = ST$.

Our  motivation for defining $\lambda(U)$ is that it captures,
informally speaking, the best possible way to apply a vector balancing
theorem of Banaszczyk~\cite{bana} in order to get upper bounds on the
vector balancing constant. Let us first recall Banaszczyk's theorem.
\begin{theorem}\label{thm:bana}
  Let $K$ be a convex body in $\R^n$ such that $\gamma_n(K) \ge
  \frac12$. Then, for any sequence of vectors $v_1, \ldots, v_N \in
  B_2^n$, there exist signs $\eps_1, \ldots, \eps_N$ such that
  \[
  \sum_{i = 1}^N{\eps_i v_i}\in 5K.
  \]
\end{theorem}


We have the following theorem, which shows that $\lambda(U)$ is, up to
constants, an upper bound on $\vb(U)$ for any operator $U$. 

\begin{theorem}\label{thm:factorization}
  There exists a constant $C$ such that for any linear operator $U:X
  \to Y$ between two $n$-dimensional normed spaces $X, Y$, we have
  \[
  \vb(U) \le C\lambda(U).
  \]
\end{theorem}
\begin{proof}
  We may assume that $U$ is invertible, or otherwise we can replace
  $Y$ with its subspace restricted to the range of $U$. 
  Let $u_1, \ldots, u_N \in B_X$ be arbitrary, and let $T:X \to
  \ell_2^n$, $S:\ell_2^n \to Y$ be such that $\ell(S)\le \lambda(U) +
  \eps$ and $\|T\| \le 1$. For $i \in \{1, \ldots, N\}$, define $v_i
  \eqdef Tu_i$; since $\|T\| \le 1$ by assumption, we have $v_i \in
  B_2^n$ for all $i$. Let $K = \sqrt{2}\ell(S) S^{-1}(B_Y)$, and let,
  as usual,
  $\|\cdot\|_K$ be the norm with unit ball $K$. Observe that for any
  $x \in \R^n$, $\|x\|_K = \frac{1}{\sqrt{2}\ell(S)}\|Sx\|_Y$. By
  Chebyshev's inequality,
  \[
  1 - \gamma_n(K) \le \int \|x\|_K^2 d\gamma_n(x)
  =
  \frac{1}{2\ell(S)^2} \int \|Sx\|_Y^2 d\gamma_n(x)
  = \frac{1}{2}.
  \]
  We can, therefore, apply Theorem~\ref{thm:bana}, and have that there
  exist signs $\eps_1, \ldots, \eps_N$ such that 
  \[
  \sum_{i =1}^N{\eps_i v_i} \in 5K
  \iff
  \sum_{i = 1}^N{\eps_i ST u_i} \in (5\sqrt{2}\ell(S))B_Y
  \iff 
  \left\|\sum_{i = 1}^N{\eps_i u_i}\right\|_Y \le 5\sqrt{2}(\lambda(I_{X,Y})
    + \eps).
  \]
  Sending $\eps$ to $0$ and taking a supremum over
  $u_1, \ldots, u_N \in B_X$  finishes the proof.
\end{proof}

Theorem~\ref{thm:factorization} refines and generalizes the connection
between the $\gamma_2$ norm and hereditary discrepancy
from~\cite{disc-gamma2}. The $\gamma_2$ norm of an operator $U:X \to
Y$ is defined by
\[
\gamma_2(U) = \inf\{\|S\| \|T\|:  T: X \to \ell_2^n,\ S: \ell_2^n
\to Y, U = ST\}.
\]
In~\cite{disc-gamma2}, the authors studied the special case in which
$X = \ell_1^n$ and $Y = \ell_\infty^m$. In that case, it is easy to
see that, for an absolute constant $C$, 
\begin{equation}
  \label{eq:ell-vs-opnorm}
 \ell(S) \le C\sqrt{1 + \log m} \cdot\|S\|,
\end{equation}
for any operator $S: \ell_2^n \to \ell_\infty^m$. Therefore,
for any $U:\ell_1^n \to \ell_\infty^m$, we have $\lambda(U) \le
C\sqrt{1 + \log m}\cdot \gamma_2(U)$. This inequality and
Theorem~\ref{thm:factorization} recover the upper bound on hereditary
discrepancy in terms of the $\gamma_2$ norm
from~\cite{disc-gamma2}. However, \eqref{eq:ell-vs-opnorm} is often
not tight, and Theorem~\ref{thm:factorization} provides a tighter
upper bound. In particular, we will show that $\lambda$ gives an
improved approximation to the hereditary discrepancy. 

Our goal in the remainder of the section is to prove that the
inequality in Theorem~\ref{thm:factorization} holds in the reverse
direction as well, as captured in the following theorem.

\begin{theorem}\label{thm:fact-vollb}
  There exists a constant $C$ such that the following holds. Let $X$
  and $Y$ be two $n$-dimensional normed spaces and let $U:X \to Y$ be
  a linear operator between them.
  %, such that the unit
  %ball of $X$ is $B_X = \mathrm{conv}\{\pm v_1, \ldots, \pm v_m\}$. 
  Then
  \[
  \lambda(U) \le C K(Y) (1 + \log n)^{3/2} \sup\{\vollb((v_i)_{i =
    1}^N, B_Y)\},
  \]
  where $K(Y) = O(\log n)$ is the $K$-convexity constant of $Y$, and
  the supremum is taken over sequences $(v_i)_{i = 1}^N$ of extreme
  points of the unit ball $B_X$ of $X$.
\end{theorem}

Theorems~\ref{thm:factorization}~and~\ref{thm:fact-vollb} together
with Lemma~\ref{lem:vol-lb} give a characterization of $\vb(U)$ in
terms of $\lambda(U)$.

\begin{corollary}
  There exists a constant $C$ such that for any two $n$-dimensional
  normed spaces $X$ and $Y$, and any linear operator $U:X \to Y$
  between them, we have
  \[
  \frac1C \le \frac{\lambda(U)}{\vb(U)} \le C K(Y) (1 + \log n)^{3/2},
  \]
  where $K(Y) = O(\log n)$ is the $K$-convexity constant of $Y$. 
\end{corollary}

Moreover, we will show that, under reasonable assumptions,
$\lambda(U)$ can be computed in polynomial time.

\subsection{Convex Formulation}


The factorization constant $\lambda(U)$ can be formulated as the
solution to a convex optimization problem, i.e.~as the infimum of a
convex function over a convex domain. This fact is useful in that it
allows applying generic convex optimization techniques to compute
$\lambda(U)$ given appropriate access to the unit balls of $X$ and
$Y$. It will also be useful in deriving a dual formulation of
$\lambda(U)$, which we then use to prove Theorem~\ref{thm:fact-vollb}.

In what follows we fix two $n$-dimensional normed spaces $X = (\R^n,
\|\cdot\|_X)$ and $Y = (\R^n,\|\cdot\|_Y)$, and an invertible linear
operator $U:X \to Y$. Note that restricting to spaces of equal
dimension and to invertible operators is without loss of generality,
because we can restrict $U$ to the orthogonal complement of its
kernel, and replace $Y$ with the subspace given by the range of
$U$. 

We first define a function $f$ on positive definite operators which
will be the objective of our convex optimization formulation of
$\lambda(U)$. The definition is as follows: for any positive definite
operator $A:X \to X^*$, we set
\begin{equation}
  \label{eq:obj-def}
  f(A) \eqdef \ell(UT^{-1}),
\end{equation}
where $T:X \to \ell_2^n$ is an invertible linear operator such that $T^*T = A$. 

A couple of clarifications are in order. First, we claim that such an
operator $T$ exists, by the positive definiteness of $A$. Indeed, we
can choose a basis $e_1, \ldots, e_n$ of $X$, and a corresponding dual
basis $e_1^*, \ldots, e_n^*$ of $X^*$, and define $M$ to be the matrix
of $A$ with respect to these bases. Then $M$ is a positive definite
matrix, so it admits a Cholesky decomposition $M = L^\T L$. We can
then define $T$ to be the operator whose matrix with respect to $e_1,
\ldots, e_n$ and the standard basis of $\ell_2^n$ is $L$; the matrix
of the dual operator $T^*$ is $L^\T$, so we have $T^*T = A$ as required.

A second concern is whether $f(A)$ is well-defined. To see that this
is the case, observe that $A = T^*T = S^*S$ implies that there is an
orthogonal transformation $O:\ell_2^n \to \ell_2^n$ for which $S =
OT$.  Then, $S^{-1} = T^{-1}O^{-1}$, and, for a standard Gaussian
random variable $Z$,
\[
\ell(US^{-1}) = (\E\|UT^{-1}O^{-1}Z\|_Y^2)^{1/2} =
(\E\|UT^{-1}Z\|_Y^2)^{1/2}
= \ell(UT^{-1}),
\]
where we used the fact that $O^{-1}Z$ and $Z$ are identically
distributed because $O^{-1}$ is an orthogonal transformation. 

The following lemma collects some key technical properties of the function $f$.
\begin{lemma}\label{lm:obj-f}
  The following statements hold for the function $f$ defined on
  positive definite operators $A:X \to X^*$ as in \eqref{eq:obj-def}:
 \begin{itemize}
  \item $f$ is a differentiable convex function on positive definite
    operators $A:X \to X^*$;
  \item $f$ is given by the formula
    \begin{equation}\label{eq:f-formula}
    f(A) = \sup\{\tr((RUA^{-1}U^*R^*)^{1/2}): 
    R: Y \to \ell_2^n, \ell^*(R) \le 1\};
    \end{equation}
  \item the derivative of $f$ at $A$ is 
    \begin{equation}\label{eq:f-derivative}
    \nabla f(A) = 
    -\frac{1}{2} (RU)^{-1} ((RU)^{-*} A (RU)^{-1})^{-3/2} (RU)^{-*},
    \end{equation}
    where $R: Y \to \ell_2^n$, $\ell^*(R) \le 1$ is such that $f(A) = 
    \tr((RUA^{-1}U^*R^*)^{1/2})$.
  \end{itemize}
\end{lemma}

First we need an auxiliary lemma. 

\begin{lemma}\label{lm:invertible}
  Let $Y$ be an $n$-dimensional normed space, and let $S:\ell_2^n \to
  Y$ be an invertible linear operator. Then any operator $R:Y \to
  \ell_2^n$ such that $\tr(RS) = \ell(S)$ is invertible.
\end{lemma}
\begin{proof}
  Assume for contradiction that $R$ is not invertible, i.e.~it has a
  non-trivial kernel. Then, if $P$ is the orthogonal projection onto
  the kernel of $R$, we have $\ell(S) = \tr(RS) = \tr(R(I-P)S) \le
  \ell((I-P)S)$. Let $k$ be the dimension of the kernel of $R$, $W$ be
  the $k$-dimensional linear subspace such that $S(W)$ is the kernel
  of $R$, and let $K = S^{-1}(B_Y)$. Using integration by parts,
  we have
  \begin{align*}
  \ell((I-P)S)^2 &= \int_{t = 0}^\infty (1-\gamma_{n-k}(\sqrt{t}K\cap W^\perp))dt\\
  \ell(S)^2 &= \int_{t = 0}^\infty (1-\gamma_n(\sqrt{t}K))dt\\
  &= \int_{t = 0}^\infty \int_{W}(1-\gamma_{n-k}(\sqrt{t}K \cap (y + W^\perp))d\gamma_{k}(y))dt
  \end{align*}
  By the logconcavity of Gaussian measure and the symmetry of $K$,
  $\gamma_{n-k}(\sqrt{t}K \cap (y + W^\perp)) \le \gamma_{n-k}(\sqrt{t}K \cap
  W^\perp)$ for all $y \in W$, and, for all $y$ outside a
  compact set we have $\gamma_{n-k}(\sqrt{t}K \cap (y + W^\perp)) = 0 <
  \gamma_{n-k}(\sqrt{t}K \cap W^\perp)$. Therefore, $\ell((I-P)S) < \ell(S)$, a
  contradiction.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lm:obj-f}]
  We begin with the proof of differentiability. Notice that we can
  write $f(A)$ as
  \[
  f(A) = \sqrt{2\pi \det A} \int_{\R^n} \|Ux\|_Y e^{-\frac12 \langle x, Ax\rangle}dx
  \]
  The integrand is differentiable in $A$, so, by the dominated
  convergence theorem the derivative of $f$ exists and is given by
  differentiating under the integral sign. 

  Next we prove the identity \eqref{eq:f-formula}. Observe that for
  any orthogonal transformation $O:\ell_2^n \to \ell_2^n$ and any
  operator $V: \ell_2^n \to Y$, $\ell(VO) = \ell(V)$ by the
  rotational invariance of the Gaussian measure. Therefore,
  \begin{align*}
  f(A) = \ell(UT^{-1}) &= \sup\{\ell(UT^{-1}O): O \text{ orthogonal}\}\\
  &= \sup\{\tr(RUT^{-1}U): R:Y\to\ell_2^n, \ell^*(R) \le 1, U \text{ orthogonal}\}\\
  &= \sup\{\nu(RUT^{-1}): R:Y\to\ell_2^n, \ell^*(R) \le 1\}\\
  &= \sup\{\tr((RUA^{-1}U^*R^*)^{1/2}): R:Y\to\ell_2^n, \ell^*(R) \le  1\}. 
  \end{align*}
  Moreover, since we assumed that $U$ is invertible, by
  Lemma~\ref{lm:invertible}, we can assume that $R$ is invertible.
  Given this formula, in order to prove convexity, it is enough to
  prove that for any invertible operator $V:X \to \ell^n_2$ (which, in
  our case, equals $RU$), the function $\tr((VA^{-1}V^*)^{1/2})$ is
  convex in $A$ for $A:X\to X^*$ positive definite. Then, we would
  have that $f(A)$ is a supremum of convex functions, and, therefore,
  convex.  The convexity of $\tr((VA^{-1}V^*)^{1/2})$ follows from a
  standard argument based on majorization, which we give next.  Since
  $V$ is invertible, we have $VA^{-1}V^* = (V^{-*}AV^{-1})^{-1}$, and
  $\tr((VA^{-1}V^*)^{1/2}) = \tr(((V^{-*}AV^{-1})^{-1/2})$. Let
  $\alpha \in (0,1)$ be arbitrary, and let $A_1, A_2$ be two positive
  definite operators from $X$ to $X^*$. Let $\mu$ be the function that
  maps a self-adjoint operator on $\ell_2^n$ to the vector of its
  eigenvalues. By the Ky-Fan inequalities, $\mu(V^{-*}(\alpha A_1 +
  (1-\alpha)A_2)V^{-1})$ is majorized by $\alpha \mu(V^{-*}A_1V^{-1}) + (1-\alpha)
  \mu(V^{-*}A_2V^{-1})$. Let $g$ be the function defined on vectors $x \in
  \R^n$ with positve coordinates by $g(x) =
  \sum_{i=1}^n{x_i^{-1/2}}$. It is easy to verify that $g$ is convex
  and Schur-convex, so
  \begin{align*}
  g(\mu(V^{-*}(\alpha A_1 + (1-\alpha)A_2)V^{-1}))
  &\le
  g(\alpha \mu(V^{-*}A_1V^{-1}) + (1-\alpha) \mu(V^{-*}A_2V^{-1}))\\
  &\le 
  \alpha g(\mu(V^{-*}A_1V^{-1})) + (1-\alpha) g(\mu(V^{-*}A_2V^{-1})).
  \end{align*}
  Since the left hand side above equals   
  \[
  \tr((V^{-*}(\alpha A_1 + (1-\alpha)A_2)V^{-1})^{-1/2})
  = 
  \tr((V(\alpha A_1 + (1-\alpha)A_2)^{-1}V^*)^{1/2}),
  \]
  and the right hand side equals
  \begin{align*}
  \alpha \tr((V^{-*}A_1V^{-1})^{-1/2}) &+  (1- \alpha) \tr((V^{-*}A_2V^{-1})^{-1/2})\\
  &= 
  \alpha \tr((VA_1^{-1}V^*)^{1/2}) +  (1- \alpha) \tr((VA_2^{-1}V^*)^{1/2}),
  \end{align*}
  we have established convexity.   
 

  From \eqref{eq:f-formula}, we can see that the subgradient of
  $f$ at $A$ is
  \begin{align*}
  \partial f(A) &= \mathrm{conv}
  \{\nabla \tr((RUA^{-1}U^*R^*)^{1/2}):\\
  &\hspace{10em}
  R: Y \to \ell_2^n, \ell^*(R) \le 1,
  f(A) = \tr((RUA^{-1}U^*R^*)^{1/2})\}\\
  &= 
  \mathrm{conv}\{
  \nabla \tr(((RU)^{-1}A(RU)^{-*})^{-1/2}):\\ 
  &\hspace{10em}
  R: Y \to \ell_2^n, \ell^*(R) \le 1, 
  f(A) = \tr((RUA^{-1}U^*R^*)^{1/2})\}\\
  &= 
  \mathrm{conv}\{
  -\frac{1}{2} 
  (RU)^{-1} ((RU)^{-*} A (RU)^{-1})^{-3/2} (RU)^{-*}:\\ 
  &\hspace{10em}
  R: Y \to \ell_2^n, \ell^*(R) \le 1, 
  f(A) = \tr((RUA^{-1}U^*R^*)^{1/2})\}.
  \end{align*}
  Above we used Lemma~\ref{lm:invertible}, and
  the fact that 
  \[
  \nabla \tr(X^{-1/2}) =
  -\frac{1}{2}X^{-3/2}
  \]
  for any positive definite $X:\ell_2^n\to\ell_2^n$ (see~\cite{Lewis95}). Since $f$ is
  differentiable, we have that $\partial f(A)$ is a singleton set,
  i.e.~
  \[
  \nabla f(A) = 
  -\frac{1}{2} (RU)^{-1} ((RU)^{-*} A (RU)^{-1})^{-3/2} (RU)^{-*}
  \]
  for the an invertible operator $R: Y \to \ell_2^n$, such that
  $\ell^*(R) \le 1$ and $f(A) =
  \tr((RUA^{-1}U^*R^*)^{1/2})$. This finishes the proof
  of the lemma.
\end{proof}

We are now ready to formulate $\lambda(U)$ as a convex
minimization problem. 

\begin{lemma}\label{lm:fact-convex}
  For any two $n$-dimensional normed spaces $X$ and $Y$, and any
  linear operator $U:X \to Y$, $\lambda(U)$ equals
  \begin{align}
    &\inf %\ell(B^{-1})
    f(A)  \label{eq:fact-obj}\\
    &\text{s.t.}\notag\\
    &A: X \to X^*,\|A\| \le 1\label{eq:fact-constr}\\
    &A \succ 0.\label{eq:fact-psd}
  \end{align}
  The function $f$ is the one defined in \eqref{eq:obj-def}.
  %$B^{-1}$ is the inverse of the principle square root
  %$M^{1/2}$ of $M$, and is taken as an operator from $\ell_2$ to $Y$. 

  Moreover, the objective \eqref{eq:fact-obj} and the constraints
  \eqref{eq:fact-constr}--\eqref{eq:fact-psd} are convex in $A$.
\end{lemma}
\begin{proof}
  The convexity of the constraints
  \eqref{eq:fact-constr}--\eqref{eq:fact-psd} is apparent from the
  definition, and the convexity of the objective was proved in
  Lemma~\ref{lm:obj-f}. We proceed to show that the value of
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd} equals $\lambda(U)$.

  Let $T:X \to \ell_2^n$ and $S:\ell_2^n \to Y$, $ST = U$ be a
  factorization achieving $\lambda(U)$ such that $\|T\| = 1$ and
  $\ell(S) = \lambda(U)$. Then we claim that $A \eqdef T^*T$
  satisfies \eqref{eq:fact-constr}--\eqref{eq:fact-psd} and 
  $f(A) = \ell(S) $, so the value of
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd} is at most
  $\ell(S) = \lambda(U)$. Indeed, $A$ is clearly positive semidefinite,
  and must be positive definite, as $T$ is
  invertible. Furthermore, since $\|T\|\le 1$, we have that for any $x
  \in B_X$,
  \[\langle x, Ax \rangle = \langle Tx, Tx\rangle =
  \|Tx\|^2_2 \le 1.\] On the other hand, since $A$ is self-adjoint,
  $\|A\| = \sup\{\langle x, Ax\rangle: x \in B_X\}$, and we have shown
  that $\|A\| \le 1$. Moreover, since $A = T^* T$, we get
  \[
  f(A) = \ell(UT^{-1}) = \ell(S) = \lambda(U).
  \]
  This finishes the proof of the claim that the value of
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd} is at most $\lambda(U)$.
  
  Next we prove the reverse inequality. Given a feasible solution $A$
  to \eqref{eq:fact-obj}--\eqref{eq:fact-psd}, we take an operator
  $T:X \to \ell_2^n$ such that $A = T^* T$. Then we construct a
  factorization $U=ST$ by setting $S = UT^{-1}$.  By
  \eqref{eq:fact-constr}--\eqref{eq:fact-psd}, for any $x \in B_X$ we
  have
  \[
  \|Tx\|^2_2 = \langle Tx, Tx\rangle = \langle x, Ax \rangle  \le
  1,\] so $\|T\| \le 1$. Moreover, $\ell(S) = f(A)$ by
  definition. This proves that the value of
  \eqref{eq:fact-obj}--\eqref{eq:fact-psd} is at least
  $\lambda(U)$, and, since we already showed that it is also at
  most $\lambda(U)$, the two are equal.
\end{proof}

\subsection{Dual Formulation}


Next we give a dual formulation of $\lambda(U)$ as a supremum over
``dual certificates''. Such a formulation is useful in proving
Theorem~\ref{thm:fact-vollb}, because it allows us to reduce such a
proof to relating the dual certificates to the terms in the volume
lower bound~\eqref{eq:vol-lb}. If we can show that every dual
certificate bounds from below one of the terms of the volume lower
bound up to polylogarithmic factors, then we can conclude that
$\lambda(U)$ also bounds the volume lower bound from below. 

The derivation of the dual formulation from the
convex program \eqref{eq:fact-obj}--\eqref{eq:fact-psd} is mostly
routine using Lagrange duality (see,
e.g.~\cite{BoydV04}). Nevertheless, because of the complicated nature
of our objective, the derivation is quite technical. The dual
formulation we use is given in the following theorem.
\begin{theorem}\label{thm:dual}
  Let $X$ and $Y$ be two $n$-dimensional normed spaces, such that the
  unit ball of $X$ is $B_X = \mathrm{conv}\{\pm v_1, \ldots, \pm
  v_m\}$. Then, for any linear operator $U:X \to Y$, $\lambda(U)$
  equals
  \begin{align}
    &\sup \tr((RU(\sum_{i = 1}^m{p_i v_i \otimes  v_i})U^*R^*)^{1/3})^{3/2}\label{eq:dual-obj}\\
    \text{s.t.}\notag\\
    &R: Y \to \ell_2^n, \ell^*(R) \le 1 \label{eq:dual-ellstar}\\
    &\sum_{i = 1}^m{p_i} = 1\label{eq:dual-prob}\\
    &p_1, \ldots, p_m \ge 0. \label{eq:dual-nonneg}
  \end{align}
\end{theorem}
\begin{proof}
  Since $B_X = \mathrm{conv}\{\pm v_1, \ldots, \pm v_m\}$, we can can
  rewrite \eqref{eq:fact-obj}--\eqref{eq:fact-psd} as
  \begin{align}
    &\inf f(A)\ \ \ 
    \text{s.t.}\label{eq:poly-obj}\\
    &\langle v_i, Av_i\rangle \le 1 \ \ \forall i \in [m]\\
    &A \succ 0\label{eq:poly-psd},
  \end{align}
  By Lemma~\ref{lm:fact-convex} this is a convex minimization problem
  and its value equals $\lambda(U)$.

  The conjugate function $f^*$ of $f$ is defined on self-adjoint
  linear operators $Q:X^* \to X$ by:
  \[
  f^*(Q) \eqdef \sup\{\tr(QA) - f(A): A \succ 0\}.
  \]
  Since $f^*(Q)$ is the supremum of affine functions, it is convex and
  lower semicontinuous. The set on which $f^*$ takes a finite value is
  called its domain. The significance of $f^*$ is the fact
  \begin{equation}\label{eq:lagrange}
  \lambda(U) = 
  \sup\left\{-\sum_{i = 1}^m{q_i} - 
    f^*\left(-\sum_{i = 1}^m{q_i v_i\otimes v_i}\right):
      q_1, \ldots, q_m \ge 0\right\}.
  \end{equation}
  This follows from the duality theory of convex optimization, since
  \eqref{eq:poly-obj}--\eqref{eq:poly-psd} satisfies Slater's
  condition, which in this case reduces to just checking the existence
  of a feasible $A$ (see~\cite[Chapter 5]{BoydV04}). We proceed to
  compute $f^*(Q)$.

  It is easy to see that unless $-Q \succeq 0$, $f^*(Q) = \infty$,
  and, conversely, if $-Q \succeq 0$, $f^*(Q) \le 0 <
  \infty$. Therefore, the domain of $f^*$ is $\{Q: -Q \succeq 0\}$. We
  will first handle the case $-Q \succ 0$, and then we will extend our
  formula for $f^*(Q)$ to $-Q \succeq 0$ by continuity. Assume then
  that $-Q \succ 0$. As $f$ is a differentiable convex function, the
  range of the derivative $\nabla f$ includes the relative
  interior of the domain of $f^*$, i.e.~the set of linear operators
  $\{X: -X \succ 0\}$ (see Corollary~26.4.1.~in~\cite{Rockafellar});
  from \eqref{eq:f-derivative} it is also apparent that $\nabla f(A)$
  is negative definite for any positive definite $A$, so the range of
  $\nabla f(A)$ is exactly $\{X: -X \succ 0\}$. This means that
  the equation 
  \[
  0 = \nabla(\tr(QA) - f(A)) = Q - \nabla f(A)
  \]
  has a solution over $A \succ 0$, and $f^*(Q)$ is achieved at this
  solution. Fix $A:X \to X^*$ to be a solution to this equation, and
  let and let $R:Y \to \ell_2^n$ be an invertible map such that
  $\ell^*(R) \le 1$ and $f(A) = \tr((RUA^{-1}U^*R^*)^{1/2})$. For $B =
  RU$, $\nabla f(A) = Q$ and \eqref{eq:f-derivative} imply
  \[
  (B^{-*} A B^{-1})^{-3/2} = -2 BQB^*,
  \]
  and, therefore,
  \begin{align*}
    f^*(Q) &= \tr(QA) - \tr((BA^{-1}B^*)^{1/2})\\
    &= \tr((BQB^*)(B^{-*}AB^{-1})) -   \tr((B^{-*}AB^{-1})^{-1/2})\\
    &= -\frac{3}{2^{2/3}} \tr((-BQB^*)^{1/3}).
  \end{align*}
  We have proved that
  \begin{equation}\label{eq:conjugate-lb}
  f^*(Q) \ge 
  \inf\left\{ -\frac{3}{2^{2/3}} \tr((-RUQU^*R^*)^{1/3}):
  R:Y \to \ell_2^n,\ell^*(R) \le 1\right\},
  \end{equation}
  for any $Q:X^*\to X$ such that $-Q \succ 0$.  Let $\mathcal{D}$ be
  the set of invertible maps $R:Y \to \ell_2^n$ such that $\ell^*(R)
  \le 1$. By \eqref{eq:f-formula} and the definition of the
  conjugate function $f^*$ we also have
  \begin{align*}
  f^*(Q) &= 
  \sup_{A: A \succ 0} 
  \inf_{R \in \mathcal{D}}
  \tr(QA) - \tr((RUA^{-1}U^*R^*)^{1/2})\\
  &\le
  \inf_{R \in \mathcal{D}}
  \sup_{A: A \succ 0} 
  \tr(QA) - \tr((RUA^{-1}U^*R^*)^{1/2}).
  \end{align*}
  For any $R \in \mathcal{D}$, the supremum on the right hand side
  equals the value at $Q$ of the conjugate $h^*_R$ of the function
  $h_R(A) = \tr((RUA^{-1}U^*R^*)^{1/2})$. A calculation analogous to
  the one above for $f^*$ shows that $h_R^*(Q) = -\frac{3}{2^{2/3}}
  \tr((-RUQU^*R^*)^{1/3})$. 
  \snote{This is sketchy, but I feel that repeating the same
    calculations for almost the same function is not worth it.}
  Therefore,
  \[
    f^*(Q) \le
    \inf\left\{ -\frac{3}{2^{2/3}} \tr((-RUQU^*R^*)^{1/3}):
    R:Y \to \ell_2^n,\ell^*(R) \le 1\right\},
  \]
  and, together with \eqref{eq:conjugate-lb}, we have established 
  \begin{equation}
    \label{eq:conjugate}
    f^*(Q) =
    \inf\left\{ -\frac{3}{2^{2/3}} \tr((-RUQU^*R^*)^{1/3}):
    R:Y \to \ell_2^n,\ell^*(R) \le 1\right\},
  \end{equation}
  for any $Q:X^*\to X$ such that $-Q \succ 0$. Since $f^*$ is a
  a proper lower-semicontinuous function, it is continuous on any
  line segment contained in its domain by Corollary
  7.5.1.~in~\cite{Rockafellar}. Therefore, \eqref{eq:conjugate}
  holds for any $Q \succeq 0$ as well. 
  
  By \eqref{eq:lagrange} and \eqref{eq:conjugate}, 
  \begin{multline}
  \lambda(U) = 
  \sup \biggl\{-\sum_{i = 1}^m{q_i} 
  + \frac{3}{2^{2/3}} \tr\Bigl(\Bigl(-RU\Bigl(\sum_{i = 1}^m{q_i  v_i\otimes  v_i}\Bigr)U^*R^*\Bigr)^{1/3}\Bigr):\\
  R:Y \to \ell_2^n,\ell^*(R) \le 1,
  q_1, \ldots, q_m \ge 0\biggr\}.
  \end{multline}
  Let us write $q = tp$ where $t \ge 0$ is a real number, and $p_1,
  \ldots, p_m \ge 0$ satisfy $\sum_i p_i = 1$. Then we can rewrite the
  equation above as
  \[
  \lambda(U) = 
  \sup\left\{-t + 
    \frac{3t^{1/3}}{2^{2/3}} \tr\Bigl(\Bigl(-RU\Bigl(\sum_{i = 1}^m{p_i  v_i\otimes v_i}\Bigr)U^*R^*\Bigr)^{1/3}\Bigr):
    t, p_1, \ldots, p_m \ge 0, \sum_{i=1}^m p_i = 1\right\}.
  \]
  Maximizing over $t$ finishes the proof. 
\end{proof}

\subsection{Proof of Theorem~\ref{thm:fact-vollb}}

Here we use Theorem~\ref{thm:dual} to prove
Theorem~\ref{thm:fact-vollb}.  In the proof, we use covering/packing
numbers and Sudakov's inequality. We also recall the definition of the
entropy number $e_k(u)$ of an operator $u:X \to Y$:
\[
e_k(u) = \inf\{\varepsilon: N(u(B_X), \varepsilon B_Y) \le 2^{k-1}\}. 
\]
In this notation, the dual Sudakov inequality says that there exists a
constant $C$ such that for any $u:\ell_2^n \to X$
\begin{equation}
  \label{eq:sudakov}
  \max_{k = 1}^n \sqrt{k}e_k(u) \le C\ell(u). 
\end{equation}

We also use the basic volumetric lower estimate on entropy numbers
\begin{equation}
  \label{eq:entropy-vol}
  e_k(u) \ge \frac{\vol_k(Pu(B_X))^{1/k}}{2\vol_k(P(B_Y))^{1/k}},
\end{equation}
valid for any $u:X \to Y$ and any rank $k$ orthogonal projection $P$.

We also make use of a weighted version of
Lemma~\ref{lm:rip-det}.  

\begin{lemma}\label{lm:rip-det-weighted}
  Let $u_1, \ldots, u_m \in \R^n$, and let $c_1, \ldots, c_m \ge 0$,
  $\sum_{i = 1}^mc_i = 1$. Let $\lambda_1 \ge \ldots \ge \lambda_n$ be
  the eigenvalues of $\sum_{i=1}^m{c_i u_i \otimes u_i}$, and let $G$
  be the Gram matrix of $u_1, \ldots, u_m$. For any
  integer $k$ such that $1 \le k \le n$, there exists a set $S
  \subseteq [m]$ of size $k$ such that 
  \[
  \frac{\det(G_{S,S})}{k!} \ge\lambda_1 \ldots \lambda_k. 
  \]
\end{lemma}
\begin{proof}
  Consider the matrix $H = (\sqrt{c_ic_j}\langle u_i, u_j
  \rangle)_{i,j = 1}^m$. This matrix has the same nonzero eigenvalues as
  $\sum_{i=1}^m{c_i u_i \otimes u_i}$, and, therefore,
  \[
  \sum_{S \subseteq [m]: |S| = k}{\left(\prod_{i \in S}{c_i}\right)\det(G_{S,S})}
  = \sum_{S \subseteq [m]: |S| = k}\det(H_{S,S}) = p_{k,n}(\lambda),
  \]
  where $p_{k,n}$ is the degree $k$ elementary symmetric polynomial in $n$
  variables. (See the proof of Lemma~\ref{lm:rip-det} for a
  justification of the final equality.) Therefore, 
  \[
  \max_{S \subseteq [m]: |S| = k}\det(G_{S,S}) 
  \ge \frac{p_{k,n}(\lambda)}{p_{k,m}(c)}. 
  \]
  We can show that $p_{k,n}(\lambda) \ge \lambda_1 \ldots \lambda_k$
  as in the proof of Lemma~\ref{lm:rip-det}. I.e.~we define a
  vector $\mu \in \R^n_{\ge 0}$ that majorizes $\lambda$ by $\mu_i
  \eqdef \lambda_i + \nu$ for $i \in [k]$ and $\mu_i = 0$ for $i > k$,
  where $\nu \eqdef \frac{1}{k}\sum_{i = k+1}^n{\lambda_i}$. Then, by
  the Schur concavity of $p_{k,n}$,
  \[
  p_{k,n}(\lambda) \ge p_{k,n}(\mu) \ge \lambda_1 \ldots \lambda_k.
  \]
  Similarly, observe that the vector $d \in \R^m_{\ge 0}$ with coordinates $d_i =
  \frac{1}{m}$ is majorized by $c$, and, therefore, 
  \[
  p_{k,m}(c) \le p_{k,m}(d) = \frac{1}{m^k} {m \choose k} \le \frac{1}{k!}.
  \]
  Combining the inequalities finishes the proof. 
\end{proof}

Finally, we use the following elementary inequality, valid for an
absolute constant $C$, and any sequence $x_1 \ge x_2 \ge \ldots \ge
x_n \ge 0$ of non-negative reals
\begin{equation}
  \label{eq:DFE} %Daniel's Favorite Inequality
  \sum_{i = 1}^n{x_i} \le C(1+\log n) 
  \max_{k = 1}^n k \left(\prod_{i = 1}^k{x_i}\right)^{1/k}.
\end{equation}

\begin{proof}[Proof of Theorem~\ref{thm:fact-vollb}]
  By Theorem~\ref{thm:dual}, there exists an operator $R: \ell_2^n \to
  Y$, $\ell^*(R) \le 1$ and non-negative reals $p_1, \ldots, p_m \ge
  0$, $\sum_{i = 1}^m{p_i} = 1$, such that 
  \[
  \lambda(U)^{2/3} 
  = \tr((RU(\sum_{i = 1}^m{p_i v_i  \otimes  v_i})U^*R^*)^{1/3})
  = \tr((\sum_{i = 1}^m{p_i RU(v_i)  \otimes  RU(v_i)})^{1/3}).
  \]
  Let $u_i \eqdef RU(v_i)$, and let $\lambda_1 \ge \ldots \ge
  \lambda_n \ge 0$ be the eigenvalues of $\sum_{i = 1}^m{p_i u_i
    \otimes u_i}$, so that $\lambda(U)^{2/3} = \sum_{i =
    1}^n{\lambda_i^{1/3}}$. Applying \eqref{eq:DFE} to $x_i \eqdef
  \lambda_i^{1/3}$, we have that there exists an integer $k$, $1 \le k
  \le n$, such that
  \[
  \lambda(U)^{2/3} \le C_0 (1+\log n) k (\lambda_1 \ldots \lambda_k)^{1/(3k)},
  \]
  for an absolute constant $C_0$. Now, by
  Lemma~\ref{lm:rip-det-weighted}, we have that there exists a set $S
  \subseteq [m]$ of size $k$ such that
  \[
  (\lambda_1 \ldots \lambda_k)^{1/k} \le
  \frac{\det(H_{S,S})^{1/k}}{(k!)^{1/k}},
  \]
  where $H$ is the Gram matrix of $u_1, \ldots, u_m$. By Stirling's
  estimate, this implies that 
  \begin{equation}\label{eq:fact-det}
  \lambda(U)^{2/3} \le C_1 (1+\log n) k^{2/3} \det(H_{S,S})^{1/(3k)},
  \end{equation}
  for an absolute constant $C_1$. 

  To finish the proof, we relate the right hand side of
  \eqref{eq:fact-det} to the volume lower bound. Let $V:\R^S \to W$ be
  the operator defined by $V(x) \eqdef \sum_{i \in S}{x_i v_i}$ and let $T$
  be the restriction of $RU$ to $W \eqdef \lspan\{v_i: i \in
  S\}$. Then, 
  \begin{equation}\label{eq:det-product}
  \det(H_{S,S}) = \det(TVV^*T^*) = \det(V)^2\det(T^*)^2 =
  \det(G_{S,S}) \det(T^*)^2,
  \end{equation}
  where $G$ is the Gram matrix of $v_1, \ldots, v_m$. Furthermore,
  \begin{align*}
  |\det(T^*)|^{1/k} =
  \frac{\vol_k(P_WR^*(B_2^n))^{1/k}}{\vol_k(B_2^k)^{1/k}}
  &= \frac{\vol_k(P_WR^*(B_2^n))^{1/k}}{\vol_k(P_WB_Y^\circ)^{1/k}} \cdot 
  \frac{\vol_k(P_WB_Y^\circ)^{1/k}}{\vol_k(B_2^k)^{1/k}}\\
  &\le 2e_k(R^*) \cdot 
  \frac{\vol_k(P_WB_Y^\circ)^{1/k}}{\vol_k(B_2^k)^{1/k}},
  \end{align*}
  where $P_W:Y^* \to Y^*/W^\perp$ is the orthogonal projection onto
  the subspace $W$, which we identify with $Y^*/W^\perp$ in the natural
  way. The last inequality follows from the volume
  estimate~\eqref{eq:entropy-vol}. Applying \eqref{eq:sudakov} to the
  first term on the right hand side, and Santalo's inequality to the
  second, we have that, for an absolute constants $C_2$,
  \[
  |\det(T^*)|^{1/k} \le C_2 \frac{\ell(R^*)}{\sqrt{k}} \cdot
  \frac{\vol_k(B_2^k)^{1/k}}{\vol_k(B_Y \cap W)^{1/k}}. 
  \]
  Finally, by $K$-convexity $\ell(R^*) \le K(Y) \ell^*(R) \le
  K(Y)$. Together with a basic estimate for $\vol_k(B_2^k)^{1/k}$, we
  have that 
  \[
  |\det(T^*)|^{1/k} \le C_3\ \frac{K(Y)}{k\vol_k(B_Y \cap W)^{1/k}},
  \]
  for to an absolute constant $C_3$. Combining with \eqref{eq:fact-det} and
  \eqref{eq:det-product}, we have, for an absolute constant $C_4$,
  \[
  \lambda(U) \le C_4K(Y) (1+\log n)^{3/2}  
  \frac{\det(G_{S,S})^{1/(2k)}}{\vol_k(B_Y \cap W)^{1/k}}
  \le C_4 K(Y) (1+\log n)^{3/2}  
  \vollb((v_i)_{i =  1}^m, Y),
  \]
  as required.
\end{proof}

Combining \eqref{eq:vol-lb} and
Theorems~\ref{thm:factorization}~and~\ref{thm:fact-vollb},
we have the following

\begin{corollary}
  Let $Y$ be a normed space defined on $\R^n$, $u_1, \ldots, u_N \in
  \R^n$, and let $X$ be the norm on $\R^n$ with unit ball $B_X
  \eqdef\mathrm{conv}\{\pm u_1, \ldots, \pm u_N\}$. Then, for absolute
  constants $C, C'$,
  \[
  \beta((u_i)_{i = 1}^N, Y) \le \beta(X,Y)
  \le C \lambda(U)
  \le C' K(Y) (1+\log n)^{3/2}  \beta((u_i)_{i = 1}^N, Y).
  \]
\end{corollary}

\bibliographystyle{alpha}
\bibliography{Discrepancy}
\end{document}
